[{"title":"Hello","type":0,"sectionRef":"#","url":"blog/hello-world/","content":"Welcome to this blog. This blog is created with Docusaurus 2. This is a test post. A whole bunch of other information.","keywords":""},{"title":"Hola","type":0,"sectionRef":"#","url":"blog/hola/","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":""},{"title":"Welcome","type":0,"sectionRef":"#","url":"blog/welcome/","content":"Blog features are powered by the blog plugin. Simply add files to the blog directory. It supports tags as well! Delete the whole directory if you don't want the blog features. As simple as that!","keywords":""},{"title":"Style Guide","type":0,"sectionRef":"#","url":"docs/examples/doc1/","content":"","keywords":""},{"title":"Markdown Syntax","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#markdown-syntax","content":"To serve as an example page when styling markdown based Docusaurus sites. "},{"title":"Headers","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#headers","content":"H1 - Create the best documentation "},{"title":"H2 - Create the best documentation","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#h2---create-the-best-documentation","content":""},{"title":"H3 - Create the best documentation","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#h3---create-the-best-documentation","content":"H4 - Create the best documentation# H5 - Create the best documentation# H6 - Create the best documentation#  "},{"title":"Emphasis","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#emphasis","content":"Emphasis, aka italics, with asterisks or underscores. Strong emphasis, aka bold, with asterisks or underscores. Combined emphasis with asterisks and underscores. Strikethrough uses two tildes. Scratch this.  "},{"title":"Lists","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#lists","content":"First ordered list itemAnother item ⋅⋅* Unordered sub-list.Actual numbers don't matter, just that it's a number ⋅⋅1. Ordered sub-listAnd another item. ⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown). ⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) Unordered list can use asterisks Or minuses Or pluses  "},{"title":"Links","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#links","content":"I'm an inline-style link I'm an inline-style link with title I'm a reference-style link I'm a relative reference to a repository file You can use numbers for reference-style link definitions Or leave it empty and use the link text itself. URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later.  "},{"title":"Images","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#images","content":"Here's our logo (hover to see the title text): Inline-style:  Reference-style:   "},{"title":"Code","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#code","content":"var s = 'JavaScript syntax highlighting';alert(s); Copy s = \"Python syntax highlighting\"print(s) Copy No language indicated, so no syntax highlighting.But let's throw in a <b>tag</b>. Copy function highlightMe() { console.log('This line can be highlighted!');} Copy  "},{"title":"Tables","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#tables","content":"Colons can be used to align columns. Tables\tAre\tCoolcol 3 is\tright-aligned\t\\$1600 col 2 is\tcentered\t\\$12 zebra stripes\tare neat\t\\$1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown\tLess\tPrettyStill\trenders\tnicely 1\t2\t3  "},{"title":"Blockquotes","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#blockquotes","content":"Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote.  "},{"title":"Inline HTML","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#inline-html","content":"Definition list Is something people use sometimes. Markdown in HTML Does *not* work **very** well. Use HTML tags.  "},{"title":"Line Breaks","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#line-breaks","content":"Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the same paragraph.  "},{"title":"Admonitions","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#admonitions","content":"note This is a note tip This is a tip important This is important caution This is a caution warning This is a warning "},{"title":"Diagrams (Mermaid)","type":1,"pageTitle":"Style Guide","url":"docs/examples/doc1/#diagrams-mermaid","content":"Refer to Mermaid website graph LR; A-->B; B-->C; B-->D[plop lanflz eknlzeknfz]; "},{"title":"Guide","type":0,"sectionRef":"#","url":"docs/examples/doc2/","content":"Guide This is a link to another document. This is a link to an external page. 123","keywords":""},{"title":"New Page Creation","type":0,"sectionRef":"#","url":"docs/examples/doc0/","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"New Page Creation","url":"docs/examples/doc0/#overview","content":"The pages are available as source code from our server, available as remote folder. It can be accessed by VS Code directly, or using mounted network drive option. "},{"title":"Access the Folder in VS Code (Remote SSH)","type":1,"pageTitle":"New Page Creation","url":"docs/examples/doc0/#access-the-folder-in-vs-code-remote-ssh","content":"tip This is the more convenient way of editting the documentation files, and at the same time obtain a command prompt. In VS Code, install plugin Remote - SSH. Then at the bottom-left corner, press the green button open a remote window, follow the prompt at the top to proceed for the user at host (tsluser@172.18.72.192), password (given to you) key-ins. The website folder is located at ~/docusaurus_html/tl-tech-details.  "},{"title":"Access the Folder (as Files)","type":1,"pageTitle":"New Page Creation","url":"docs/examples/doc0/#access-the-folder-as-files","content":"All website generation files is located at our server within NUS intranet (VPN is needed if accessing outside NUS) smb://172.18.72.192/techdetails/ for Linux, and\\\\172.18.72.192\\techdetails for Windows (Mount as network drive, instead of a network location)  Folder displayed in Linux\tMounted Drive in Windows Regarding where to put in the address On Linux machines, go to the file explorer, in the location bar, key in the address and press enter. On Windows machines, go to file explorer --> This PC (Computer) --> Map network drive, then key in the credentials (switch domain to WORKGROUP)  "},{"title":"File Functions","type":1,"pageTitle":"New Page Creation","url":"docs/examples/doc0/#file-functions","content":".md file# Each page is one .md file located in the docs subfolder, arranged according to the navigation bar (e.g. ./docs/hardware/ and ./docs/systems). sidebars.js file# To make the page appear on the sidebar, file ./docs/sidebars.js requires modification docusaurus.config.js file# In rarer cases, if a new tab or sub-entries need to be created at the navigation bar (on the top of the website), docusaurus.config.js requires modification "},{"title":"Create a Page","type":1,"pageTitle":"New Page Creation","url":"docs/examples/doc0/#create-a-page","content":"Creating a page is as simple as by creating a file like mypage.md. For example, if we create the file under the path ./docs/examples/mypage.md, then the page should be immediately accessible through the live preview at port 8888: http://172.18.72.192:8888/tech-details/docs/examples/mypage Within the empty mypage.md, add the front matter to configure the title ---hide_title: truesidebar_label: My Page--- # My Title The rest of your content... Copy The end result should look like this:  Notice that there is no sidebar available for this page, which means the page is currently not browsable unless the exact URL is keyed in. Therefore we will add the page to sidebar next. "},{"title":"Add a Page to the Sidebar","type":1,"pageTitle":"New Page Creation","url":"docs/examples/doc0/#add-a-page-to-the-sidebar","content":"The file ./docs/sidebars.js is organised in blocks like this:  // systems systemsSidebar: { 'Past Platforms : pixhawk v2': [ 'systems/pixhawk_v1/pixhawk', ], 'Simulations': [ 'systems/simulation/unity-SITL', 'systems/simulation/unity-HITL', ], 'Vicon': [ 'systems/vicon', ], }, Copy Each block has a name like systemsSidebar which is arbitrary (does not affect the website display), but it logically links all relevant documents under one sidebar group. Within each block, there can be nested levels of sidebar entries, and the format should be self-explanatory. But be reminded of the , at various places for the correct syntax. The .md postfix should be omitted when mentioning each file. After adding content in sidebars.js, the page should now be rendered like this:  "},{"title":"Add Important Pages to Navigation Bar","type":1,"pageTitle":"New Page Creation","url":"docs/examples/doc0/#add-important-pages-to-navigation-bar","content":"The navigation bar is configured within the main docusaurus.config.js file. It is under the object themeConfig.navbar.items. Each item can be nested, an example would be:  // Systems { to: 'docs/systems/vicon', activeBasePath: 'docs/systems', label: 'Systems', position: 'left', items: [ { // activeBasePath:'docs/systems/', label: 'Simulations', to: 'docs/systems/simulation/unity-SITL' }, { // activeBasePath:'docs/systems/', label: 'Vicon', to: 'docs/systems/vicon' }, { // activeBasePath:'docs/systems/pixhawk_v1', label: 'Past Platforms : pixhawk v2', to: 'docs/systems/pixhawk_v1/pixhawk' }, ] }, Copy Again the syntax should be self-explanatory. "},{"title":"Preview Your Changes","type":1,"pageTitle":"New Page Creation","url":"docs/examples/doc0/#preview-your-changes","content":"Preview your live changes at http://172.18.72.192:8888/tech-details/ "},{"title":"Work with Github Pages","type":0,"sectionRef":"#","url":"docs/examples/doc3/","content":"","keywords":""},{"title":"Dependencies","type":1,"pageTitle":"Work with Github Pages","url":"docs/examples/doc3/#dependencies","content":"For changing NodeJS version to >= 12.13.0, you can refer to https://phoenixnap.com/kb/update-node-js-version sudo apt updatecurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bashsource ~/.bashrcnvm --version #Check NVM versionnvm lsnvm ls-remote #Will display all the available versionsnvm install 12.13.0node --version #Check NodeJS version Copy "},{"title":"Overview","type":1,"pageTitle":"Work with Github Pages","url":"docs/examples/doc3/#overview","content":"Clone the master branch https://github.com/tsltech/tech-details Follow the README on https://github.com/tsltech/tech-details for initialization. Make your changes. Test your changes: yarn start If everything looks great, go ahead to commit and push your changes to master. "},{"title":"Powered by MDX","type":0,"sectionRef":"#","url":"docs/examples/mdx/","content":"Powered by MDX You can write JSX and use React components within your Markdown thanks to MDX. Docusaurus green and Facebook blue are my favorite colors. I can write Markdown alongside my JSX!","keywords":""},{"title":"Docusaurus Pre-requisite","type":0,"sectionRef":"#","url":"docs/examples/doc4/","content":"","keywords":""},{"title":"Requirements","type":1,"pageTitle":"Docusaurus Pre-requisite","url":"docs/examples/doc4/#requirements","content":"Node.js version >= 12.13.0 or above (which can be checked by running node -v). You can use nvm for managing multiple Node versions on a single machine installedYarn version >= 1.5 (which can be checked by running yarn --version). Yarn is a performant package manager for JavaScript and replaces the npm client. It is not strictly necessary but highly encouraged. "},{"title":"Installation","type":1,"pageTitle":"Docusaurus Pre-requisite","url":"docs/examples/doc4/#installation","content":"Install Node.js and npm from snap (the command will install both) sudo snap install node --classic Install Yarn via npm npm install --global yarn Check installation with Node.js: node -v npm: npm -v Yarn: yarn -v "},{"title":"camera-flash-wiring","type":0,"sectionRef":"#","url":"docs/hardware/cameras/camera-flash-wiring/","content":"","keywords":""},{"title":"Camera and Strobe wiring","type":1,"pageTitle":"camera-flash-wiring","url":"docs/hardware/cameras/camera-flash-wiring/#camera-and-strobe-wiring","content":"The following below is based on the set used on cam 4. Feel free to modify to simplify/improve the design. Wiring diagram Camera trigger Opamp Camera trigger signal from pixhawk/pixhack has a voltage of 2.3V, while the camera modules require 3.3-5V in order to be counted as an active high signal. An opamp is hence used to amplify the signal. Model: SparkFun OpAmp Breakout - LMV358 Purpose: Amplify signal between pixhawk AUX pins and Camera  Procedures: 1) Remove the two resistors as above and short using solder. (Blue arrows) 2) Connect corresponding wires Vcc: External 3.3/5V voltage from Pixhawk(Serial, GPS port, etc), or BatteryGnd: Ground wire.Out: Out trigger wire, to both camera modules.In: Signal in from Pixhawk/Pixhack. 3) Connect OUT signal to Oscilloscope, adjust knob (green arrow) till amplified signal can be seen(3.3V - 5V). Fixed the knob with hot glue. LED Flash driver Model: RCD-24-PL  Pinout Gnd: GroundPWM: Connected to Strobe out port from one or both camera modules+Vin: >10V voltage from battery+Vout/-Vout: To flash unit Strobe setting(Inside tcam capture)  Important settings: Strobe enable: [✓]Strobe exposure: [x]Strobe polarity: [x]Strobe duration:Substainable (>10K) Strobe delay : 0Trigger polarity: Rising edgeTrigger exposure mode: Frame start "},{"title":"My Page","type":0,"sectionRef":"#","url":"docs/examples/mypage/","content":"My Page Render $\\LaTeX$ equation $y = ax + b$ Render $\\LaTeX$ fraction and matrix $$ \\frac{2}{3} $$ $$ \\begin{bmatrix} 1 0 \\ 0 1 \\ \\end{bmatrix} $$ Render code main.cpp void main(){ return 0;} Copy For more Markdown examples, refer to this. Feel free to try adding things below for experiments!","keywords":""},{"title":"RealSense Cameras","type":0,"sectionRef":"#","url":"docs/hardware/cameras/realsense-install/","content":"","keywords":""},{"title":"Install librealsense on Jetson","type":1,"pageTitle":"RealSense Cameras","url":"docs/hardware/cameras/realsense-install/#install-librealsense-on-jetson","content":"https://github.com/IntelRealSense/librealsense/blob/master/doc/installation_jetson.md #!/bin/bash# Installs the Intel Realsense library librealsense on a Jetson Nano Development Kit# The installation is from a RealSense Debian repository# Copyright (c) 2016-19 Jetsonhacks # MIT License# https://github.com/IntelRealSense/librealsense/blob/master/doc/installation_jetson.md# Register the server's public key:sudo apt-key adv --keyserver keys.gnupg.net --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDE || sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key # Ubuntu 18 is bionicsudo add-apt-repository \"deb http://realsense-hw-public.s3.amazonaws.com/Debian/apt-repo bionic main\" -u sudo apt-get install apt-utils -ysudo apt-get install librealsense2-utils librealsense2-dev -y Copy "},{"title":"The Imaging Source USB3.1 IMX Board Cameras","type":0,"sectionRef":"#","url":"docs/hardware/cameras/tiscamera-install/","content":"","keywords":""},{"title":"Camera Driver Installation Guide","type":1,"pageTitle":"The Imaging Source USB3.1 IMX Board Cameras","url":"docs/hardware/cameras/tiscamera-install/#camera-driver-installation-guide","content":"The camera driver has two components tiscamera which is the core drivertiscamera-dutil which contains many additional useful features, such as tonemapping "},{"title":"Step 1/3: Install tiscamera Core Driver","type":1,"pageTitle":"The Imaging Source USB3.1 IMX Board Cameras","url":"docs/hardware/cameras/tiscamera-install/#step-13-install-tiscamera-core-driver","content":"For Intel-based processor, you may choose to install the official compiled tiscamera package from here: https://github.com/TheImagingSource/tiscamera/releases note As of Jan 2021, we are using version 0.12.0For TX2 or Xavier, if cannot build from source, can try to install the official compiled tiscamera package tiscamera_0.12.0_arm64.deb directly. However, it is strongly recommanded to build from source, for the main tiscamera driver, especially on Nvidia arm64 platform. "},{"title":"Build tiscamera From Source","type":1,"pageTitle":"The Imaging Source USB3.1 IMX Board Cameras","url":"docs/hardware/cameras/tiscamera-install/#build-tiscamera-from-source","content":"Install GStreamer through apt first Clone https://github.com/TheImagingSource/tiscamera and checkout the release version, for example v-tiscamera-0.12.0 git clone https://github.com/TheImagingSource/tiscamera.gitgit checkout v-tiscamera-0.12.0 Copy Change CMakeList.txt BUILD_TOOLS to ON (Jetson TX2) Install dependencies gstreamer-1.0 libusb-1.0 libglib2.0 libgirepository1.0-dev libudev-dev libtinyxml-dev libzip-dev libnotify-dev Install python dependencies python3-gi python3-pyqt5 Uninstall existing apt package sudo apt remove tiscamera use CMake build and install: mkdir build && cd buildcmake -DBUILD_ARAVIS=OFF ..makesudo make install Copy "},{"title":"Step 2/3: Install tiscamera-dutils","type":1,"pageTitle":"The Imaging Source USB3.1 IMX Board Cameras","url":"docs/hardware/cameras/tiscamera-install/#step-23-install-tiscamera-dutils","content":"Also install the dutils package here tiscamera-dutils_1.0.0.160https://github.com/chengguizi/tiscamera_ros/tree/master/sdk_debs tip To verify driver installation, connect the camera using usb cable, then run tcam-capture, and there should be an option for tonemapping showing up. Otherwise, the installation is unsuccessful. Also, in gst-inspect-1.0 tcambin, the dutil should have default to be true. "},{"title":"Step 3/3: Integrating with ROS: tiscamera_ros","type":1,"pageTitle":"The Imaging Source USB3.1 IMX Board Cameras","url":"docs/hardware/cameras/tiscamera-install/#step-33-integrating-with-ros-tiscamera_ros","content":"The ROS driver could be found at https://github.com/chengguizi/tiscamera_ros/ Dependencies: mavlink (install by apt ros-melodic-mavlink)mavros (https://github.com/chengguizi/mavros, monotonic branch) There are two files important for configuration, in the launch folder the device_list file, which defines the camera string name, as well as serial number hardware_sync_mode set to none if no hardware synchronisation is usedif hardware sync is used, it should set to slave the param file, which sets the exposure settings for all cameras "},{"title":"Wiring for Hardware Synchronisation","type":1,"pageTitle":"The Imaging Source USB3.1 IMX Board Cameras","url":"docs/hardware/cameras/tiscamera-install/#wiring-for-hardware-synchronisation","content":""},{"title":"Steps in preparing an External SD Card with the right permissions for TX2","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/external-sd-card/","content":"","keywords":""},{"title":"To clear ACL settings","type":1,"pageTitle":"Steps in preparing an External SD Card with the right permissions for TX2","url":"docs/hardware/jetson-tx2/external-sd-card/#to-clear-acl-settings","content":"setfacl -b nvidia Regarding Removable Devices Permissions The SD Card should be formated as ext4 filesystem, as it supports standard Linux permission flags, in contrast to FAT  Like any unix-style filesystem, ext4 includes standard Unix file ownership and permission conventions. That is, the user is identified by an UID number, and each user will belong to one or more groups, each group identified by its GID number. Each file has an owner UID and one group owner GID. The three classic Unix file permission sets are: one set of permissions for the owner, identified by the owner's UID number one set of permissions for the group owner, identified by the group's GID number one set of permissions for everyone else In order to be able to access the stick without needing to adjust permissions, you must make sure any files and directories created on the stick will have non-restrictive permissions automatically. The problem is, permissions on any new files created are controlled by the umask value... and you don't really want to keep changing it to 000 for creating files on the USB stick and back to the default value (usually 002 or 022) for normal use. A single mistake could lead you creating an important configuration file with wide-open permissions, that might compromise the security of your user account or cause other more minor problems. If you can make sure that your normal user's UID number is the same across all your Linux systems, and you only care about access for that one user (plus root of course), you can get away with just formatting the USB stick to ext4, mounting it for the first time, and assigning the ownership of its root directory to your regular user account before you begin using the filesystem. Assuming that /dev/sdX1 is the USB stick partition you wish to create the filesystem in, and <username> is your username, you can do this when setting up the USB stick for use: sudo mkfs.ext4 /dev/sdX1sudo mount /dev/sdX1 /mntsudo chown <username>: /mntsudo umount /mnt But if you cannot guarantee matching UID/GID numbers, and/or there are multiple users who might want to use the USB stick, you'll need to do something a bit more complicated, but still an one-time operation after creating the ext4 filesystem on the stick. We need to set a default ACL on the root directory of the USB stick filesystem that assigns full access to everyone on any new file or directory. And to ensure that the stick will be mounted with ACL support enabled, we need to use tune2fs to adjust the default mount options stored in the filesystem metadata. sudo mkfs.ext4 /dev/sdX1sudo tune2fs -o acl /dev/sdX1sudo mount /dev/sdX1 /mntsudo chown <username>: /mntchmod 777 /mntsetfacl -m d:u::rwx,d:g::rwx,d:o::rwx /mntsudo umount /mnt Assuming that all your systems support ACLs on ext4 filesystems, and that any removable media mounting tool you might use won't choose to ignore the acl mount option, you now should have a USB stick on which all files created on it will have permissions -rw-rw-rw- and all created sub-directories will be drwxrwxrwx+. The plus sign indicates that the sub-directory will have an ACL: the custom default permission set configured for the stick's root directory will be inherited by the sub-directories too, and they will behave the same. The owner UID/GID will still match the UID and primary GID of the user that created the file on the filesystem, but because of relaxed file and directory permissions, that should not be much of an issue. The only problem I might expect is that copying files to the USB stick will by default attempt to duplicate the file permissions of the original, which you don't want in this case. For example, if you create a file on System A with permissions -rw-r--r-- and copy it to the stick, then move the stick to System B with non-matching UID numbers. You can still read the file on System B, but you cannot overwrite it on the stick without first explicitly deleting or renaming the original file. But you can do that, as long as you have write access to the directory the file's in. This can actually be an useful feature: if you modify the same file on multiple systems, this will push you towards saving a new version of the file each time instead of overwriting the One True File... and if the file is important, that might actually be a good thing. Copy "},{"title":"Backup Dos","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/backup-procedures/","content":"","keywords":""},{"title":"How to save space before you back up","type":1,"pageTitle":"Backup Dos","url":"docs/hardware/jetson-tx2/backup-procedures/#how-to-save-space-before-you-back-up","content":"Clear trash and caches at ~/.cache/chromium/Defaultat ~/.ros'pip3 cache purge'Lastly, at ~/.local/share/Trash/expunged/* To check large files and directories sudo du -Sh | sort -rh | head -20 To clear unallocated disk space to zero, use zerofree utility (apt install) boot into single user mode sudo init 1 (init level 3 may not work. If no keyboard response, try switch to init 3 first and then into init 1 state, after a fresh reboot) remount the filesystem to readonly (if failed, reboot the system and try again) mount -o remount,ro /dev/mmcblk0p1 If the mounting is busy try stop services as followedkillall dhclient (this should make things work already)systemctl stop rsyslogsystemctl stop network-managerif still, run systemctl status to identify further things to stop perform zerofree command zerofree -v /dev/mmcblk0p1 regarding the meaning of the zerofree output: So after some detailed analysis it would look like those numbers are as follows:- number of nonzero blocks encountered- number of free blocks within the filesystem- total number of blocks within the filesystem Copy poweroff and start backup in recovery mode "},{"title":"Backup the Image File","type":1,"pageTitle":"Backup Dos","url":"docs/hardware/jetson-tx2/backup-procedures/#backup-the-image-file","content":"In the Linux_for_Tegra/ folder, perform backup of the APP partition sudo ./flash.sh -r -k APP -G backup.img jetson-tx2 mmcblk0p1 "},{"title":"Creating Compressed File (tar.xz)","type":1,"pageTitle":"Backup Dos","url":"docs/hardware/jetson-tx2/backup-procedures/#creating-compressed-file-tarxz","content":"sudo XZ_OPT=-T0 tar -Jcvf xxx.tar.xz Linux_for_Tegra sudo is used to be able to access all files-T0 is to enable multi-threading (tar -Jcvf file.tar.xz /path is too slow, as it is single threaded)  References:Official Flashing Guide "},{"title":"Flashing An Existing Image to TX2","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/flash-existing-image/","content":"","keywords":""},{"title":"Pre-requisite","type":1,"pageTitle":"Flashing An Existing Image to TX2","url":"docs/hardware/jetson-tx2/flash-existing-image/#pre-requisite","content":"A host machine running Ubuntu 16 or 18Prepared the Patched L4T BSP (e.g. patched with Auvidea's Firmware)Have the .img file readyA bare TX2 moduleA Dev Kit with the stock 19V supplyA micro USB cable for flashingHDMI Monitor and cable(Optionally) A Ethernet cable or WiFi Antenna connected, for updating the Ubuntu packages to the latest "},{"title":"Flashing Steps","type":1,"pageTitle":"Flashing An Existing Image to TX2","url":"docs/hardware/jetson-tx2/flash-existing-image/#flashing-steps","content":"warning Make sure ./apply_binaries.sh is executed once before ANY flashing, after a BSP update / overwrite from the board overlay. Make sure patched BSP is ready. If it is prepared by someone else in a zip file. Make sure it is unzipped with permission preserved sudo tar -xpf *** Power off the dev kit The CR5 red LED should not light up. Otherwise, removing or installing TX2 module while the dev kit is powered may damage the system electrically Install the TX2 module firmly, and plug-in the power for dev kit Boot TX2 into recovery mode You could enter the recovery mode by PWR - press and release (Skip if the board is already powered on)REC - press and holdRST - press and release Wait 2 sec REC - release (Advanced) if not successful, you could still enter by interrupting the bootloader in the UART0 console, and by typing enterrcm  With the OTG micro-USB attached (hidden on the left of the antennas) to the host. Check the connection by opening a terminal, key in lsusb Copy It should show ID 0955:7c18 NVidia Corp. The image to be flashed should be symlinked as ./bootloader/system.img For example: cd bootloader and sudo ln -s ../../../images/backup_190413.img system.imgIt should always be named system.img Go back to the root of the BSP folder, flash all board partitions by sudo ./flash.sh -r jetson-tx2 mmcblk0p1 # flash all partitions # Below only for reference# sudo ./flash.sh -k kernel-dtb jetson-tx2 mmcblk0p1 # flash kernel device tree-blob# other -k options: cpu-bootloader, bootloader-dtb, LNX, kernel Copy The process should end with *** The target t186ref has been flashed successfully. *** You are done! Sanity check with sha1sum –c /etc/nv_tegra_release Nvidia Official Reference: Flashing and Booting the Target Device Jetpack ArchiveL4T Archive "},{"title":"Hotspot for TX2","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/hotspot-setup/","content":"","keywords":""},{"title":"To enable broadcast of SSID","type":1,"pageTitle":"Hotspot for TX2","url":"docs/hardware/jetson-tx2/hotspot-setup/#to-enable-broadcast-of-ssid","content":"This is needed to turn on hotspot functionecho 2 > /sys/module/bcmdhd/parameters/op_mode "},{"title":"To disable broadcast, and allow normal wifi connection","type":1,"pageTitle":"Hotspot for TX2","url":"docs/hardware/jetson-tx2/hotspot-setup/#to-disable-broadcast-and-allow-normal-wifi-connection","content":"echo 0 > /sys/module/bcmdhd/parameters/op_mode "},{"title":"To make things persistent","type":1,"pageTitle":"Hotspot for TX2","url":"docs/hardware/jetson-tx2/hotspot-setup/#to-make-things-persistent","content":"nano /etc/modprobe.d/bcmdhd.confadd line options bcmdhd op_mode=2 "},{"title":"To change IP address for NetworkManager","type":1,"pageTitle":"Hotspot for TX2","url":"docs/hardware/jetson-tx2/hotspot-setup/#to-change-ip-address-for-networkmanager","content":"add line address1=192.168.1.150/24 to file /etc/NetworkManager/system-connections/<corresponding_file> in the [ipv4] section "},{"title":"Mount .img.raw as Read-write Loopback Device","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/mount-image-as-loopback/","content":"","keywords":""},{"title":"Regarding Sparse Image (*.img.raw)","type":1,"pageTitle":"Mount .img.raw as Read-write Loopback Device","url":"docs/hardware/jetson-tx2/mount-image-as-loopback/#regarding-sparse-image-imgraw","content":"Historically there was not always a sparse (compressed image), but due to how long it took to copy over USB2 the sparse image had support added. Prior to sparse images the flash program just used the raw image directly. The current flash program can still use an uncompressed/raw image. The order of dealing with flash images is that \"bootloader/system.img\" is created; this is then moved to \"bootloader/system.img.raw\", and then system.img.raw is used to compress into system.img. flash.sh does not care if system.img is raw or sparse, it works. (post) To make the sparse .img out of .img.raw, use the following command ./mksparse -v --fillpattern=0 system.img.raw system.img "},{"title":"Miscellaneous","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/misc/","content":"","keywords":""},{"title":"To Add A New IP address to network of machine (only when needed)","type":1,"pageTitle":"Miscellaneous","url":"docs/hardware/jetson-tx2/misc/#to-add-a-new-ip-address-to-network-of-machine-only-when-needed","content":"To add temporarily: ip addr add 192.168.1.149/24 broadcast 192.168.1.255 dev wlan0 label wlan0:1 "},{"title":"Ghostwriter Markdown Viewer","type":1,"pageTitle":"Miscellaneous","url":"docs/hardware/jetson-tx2/misc/#ghostwriter-markdown-viewer","content":"git clone https://github.com/wereturtle/ghostwriterFollow the build instructions in readmeqmake, make, make install "},{"title":"Check DTS file version","type":1,"pageTitle":"Miscellaneous","url":"docs/hardware/jetson-tx2/misc/#check-dts-file-version","content":"dmesg | grep \"DTS File Name\" "},{"title":"Known Issues","type":1,"pageTitle":"Miscellaneous","url":"docs/hardware/jetson-tx2/misc/#known-issues","content":"Fans is not working properly on Jetpack 4.2 Link "},{"title":"GPIO","type":1,"pageTitle":"Miscellaneous","url":"docs/hardware/jetson-tx2/misc/#gpio","content":"https://elixir.bootlin.com/linux/latest/source/include/dt-bindings/gpio/tegra-gpio.h#L49 "},{"title":"Set Power Button to Shutdown","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/power-button/","content":"","keywords":""},{"title":"Not working ones","type":1,"pageTitle":"Set Power Button to Shutdown","url":"docs/hardware/jetson-tx2/power-button/#not-working-ones","content":"In dconf tool or in gsetting commandline: gsettings set org.gnome.settings-daemon.plugins.power button-power 'shutdown' or In dconf tool or in gsetting commandline: gsettings set org.gnome.settings-daemon.plugins.power power-button-action shutdown "},{"title":"Pytorch for TX2","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/pytorch-install/","content":"","keywords":""},{"title":"Version for Python 2","type":1,"pageTitle":"Pytorch for TX2","url":"docs/hardware/jetson-tx2/pytorch-install/#version-for-python-2","content":"The last PyTorch Version to support Python 2 is version 1.4, The The address for Installing v1.4 for Jetpack 4.2/4.3 is https://nvidia.box.com/shared/static/1v2cc4ro6zvsbu0p8h6qcuaqco1qcsif.whl "},{"title":"Installation instruction","type":1,"pageTitle":"Pytorch for TX2","url":"docs/hardware/jetson-tx2/pytorch-install/#installation-instruction","content":"Run the following wget https://nvidia.box.com/shared/static/1v2cc4ro6zvsbu0p8h6qcuaqco1qcsif.whl -O torch-1.4.0-cp27-cp27mu-linux_aarch64.whlsudo apt-get install libopenblas-base libopenmpi-dev pip install torch-1.4.0-cp27-cp27mu-linux_aarch64.whl Copy Python dependencies pip install futurepip install torchvision --user Copy Install torchvision sudo apt-get install libjpeg-dev zlib1g-devgit clone --branch v0.5.0 https://github.com/pytorch/vision torchvisioncd torchvisionsudo python setup.py installcd ../ # attempting to load torchvision from build dir will result in import errorpip install 'pillow<7' # not needed for torchvision v0.5.0+ Copy Reference "},{"title":"Preparing The Patched BSP","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/prepare-bsp-rfs/","content":"","keywords":""},{"title":"For Auvidea J120","type":1,"pageTitle":"Preparing The Patched BSP","url":"docs/hardware/jetson-tx2/prepare-bsp-rfs/#for-auvidea-j120","content":""},{"title":"What You Need:","type":1,"pageTitle":"Preparing The Patched BSP","url":"docs/hardware/jetson-tx2/prepare-bsp-rfs/#what-you-need","content":"Check the latest firmware version from Auvidea Firmware, and download the zip file.Download the corresponding L4T BSP (inside which provides Linux kernel, bootloader, NVIDIA drivers, and flashing utilities) "},{"title":"The Steps","type":1,"pageTitle":"Preparing The Patched BSP","url":"docs/hardware/jetson-tx2/prepare-bsp-rfs/#the-steps","content":"Unzip the BSP and Sample Root Filesystem, using command line to preserve the correct permissions sudo tar -xpf <BSP zip file>sudo tar -xpf <Sample Root Filesystem zip file># use sudo cp -a to copy the Sample Root Filesystem to rootfs folder inside BSPsudo cp -a <Sample Root Filesystem>/* <BSP>/rootfs Copy Rename the extracted Linux_for_Tegra folder, to indicate its target Auvidea board and firmware version. Such as Linux_for_Tegra_J120_Dec2019 Extract the corresponding J120 / J90 Firmware sudo tar -xpf <Firmware zip file> Copy Navigate into the Auvidea firmware's Linux_for_Tegra folder, copy the files to overide Nvidia's Original, preserving permissions sudo cp -a * <BSP Root Folder> Copy Double check by ensureing all relevant files in the BSP's folder is the same modification dates as the Auvidea's Firmware folder. Apply binaries sudo ./apply_binaries.sh Copy You are done! "},{"title":"Flashing Your First Image (Clean Install)","type":1,"pageTitle":"Preparing The Patched BSP","url":"docs/hardware/jetson-tx2/prepare-bsp-rfs/#flashing-your-first-image-clean-install","content":"to flash every partitions with fresh build of system.img: sudo ./flash.sh jetson-tx2 mmcblk0p1 Copy tip Addtional Nvidia Libraries (e.g. CUDA, cuDNN), could be installed by Nvidia's SDK Manager, by Boot up TX2 into UbuntuPlug in the debugging USB to host Linux machine; TX2 should be mounted as a network device automatically with 192.168.55.1 addressThereafter, the SDK Manager could do the install automatically "},{"title":"Post-flashing Dos","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/tx2-post-flashing/","content":"","keywords":""},{"title":"Check L4T and Jetpack version","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#check-l4t-and-jetpack-version","content":"-cat /etc/nv_tegra_release, e.g. R32.3.2 is the L4T Version "},{"title":"Install Absolute Dependencies","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#install-absolute-dependencies","content":"sudo apt update to the latestInstall CUDA and cuDNN etc. from Nvidia SDK Manager, over the USB Cable (check install by nvcc --version and ~/.bashrc export 2 lines are there)Install ROS, and apt install python-catkin-tools python-rosdep; do sudo rosdep initSetup SD Card /etc/fstabAdd user nvidia to dialout group, to access tty serial port normallyInstall pytorch and torchvisionInstall ZED SDKsudo apt install ros-melodic-mavlink "},{"title":"Downgrade OpenCV","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#downgrade-opencv","content":"For new Jetpack, the OpenCV version is 4.1+. Whereas ROS 1 needs Version 3.x to function. Hence, downgrade by using Nvidia's SDK Manager's .deb file , e.g. libopencv_3.3.1-2-g31ccdfe11_arm64.deblibopencv-dev_3.3.1-2-g31ccdfe11_arm64.deblibopencv-python_3.3.1-2-g31ccdfe11_arm64.deblibopencv-samples_3.3.1-2-g31ccdfe11_arm64.deb "},{"title":"Install Optional Dependencies","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#install-optional-dependencies","content":"For Pangolin OpenGL Display sudo apt install libgl1-mesa-dev libglew-devsudo apt install libegl1-mesa-dev libwayland-dev libxkbcommon-dev wayland-protocolssudo python -mpip install numpy pyopengl Pillow pybind11 librealsense for Jetson from official "},{"title":"Python Dependencies","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#python-dependencies","content":"pip install --upgrade pip, same to pip3pip install and pip3 install the following cpython (may try pip install --upgrade)numpyscipy (need gfortran compiler) "},{"title":"Convenience","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#convenience","content":"Add right-click to create new file: touch ~/Templates/Empty\\ DocumentAdd system monitor applet at status bar (gnome extension)Setting of power button to shutdown without prompt of 60 secondsDo not blank screen display "},{"title":"Tools","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#tools","content":"Install VS CodeInstall SambaInstall git, and configure timeout for asking password git config --global credential.helper 'cache --timeout=300'catkin build, make it default Release mode catkin config --cmake-args -DCMAKE_BUILD_TYPE=Release "},{"title":"Hardware / Kernel","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#hardware--kernel","content":"Increase USBFS buffer size (uboot at /boot/extlinux/extlinux.conf) "},{"title":"Web Applications","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#web-applications","content":"Nodejs and yarnNginx sudo apt install nginx "},{"title":"OpenCV 4 from Source","type":1,"pageTitle":"Post-flashing Dos","url":"docs/hardware/jetson-tx2/tx2-post-flashing/#opencv-4-from-source","content":"It is required, if contrib features of OpenCV is to be used. Does not come with shipped .debbuild it from source, using a external ext4 disk drive, as large space is neededfor TX2, the CUDA_ARCH_BIN version is 6.2 "},{"title":"Use UART2 on Jetson TX2","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/uart-setup/","content":"","keywords":""},{"title":"Use tty ports without super user","type":1,"pageTitle":"Use UART2 on Jetson TX2","url":"docs/hardware/jetson-tx2/uart-setup/#use-tty-ports-without-super-user","content":"sudo adduser nvidia dialout "},{"title":"Auvidea J120","type":1,"pageTitle":"Use UART2 on Jetson TX2","url":"docs/hardware/jetson-tx2/uart-setup/#auvidea-j120","content":"UART2 on Auvidea J120 Rev 6, could be accessed from /dev/ttyTHS1 Minicom Setting OPTI+-----------------------------------------------------------------------+Comp| A - Serial Device : /dev/ttyTHS1 |Port| B - Lockfile Location : /var/lock | | C - Callin Program : |Pres| D - Callout Program : | | E - Bps/Par/Bits : 921600 8N1 |dffd| F - Hardware Flow Control : No | | G - Software Flow Control : No | | | | Change which setting? | +-----------------------------------------------------------------------+ Copy "},{"title":"USB3.0 Availability","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/usb-setup/","content":"","keywords":""},{"title":"Buffer Increase","type":1,"pageTitle":"USB3.0 Availability","url":"docs/hardware/jetson-tx2/usb-setup/#buffer-increase","content":"Temporarysudo sh -c 'echo 1000 > /sys/module/usbcore/parameters/usbfs_memory_mb'Permanent in /etc/default/grubGRUB_CMDLINE_LINUX_DEFAULT=\"usbcore.usbfs_memory_mb=1000\" for TX2, this should lie in /boot/extlinux/extlinux.conf. Add after ${cbootargs} usbcore.usbfs_memory_mb=1000 "},{"title":"Jetson Xavier NX","type":0,"sectionRef":"#","url":"docs/hardware/jetson-xavier-nx/getting-started/","content":"","keywords":""},{"title":"Getting started: Installation of operation system(JetPack 4.4.1) on Windows","type":1,"pageTitle":"Jetson Xavier NX","url":"docs/hardware/jetson-xavier-nx/getting-started/#getting-started-installation-of-operation-systemjetpack-441-on-windows","content":""},{"title":"Download the required software and files","type":1,"pageTitle":"Jetson Xavier NX","url":"docs/hardware/jetson-xavier-nx/getting-started/#download-the-required-software-and-files","content":"Download Jetson Xavier NX Developer Kit SD Card ImageDownload SD Card Formatter Select card driveSelect “Quick format”Leave “Volume label” blankClick “Format” to start formatting, and “Yes” on the warning dialog Download Etcher Click “Select image” and choose the zipped image file downloaded earlier.Insert your microSD cardClick “Select drive” and choose the correct deviceClick “Flash!” It will take Etcher about 15 minutes After the microSD card is ready, it can be plugged into Xavier "},{"title":"VS Code for TX2","type":0,"sectionRef":"#","url":"docs/hardware/jetson-tx2/vscode-install/","content":"VS Code for TX2 Download the deb file from https://packagecloud.io/headmelted/codebuilds (Download button at top-right corner) if have error in apt update, run wget -qO - https://packagecloud.io/headmelted/codebuilds/gpgkey | sudo apt-key add -","keywords":""},{"title":"Install OpenCV","type":0,"sectionRef":"#","url":"docs/hardware/jetson-xavier-nx/OpenCV/","content":"Install OpenCV","keywords":""},{"title":"ECL EKF Initialisation","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-initialisation/","content":"","keywords":""},{"title":"Software Architecture Overview","type":1,"pageTitle":"ECL EKF Initialisation","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-initialisation/#software-architecture-overview","content":""},{"title":"Multi-EKF and Sensor Voting","type":1,"pageTitle":"ECL EKF Initialisation","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-initialisation/#multi-ekf-and-sensor-voting","content":"The EKF logic is wrapped around by class EKF2, which is implemented as a ScheduledWorkItem. It is initialised by the static method task_spawn. The reason for this wrapper architecture is that, the EKF2 class can turn on a mode called multi mode, which enables multi-EKF support. This essentially runs one EKF instance per physical IMU detected on the hardware platform. With the multi mode enabled, the ekf selector is also run to perform the selection logic.  note ScheduledWorkItem inherits WorkItem which contains private object WorkQueue. Given the wq_config_t struct in the constructor of WorkItem, it will find or create the relevant Work Queue by WorkQueueFindOrCreate(config), then attach the current work item class object this to the target work queue by Attach(this). Here each work queue is identified by name, in struct wq_config_t.name. For more info refer to here. Mode Selection# The multi-EKF mode is enabled when SENS_IMU_MODE is set to 0 (disabled), reference. Setting the value to 0 disable Sensors hub IMU, hence to expose all IMU data out. Otherwise, setting SENS_IMU_MODE to 1, enables sensor voting, in which disable multi-EKF support. The sensor voting result is published as the uORB sensor_combined message (VotedSensorsUpdate::sensorsPoll). It is received by _sensor_combined_sub in EKF2 class (non multi-EKF mode). tip Currently, we are using the default value of SENS_IMU_MODE which is 1. We are not using multi-EKF mode now. "},{"title":"Bootstraping Process (Non Multi-EKF)","type":1,"pageTitle":"ECL EKF Initialisation","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-initialisation/#bootstraping-process-non-multi-ekf","content":"ekf2_main C method, which handles help, start, stop parameters.EKF2::task_spawn(), checking SENS_IMU_MODE. If it is 1 (default), enter normal non multi-EKF mode. A new EKF2 instance is allocated. Inside the spawning, it schedule the EKF2::Run() as well EKF2 has a member variable of class Ekf, which is the actuall ECL library (external to PX4 firmware itself)EKF2::Run() On the first run, it will register itself to the callback of sensor_combined message: _sensor_combined_sub.registerCallback(). Hence the Run() function will be called everytime a new combined IMU message is available. "},{"title":"IMU Update Routine","type":1,"pageTitle":"ECL EKF Initialisation","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-initialisation/#imu-update-routine","content":"As we have discussed the EKF2::Run() function is called whenever a new IMU data is available (callback). For each run: update status like _armed, _standbymaintain _preflt_checker, which is updated during stand by mode _preflt_checker is responsible for the xy_valid, z_valid, v_xy_valid, v_z_valid entries in vehicle_local_position uORB message. PreFlightChecker.cpp void PreFlightChecker::update(const float dt, const estimator_innovations_s &innov){ const float alpha = InnovationLpf::computeAlphaFromDtAndTauInv(dt, _innov_lpf_tau_inv); _has_heading_failed = preFlightCheckHeadingFailed(innov, alpha); _has_horiz_vel_failed = preFlightCheckHorizVelFailed(innov, alpha); _has_vert_vel_failed = preFlightCheckVertVelFailed(innov, alpha); _has_height_failed = preFlightCheckHeightFailed(innov, alpha);} Copy call _ekf.setIMUData(imu_sample_new) and then _ekf.update()Among other things, update sensors sample too  UpdateAirspeedSample(ekf2_timestamps); UpdateAuxVelSample(ekf2_timestamps); UpdateBaroSample(ekf2_timestamps); UpdateGpsSample(ekf2_timestamps); UpdateMagSample(ekf2_timestamps); UpdateRangeSample(ekf2_timestamps); vehicle_odometry_s ev_odom; const bool new_ev_odom = UpdateExtVisionSample(ekf2_timestamps, ev_odom); optical_flow_s optical_flow; const bool new_optical_flow = UpdateFlowSample(ekf2_timestamps, optical_flow); Copy "},{"title":"Yaw Alignment","type":1,"pageTitle":"ECL EKF Initialisation","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-initialisation/#yaw-alignment","content":"Inside bool Ekf::initialiseFilter(), yaw alignment is attempted:  // calculate the initial magnetic field and yaw alignment _control_status.flags.yaw_align = resetMagHeading(_mag_lpf.getState(), false, false); Copy If the type is MAG_FUSE_TYPE_NONE, then the yawalign will return false. If the type is MAG_FUSE_TYPE_INDOOR (`is_yaw_fusion_inhibited would be false at initialisation) "},{"title":"ECL EKF State Reset Logics","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-state-reset/","content":"","keywords":""},{"title":"Possible Resets and Updated Flag","type":1,"pageTitle":"ECL EKF State Reset Logics","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-state-reset/#possible-resets-and-updated-flag","content":"The reset logics for modern PX4 ECL EKF resides in ekf_helper.cpp. All resets are tracked by _state_reset_status struct, stored within uORB vehicle_local_position message (PublishLocalPosition()). Reset Logic\tUpdated Flag\tDelta Frame\tuORB\tMAVLinkresetHorizontalVelocityTo()\tvelNE_change, velNE_counter\tlocal NED\tdelta_vxy, vxy_reset_counter\tNIL resetVerticalVelocityTo()\tvelD_change, velD_counter\tlocal NED\tdelta_vz, vz_reset_counter\tNIL resetHorizontalPositionTo()\tposNE_change, posNE_counter\tlocal NED\tdelta_xy, xy_reset_counter\tNIL resetHeight()\tposD_change , posD_counter\tlocal NED\tdelta_z, z_reset_counter\tNIL resetQuatStateYaw()\tquat_change, quat_counter\tbody frame?\tdelta_heading, heading_reset_counter\tNIL struct { uint8_t velNE_counter; ///< number of horizontal position reset events (allow to wrap if count exceeds 255) uint8_t velD_counter; ///< number of vertical velocity reset events (allow to wrap if count exceeds 255) uint8_t posNE_counter; ///< number of horizontal position reset events (allow to wrap if count exceeds 255) uint8_t posD_counter; ///< number of vertical position reset events (allow to wrap if count exceeds 255) uint8_t quat_counter; ///< number of quaternion reset events (allow to wrap if count exceeds 255) Vector2f velNE_change; ///< North East velocity change due to last reset (m) float velD_change; ///< Down velocity change due to last reset (m/sec) Vector2f posNE_change; ///< North, East position change due to last reset (m) float posD_change; ///< Down position change due to last reset (m) Quatf quat_change; ///< quaternion delta due to last reset - multiply pre-reset quaternion by this to get post-reset quaternion} _state_reset_status{}; ///< reset event monitoring structure containing velocity, position, height and yaw reset information Copy Suggestion: shall we expose the information to MAVLink for planning algorithms to work with? "},{"title":"Reference: vehicle_local_position.msg","type":1,"pageTitle":"ECL EKF State Reset Logics","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-state-reset/#reference-vehicle_local_positionmsg","content":"# Fused local position in NED. uint64 timestamp # time since system start (microseconds)uint64 timestamp_sample # the timestamp of the raw data (microseconds) bool xy_valid # true if x and y are validbool z_valid # true if z is validbool v_xy_valid # true if vy and vy are validbool v_z_valid # true if vz is valid # Position in local NED framefloat32 x # North position in NED earth-fixed frame, (metres)float32 y # East position in NED earth-fixed frame, (metres)float32 z # Down position (negative altitude) in NED earth-fixed frame, (metres) # Position reset deltafloat32[2] delta_xyuint8 xy_reset_counter float32 delta_zuint8 z_reset_counter # Velocity in NED framefloat32 vx # North velocity in NED earth-fixed frame, (metres/sec)float32 vy # East velocity in NED earth-fixed frame, (metres/sec)float32 vz # Down velocity in NED earth-fixed frame, (metres/sec)float32 z_deriv # Down position time derivative in NED earth-fixed frame, (metres/sec) # Velocity reset deltafloat32[2] delta_vxyuint8 vxy_reset_counter float32 delta_vzuint8 vz_reset_counter# Acceleration in NED framefloat32 ax # North velocity derivative in NED earth-fixed frame, (metres/sec^2)float32 ay # East velocity derivative in NED earth-fixed frame, (metres/sec^2)float32 az # Down velocity derivative in NED earth-fixed frame, (metres/sec^2) float32 heading # Euler yaw angle transforming the tangent plane relative to NED earth-fixed frame, -PI..+PI, (radians)float32 delta_headinguint8 heading_reset_counter # Position of reference point (local NED frame origin) in global (GPS / WGS84) framebool xy_global # true if position (x, y) has a valid global reference (ref_lat, ref_lon)bool z_global # true if z has a valid global reference (ref_alt)uint64 ref_timestamp # Time when reference position was set since system start, (microseconds)float64 ref_lat # Reference point latitude, (degrees)float64 ref_lon # Reference point longitude, (degrees)float32 ref_alt # Reference altitude AMSL, (metres) # Distance to surfacefloat32 dist_bottom # Distance from from bottom surface to ground, (metres)bool dist_bottom_valid # true if distance to bottom surface is valid float32 eph # Standard deviation of horizontal position error, (metres)float32 epv # Standard deviation of vertical position error, (metres)float32 evh # Standard deviation of horizontal velocity error, (metres/sec)float32 evv # Standard deviation of horizontal velocity error, (metres/sec) # estimator specified vehicle limitsfloat32 vxy_max # maximum horizontal speed - set to 0 when limiting not required (meters/sec)float32 vz_max # maximum vertical speed - set to 0 when limiting not required (meters/sec)float32 hagl_min # minimum height above ground level - set to 0 when limiting not required (meters)float32 hagl_max # maximum height above ground level - set to 0 when limiting not required (meters) # TOPICS vehicle_local_position vehicle_local_position_groundtruth# TOPICS estimator_local_position Copy "},{"title":"ECL EKF Yaw Fusion Logics","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-yaw-fusion/","content":"","keywords":""},{"title":"Magnetometer Yaw Fusion","type":1,"pageTitle":"ECL EKF Yaw Fusion Logics","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-yaw-fusion/#magnetometer-yaw-fusion","content":"The type of fusion is controlled by the px4 parameter mag_fusion_type in code or equivalently EKF2_MAG_TYPE in parameter name. /** * Type of magnetometer fusion * * Integer controlling the type of magnetometer fusion used - magnetic heading or 3-component vector. The fuson of magnetomer data as a three component vector enables vehicle body fixed hard iron errors to be learned, but requires a stable earth field. * If set to 'Automatic' magnetic heading fusion is used when on-ground and 3-axis magnetic field fusion in-flight with fallback to magnetic heading fusion if there is insufficient motion to make yaw or magnetic field states observable. * If set to 'Magnetic heading' magnetic heading fusion is used at all times * If set to '3-axis' 3-axis field fusion is used at all times. * If set to 'VTOL custom' the behaviour is the same as 'Automatic', but if fusing airspeed, magnetometer fusion is only allowed to modify the magnetic field states. This can be used by VTOL platforms with large magnetic field disturbances to prevent incorrect bias states being learned during forward flight operation which can adversely affect estimation accuracy after transition to hovering flight. * If set to 'MC custom' the behaviour is the same as 'Automatic, but if there are no earth frame position or velocity observations being used, the magnetometer will not be used. This enables vehicles to operate with no GPS in environments where the magnetic field cannot be used to provide a heading reference. Prior to flight, the yaw angle is assumed to be constant if movement tests controlled by the EKF2_MOVE_TEST parameter indicate that the vehicle is static. This allows the vehicle to be placed on the ground to learn the yaw gyro bias prior to flight. * If set to 'None' the magnetometer will not be used under any circumstance. If no external source of yaw is available, it is possible to use post-takeoff horizontal movement combined with GPS velocity measurements to align the yaw angle with the timer required (depending on the amount of movement and GPS data quality). Other external sources of yaw may be used if selected via the EKF2_AID_MASK parameter. * @group EKF2 * @value 0 Automatic * @value 1 Magnetic heading * @value 2 3-axis * @value 3 VTOL custom * @value 4 MC custom * @value 5 None * @reboot_required true */PARAM_DEFINE_INT32(EKF2_MAG_TYPE, 0); Copy The default automatic mode may not be suitable for indoor-outdoor transition, as the fusion always use magnetometer readings in flight. The main logics is written in controlMagFusion() within file mag_control.cpp. "},{"title":"MAG_FUSE_TYPE_NONE","type":1,"pageTitle":"ECL EKF Yaw Fusion Logics","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-yaw-fusion/#mag_fuse_type_none","content":"The magnetometer is only used pre-flight, for gyro bias estimation. This mode should be used, if we do not want to have yaw drift, when we perform indoor-outdoor transitions "},{"title":"MAG_FUSE_TYPE_INDOOR aka MC custom","type":1,"pageTitle":"ECL EKF Yaw Fusion Logics","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-yaw-fusion/#mag_fuse_type_indoor-aka-mc-custom","content":"This mode will inhibit yaw fusion (_is_yaw_fusion_inhibited sets to true), only if GPS and Vision position / velocity fusion is not used. In our use case, the mode has no practical use. "},{"title":"26/11 Tests","type":1,"pageTitle":"ECL EKF Yaw Fusion Logics","url":"docs/hardware/px4-firmware/ecl-ekf/px4-ecl-yaw-fusion/#2611-tests","content":"4 tests were conducted to test different combinations of magnetometer settings and vision . MC custom & Vision OFFMC custom & Vision ONNone & Vision OFFNone & Vision ON Tests Results: Test 1 MC custom & Vision OFF Test 2 MC custom & Vision ON Test 3 None & Vision OFF Test 4 None & Vsion ON Observations: There is still a yaw drift. EKF2 fusion setting is 321 for these 4 tests. To include GPS yaw fusion, the setting should be changed to 450. "},{"title":"Hardware Camera Triggering using AUX Pins","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/px4-camera-trigger/","content":"Hardware Camera Triggering using AUX Pins","keywords":""},{"title":"MAVLink IMU Topics","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/mavlink-imu/","content":"","keywords":""},{"title":"IMU Data Rates","type":1,"pageTitle":"MAVLink IMU Topics","url":"docs/hardware/px4-firmware/mavlink-imu/#imu-data-rates","content":"HIGHRES_IMU has a default rate limiting at 50Hz, when at ONBOARD (with OBC) or CONFIG (USB) mode. This topic is published whenever any of the field in the message is refreshed. Therefore, the actual rates, where the IMU data relevant to the visual algorithm is updated, might be lower. For example, if the rate is set at 200Hz, the actual rates might only achieve 150Hz observed on the final ROS topic /mavros/imu/data_raw. Therefore, set the data rates to be higher when necessary. "},{"title":"Add extras.txt","type":1,"pageTitle":"MAVLink IMU Topics","url":"docs/hardware/px4-firmware/mavlink-imu/#add-extrastxt","content":"Place the extras.txt file in the etc/ folder within the SD card for Pixhawk. First in the QGC's mavlink console, identify the tty device, by typing mavlink status in the console. Example output is \"/dev/ttyS2\" Using SCALED_IMUx# mavlink stream -d /dev/ttyACM0 -s HIGHRES_IMU -r 0mavlink stream -d /dev/ttyS2 -s HIGHRES_IMU -r 0 Copy Using HIGHRES_IMU (not recommnaded)# mavlink stream -d /dev/ttyACM0 -s HIGHRES_IMU -r 300mavlink stream -d /dev/ttyS2 -s HIGHRES_IMU -r 300 Copy "},{"title":"MAVLINK Defaults in PX4 Firmware","type":1,"pageTitle":"MAVLink IMU Topics","url":"docs/hardware/px4-firmware/mavlink-imu/#mavlink-defaults-in-px4-firmware","content":"mavlink.cpp switch (_mode) { case MAVLINK_MODE_NORMAL: configure_stream_local(\"ADSB_VEHICLE\", unlimited_rate); configure_stream_local(\"ALTITUDE\", 1.0f); configure_stream_local(\"ATTITUDE\", 15.0f); configure_stream_local(\"ATTITUDE_TARGET\", 2.0f); configure_stream_local(\"BATTERY_STATUS\", 0.5f); configure_stream_local(\"CAMERA_IMAGE_CAPTURED\", unlimited_rate); configure_stream_local(\"COLLISION\", unlimited_rate); configure_stream_local(\"DEBUG\", 1.0f); configure_stream_local(\"DEBUG_FLOAT_ARRAY\", 1.0f); configure_stream_local(\"DEBUG_VECT\", 1.0f); configure_stream_local(\"DISTANCE_SENSOR\", 0.5f); configure_stream_local(\"ESTIMATOR_STATUS\", 0.5f); configure_stream_local(\"EXTENDED_SYS_STATE\", 1.0f); configure_stream_local(\"GLOBAL_POSITION_INT\", 5.0f); configure_stream_local(\"GPS2_RAW\", 1.0f); configure_stream_local(\"GPS_RAW_INT\", 1.0f); configure_stream_local(\"HOME_POSITION\", 0.5f); configure_stream_local(\"LOCAL_POSITION_NED\", 1.0f); configure_stream_local(\"NAMED_VALUE_FLOAT\", 1.0f); configure_stream_local(\"NAV_CONTROLLER_OUTPUT\", 1.0f); configure_stream_local(\"OBSTACLE_DISTANCE\", 1.0f); configure_stream_local(\"ORBIT_EXECUTION_STATUS\", 2.0f); configure_stream_local(\"PING\", 0.1f); configure_stream_local(\"POSITION_TARGET_GLOBAL_INT\", 1.0f); configure_stream_local(\"POSITION_TARGET_LOCAL_NED\", 1.5f); configure_stream_local(\"RC_CHANNELS\", 5.0f); configure_stream_local(\"SERVO_OUTPUT_RAW_0\", 1.0f); configure_stream_local(\"SYS_STATUS\", 1.0f); configure_stream_local(\"UTM_GLOBAL_POSITION\", 0.5f); configure_stream_local(\"VFR_HUD\", 4.0f); configure_stream_local(\"WIND_COV\", 0.5f); break; case MAVLINK_MODE_ONBOARD: configure_stream_local(\"ACTUATOR_CONTROL_TARGET0\", 10.0f); configure_stream_local(\"ADSB_VEHICLE\", unlimited_rate); configure_stream_local(\"ALTITUDE\", 10.0f); configure_stream_local(\"ATTITUDE\", 100.0f); configure_stream_local(\"ATTITUDE_QUATERNION\", 50.0f); configure_stream_local(\"ATTITUDE_TARGET\", 10.0f); configure_stream_local(\"BATTERY_STATUS\", 0.5f); configure_stream_local(\"CAMERA_CAPTURE\", 2.0f); configure_stream_local(\"CAMERA_IMAGE_CAPTURED\", unlimited_rate); configure_stream_local(\"CAMERA_TRIGGER\", unlimited_rate); configure_stream_local(\"COLLISION\", unlimited_rate); configure_stream_local(\"DEBUG\", 10.0f); configure_stream_local(\"DEBUG_FLOAT_ARRAY\", 10.0f); configure_stream_local(\"DEBUG_VECT\", 10.0f); configure_stream_local(\"DISTANCE_SENSOR\", 10.0f); configure_stream_local(\"ESTIMATOR_STATUS\", 1.0f); configure_stream_local(\"EXTENDED_SYS_STATE\", 5.0f); configure_stream_local(\"GLOBAL_POSITION_INT\", 50.0f); configure_stream_local(\"GPS2_RAW\", unlimited_rate); configure_stream_local(\"GPS_RAW_INT\", unlimited_rate); configure_stream_local(\"HIGHRES_IMU\", 50.0f); configure_stream_local(\"HOME_POSITION\", 0.5f); configure_stream_local(\"LOCAL_POSITION_NED\", 30.0f); configure_stream_local(\"NAMED_VALUE_FLOAT\", 10.0f); configure_stream_local(\"NAV_CONTROLLER_OUTPUT\", 10.0f); configure_stream_local(\"ODOMETRY\", 30.0f); configure_stream_local(\"OPTICAL_FLOW_RAD\", 10.0f); configure_stream_local(\"ORBIT_EXECUTION_STATUS\", 5.0f); configure_stream_local(\"PING\", 1.0f); configure_stream_local(\"POSITION_TARGET_GLOBAL_INT\", 10.0f); configure_stream_local(\"POSITION_TARGET_LOCAL_NED\", 10.0f); configure_stream_local(\"RC_CHANNELS\", 20.0f); configure_stream_local(\"SERVO_OUTPUT_RAW_0\", 10.0f); configure_stream_local(\"SYS_STATUS\", 5.0f); configure_stream_local(\"SYSTEM_TIME\", 1.0f); configure_stream_local(\"TIMESYNC\", 10.0f); configure_stream_local(\"TRAJECTORY_REPRESENTATION_WAYPOINTS\", 5.0f); configure_stream_local(\"UTM_GLOBAL_POSITION\", 1.0f); configure_stream_local(\"VFR_HUD\", 10.0f); configure_stream_local(\"WIND_COV\", 10.0f); break; case MAVLINK_MODE_EXTVISION: configure_stream_local(\"HIGHRES_IMU\", unlimited_rate); // for VIO configure_stream_local(\"TIMESYNC\", 10.0f); // FALLTHROUGH case MAVLINK_MODE_EXTVISIONMIN: configure_stream_local(\"ADSB_VEHICLE\", unlimited_rate); configure_stream_local(\"ALTITUDE\", 10.0f); configure_stream_local(\"ATTITUDE\", 20.0f); configure_stream_local(\"ATTITUDE_TARGET\", 2.0f); configure_stream_local(\"BATTERY_STATUS\", 0.5f); configure_stream_local(\"CAMERA_IMAGE_CAPTURED\", unlimited_rate); configure_stream_local(\"CAMERA_TRIGGER\", unlimited_rate); configure_stream_local(\"COLLISION\", unlimited_rate); configure_stream_local(\"DEBUG\", 1.0f); configure_stream_local(\"DEBUG_FLOAT_ARRAY\", 1.0f); configure_stream_local(\"DEBUG_VECT\", 1.0f); configure_stream_local(\"DISTANCE_SENSOR\", 10.0f); configure_stream_local(\"ESTIMATOR_STATUS\", 1.0f); configure_stream_local(\"EXTENDED_SYS_STATE\", 1.0f); configure_stream_local(\"GLOBAL_POSITION_INT\", 5.0f); configure_stream_local(\"GPS2_RAW\", 1.0f); configure_stream_local(\"GPS_RAW_INT\", 1.0f); configure_stream_local(\"HOME_POSITION\", 0.5f); configure_stream_local(\"LOCAL_POSITION_NED\", 30.0f); configure_stream_local(\"NAMED_VALUE_FLOAT\", 1.0f); configure_stream_local(\"NAV_CONTROLLER_OUTPUT\", 1.5f); configure_stream_local(\"ODOMETRY\", 30.0f); configure_stream_local(\"OPTICAL_FLOW_RAD\", 1.0f); configure_stream_local(\"ORBIT_EXECUTION_STATUS\", 5.0f); configure_stream_local(\"PING\", 0.1f); configure_stream_local(\"POSITION_TARGET_GLOBAL_INT\", 1.5f); configure_stream_local(\"POSITION_TARGET_LOCAL_NED\", 1.5f); configure_stream_local(\"RC_CHANNELS\", 5.0f); configure_stream_local(\"SERVO_OUTPUT_RAW_0\", 1.0f); configure_stream_local(\"SYS_STATUS\", 5.0f); configure_stream_local(\"TRAJECTORY_REPRESENTATION_WAYPOINTS\", 5.0f); configure_stream_local(\"UTM_GLOBAL_POSITION\", 1.0f); configure_stream_local(\"VFR_HUD\", 4.0f); configure_stream_local(\"WIND_COV\", 1.0f); break; case MAVLINK_MODE_OSD: configure_stream_local(\"ALTITUDE\", 10.0f); configure_stream_local(\"ATTITUDE\", 25.0f); configure_stream_local(\"ATTITUDE_TARGET\", 10.0f); configure_stream_local(\"BATTERY_STATUS\", 0.5f); configure_stream_local(\"ESTIMATOR_STATUS\", 1.0f); configure_stream_local(\"EXTENDED_SYS_STATE\", 1.0f); configure_stream_local(\"GLOBAL_POSITION_INT\", 10.0f); configure_stream_local(\"GPS_RAW_INT\", 1.0f); configure_stream_local(\"HOME_POSITION\", 0.5f); configure_stream_local(\"RC_CHANNELS\", 5.0f); configure_stream_local(\"SERVO_OUTPUT_RAW_0\", 1.0f); configure_stream_local(\"SYS_STATUS\", 5.0f); configure_stream_local(\"SYSTEM_TIME\", 1.0f); configure_stream_local(\"VFR_HUD\", 25.0f); configure_stream_local(\"WIND_COV\", 2.0f); break; case MAVLINK_MODE_MAGIC: /* fallthrough */ case MAVLINK_MODE_CUSTOM: //stream nothing break; case MAVLINK_MODE_CONFIG: // Enable a number of interesting streams we want via USB configure_stream_local(\"ACTUATOR_CONTROL_TARGET0\", 30.0f); configure_stream_local(\"ADSB_VEHICLE\", unlimited_rate); configure_stream_local(\"ALTITUDE\", 10.0f); configure_stream_local(\"ATTITUDE\", 50.0f); configure_stream_local(\"ATTITUDE_QUATERNION\", 50.0f); configure_stream_local(\"ATTITUDE_TARGET\", 8.0f); configure_stream_local(\"BATTERY_STATUS\", 0.5f); configure_stream_local(\"CAMERA_IMAGE_CAPTURED\", unlimited_rate); configure_stream_local(\"CAMERA_TRIGGER\", unlimited_rate); configure_stream_local(\"COLLISION\", unlimited_rate); configure_stream_local(\"DEBUG\", 50.0f); configure_stream_local(\"DEBUG_FLOAT_ARRAY\", 50.0f); configure_stream_local(\"DEBUG_VECT\", 50.0f); configure_stream_local(\"DISTANCE_SENSOR\", 10.0f); configure_stream_local(\"ESTIMATOR_STATUS\", 5.0f); configure_stream_local(\"EXTENDED_SYS_STATE\", 2.0f); configure_stream_local(\"GLOBAL_POSITION_INT\", 10.0f); configure_stream_local(\"GPS2_RAW\", unlimited_rate); configure_stream_local(\"GPS_RAW_INT\", unlimited_rate); configure_stream_local(\"HIGHRES_IMU\", 50.0f); configure_stream_local(\"HOME_POSITION\", 0.5f); configure_stream_local(\"LOCAL_POSITION_NED\", 30.0f); configure_stream_local(\"MANUAL_CONTROL\", 5.0f); configure_stream_local(\"NAMED_VALUE_FLOAT\", 50.0f); configure_stream_local(\"NAV_CONTROLLER_OUTPUT\", 10.0f); configure_stream_local(\"ODOMETRY\", 30.0f); configure_stream_local(\"OPTICAL_FLOW_RAD\", 10.0f); configure_stream_local(\"ORBIT_EXECUTION_STATUS\", 5.0f); configure_stream_local(\"PING\", 1.0f); configure_stream_local(\"POSITION_TARGET_GLOBAL_INT\", 10.0f); configure_stream_local(\"RC_CHANNELS\", 10.0f); configure_stream_local(\"SCALED_IMU\", 25.0f); configure_stream_local(\"SCALED_IMU2\", 25.0f); configure_stream_local(\"SCALED_IMU3\", 25.0f); configure_stream_local(\"SERVO_OUTPUT_RAW_0\", 20.0f); configure_stream_local(\"SERVO_OUTPUT_RAW_1\", 20.0f); configure_stream_local(\"SYS_STATUS\", 1.0f); configure_stream_local(\"SYSTEM_TIME\", 1.0f); configure_stream_local(\"TIMESYNC\", 10.0f); configure_stream_local(\"UTM_GLOBAL_POSITION\", 1.0f); configure_stream_local(\"VFR_HUD\", 20.0f); configure_stream_local(\"WIND_COV\", 10.0f); break; case MAVLINK_MODE_IRIDIUM: configure_stream_local(\"HIGH_LATENCY2\", 0.015f); break; case MAVLINK_MODE_MINIMAL: configure_stream_local(\"ALTITUDE\", 0.5f); configure_stream_local(\"ATTITUDE\", 10.0f); configure_stream_local(\"EXTENDED_SYS_STATE\", 0.1f); configure_stream_local(\"GLOBAL_POSITION_INT\", 5.0f); configure_stream_local(\"GPS_RAW_INT\", 0.5f); configure_stream_local(\"HOME_POSITION\", 0.1f); configure_stream_local(\"NAMED_VALUE_FLOAT\", 1.0f); configure_stream_local(\"RC_CHANNELS\", 0.5f); configure_stream_local(\"SYS_STATUS\", 0.1f); configure_stream_local(\"VFR_HUD\", 1.0f); break; default: ret = -1; break; } Copy "},{"title":"PX4 Dynamic Control Allocation","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/px4-dynamic-control-allocation/","content":"","keywords":""},{"title":"Introduction","type":1,"pageTitle":"PX4 Dynamic Control Allocation","url":"docs/hardware/px4-firmware/px4-dynamic-control-allocation/#introduction","content":" Control allocation is a part of the PX4 system that computes the actuator commands from torque and thrust setpoints. The old version of PX4 uses static mixing tables generated by mixers. A mixer uses the geometry of a vehicle to calculate an effectiveness matrix consisting of the force and torque generated by each actuator. Then the pseudo-inverse of the effectiveness matrix, aka the mixer matrix, is calculated. The mixer matrix can be multiplied with the moment and thrust setpoints to obtain the required actuator commands. All these works are done at compile time, therefore the mixing table cannot be modified during runtime. To achieve dynamic control allocation, we need to be able to change the mixing table at runtime.  The new control_allocator module does so by calculating the effectiveness matrix and its inverse in the controller. Its input, torque and thrust setpoints, are calculated in the angular_velocity_controller module which uses mass and inertia parameters to get gains with physical units. It also reads parameters that indicate which rotors are disabled and updates the effectiveness matrix accordingly. To integrate these modules into the existing system, a direct mixer is used. "},{"title":"Overall Structure","type":1,"pageTitle":"PX4 Dynamic Control Allocation","url":"docs/hardware/px4-firmware/px4-dynamic-control-allocation/#overall-structure","content":"angular_velocity_controller Calculate thrust_sp thrust_sp = thrust vm_mass G / hover_thrust Calculate torque_sp angular_accel_sp = gain_p . angular_velocity_error - gain_d . angular_accelerationtorque_ff = angular_velocity_int + gain_ff .* angular_velocity_sptorque_sp = inertia angular_accel_sp + torque_ff + angular_velocity x (inertia angular_velocity)angular_velocity_int = angular_velocity_int + i_factor gain_i angular_velocity_error * dt Publish thrust_sp and torque_sp ParametersParameter\tDescriptionAVC_*_P\tBody * axis angular velocity P gain (unit 1/s) AVC_*_I\tBody * axis angular velocity I gain (unit Nm/rad) AVC_*_D\tBody * axis angular velocity D gain AVC_*_I_LIM\tBody * axis angular velocity integrator limit (unit Nm) AVC_*_FF\tBody * axis angular velocity feedforward gain (unit Nm/(rad/s)) AVC_*_K\tBody * axis angular velocity controller global gain VM_MASS\tMass (unit kg) VM_INERTIA_XX\tInertia matrix, XX component (unit kg m^2) VM_INERTIA_YY\tInertia matrix, YY component (unit kg m^2) VM_INERTIA_ZZ\tInertia matrix, ZZ component (unit kg m^2) VM_INERTIA_XY\tInertia matrix, XY component (unit kg m^2) VM_INERTIA_XZ\tInertia matrix, XZ component (unit kg m^2) VM_INERTIA_YZ\tInertia matrix, YZ component (unit kg m^2) MPC_THR_HOVER\tHover thrust (Vertical thrust required to hover) MPC_USE_HTE\tHover thrust source selector (Set false to use the fixed parameter MPC_THR_HOVER Set true to use the value computed by the hover thrust estimator) see this PR for details Reference: https://docs.px4.io/master/en/advanced_config/parameter_reference.html#angular-velocity-control uORB SubscriptionsTopic\tDescriptioncontrol_allocator_status\ttorque_setpoint_achieved, unallocated_torque[3] vehicle_angular_acceleration\txyz[3] vehicle_control_mode\tflag_control_rates_enabled (true if rates are stabilized), flag_armed vehicle_land_detected\t... vehicle_rates_setpoint\ttimestamp, roll, pitch, yaw, thrust_body[3] (for multicopter, [0] and [1] are usually 0 and [2] is negative throttle demand, normalized in body NED frame [-1, 1]) vehicle_status\tvehicle_type hover_thrust_estimate\thover_thrust [0.1, 0.9] vehicle_angular_velocity\txyz[3]uORB PublicationsTopic\tDescriptionrate_ctrl_status\ttimestamp, rollspeed_integ, pitchspeed_integ, yawspeed_integ vehicle_angular_acceleration_setpoint\ttimestamp, timestamp_sample, xyz[3] vehicle_thrust_setpoint\ttimestamp, timestamp_sample, xyz[3] (unit N) vehicle_torque_setpoint\ttimestamp, timestamp_sample, xyz[3] (unit N.m)  control_allocator Update effectiveness matrix (repeat for each rotor) thrust = ct * axismoment = ct position x axis - ct km * axisPut thrust and moment in effectiveness matrix Set 0 effectiveness if act_min >= act_max in paramsRun allocation (pseudo-inverse) mix = pseudo-inverse(effectiveness)actuator_sp = mix * thrust_spClip actuator_spcontrol_allocated = effectiveness * actuator_sp Publish actuator_spPublish normalized actuator_sp as actuator_controls_4 and actuator_controls_5 for the mixer system Class Diagram Draw.io source ParametersParameter\tDescriptionCA_AIRFRAME\t0 Multirotor, 1 Standard VTOL (WIP), 2 Tiltrotor VTOL (WIP) CA_METHOD\t0 Pseudo-inverse with output clipping, 1 Pseudo-inverse with sequential desaturation technique CA_BAT_SCALE_EN\tBattery power level scaler CA_AIR_SCALE_EN\tAirspeed scaler CA_ACT*_MIN\t... CA_ACT*_MAX\t... Reference: https://docs.px4.io/master/en/advanced_config/parameter_reference.html#control-allocation uORB SubscriptionsParameter\tDescriptionbattery_status\t... airspeed\t... vehicle_status\t... vehicle_torque_setpoint\txyz[3]: torque setpoint along X, Y, Z body axis (in N.m) vehicle_thrust_setpoint\txyz[3]: thrust setpoint along X, Y, Z body axis (in N)uORB PublicationsParameter\tDescriptionvehicle_actuator_setpoint\tactuator[16] control_allocator_status\ttimestamp, allocated_torque[3], allocated_thrust[3], unallocated_torque[3], allocated_torque[3], torque_setpoint_achieved, thrust_setpoint_achieved, actuator_saturation[NUM_ACTUATORS] actuator_controls_4\tcontrol[8] actuator_controls_5\tcontrol[8] "},{"title":"SITL (Gazebo Simulation)","type":1,"pageTitle":"PX4 Dynamic Control Allocation","url":"docs/hardware/px4-firmware/px4-dynamic-control-allocation/#sitl-gazebo-simulation","content":"# Clone the repogit clone -b lirc-ca-new https://github.com/lirc572/PX4-Autopilot.git --recursive cd PX4-Autopilot # Run one of the make commands below to compile and start simulation: # Hexrotor x:make px4_sitl_ctrlalloc gazebo_typhoon_ctrlalloc # Quadrotor Wide:make px4_sitl_ctrlalloc gazebo_iris_ctrlalloc # Octorotor Coaxial:make px4_sitl_ctrlalloc gazebo_iris_cox_ctrlalloc # Stop one motor in PX4 Shell:param set CA_MC_R0_CT 0 Copy Commands of the control_allocator module: https://docs.px4.io/master/en/modules/modules_controller.html#description caution The iris_cox simulation does not work as expected. See the last section of this page for details. "},{"title":"Control Allocation Algorithm Implementation in Python","type":1,"pageTitle":"PX4 Dynamic Control Allocation","url":"docs/hardware/px4-firmware/px4-dynamic-control-allocation/#control-allocation-algorithm-implementation-in-python","content":"https://deepnote.com/project/PX4-9lguiAoGSLeQGbSWiN-9-g/%2Fctrlalloc.ipynb "},{"title":"TLab Octo-Coax Model","type":1,"pageTitle":"PX4 Dynamic Control Allocation","url":"docs/hardware/px4-firmware/px4-dynamic-control-allocation/#tlab-octo-coax-model","content":"Incomplete Gazebo Model: GitHub link init script (ROMFS/px4fmu_common/init.d/airframes/12001_octo_cox) #!/bin/sh## @name Octo Coaxial## @type Octorotor Coaxial# @class Copter## @output MAIN1 motor 1# @output MAIN2 motor 2# @output MAIN3 motor 3# @output MAIN4 motor 4# @output MAIN5 motor 5# @output MAIN6 motor 6# @output MAIN7 motor 7# @output MAIN8 motor 8## @board intel_aerofc-v1 exclude# @board bitcraze_crazyflie exclude# sh /etc/init.d/rc.mc_defaultssh /etc/init.d/rc.ctrlalloc if [ $AUTOCNF = yes ]then param set MPC_XY_VEL_I_ACC 4 param set MPC_XY_VEL_P_ACC 3 param set RTL_DESCEND_ALT 10 param set RTL_LAND_DELAY 0 param set MNT_MODE_IN 0 param set MAV_PROTO_VER 2 param set MPC_USE_HTE 0 # Set according to actual vehicle model param set VM_MASS 1.4995 # 2.05 param set VM_INERTIA_XX 0.018343 # 0.029125 param set VM_INERTIA_YY 0.019718 # 0.029125 param set VM_INERTIA_ZZ 0.032193 # 0.055225 param set CA_AIRFRAME 0 param set CA_METHOD 1 param set CA_ACT0_MIN 0.0 param set CA_ACT1_MIN 0.0 param set CA_ACT2_MIN 0.0 param set CA_ACT3_MIN 0.0 param set CA_ACT4_MIN 0.0 param set CA_ACT5_MIN 0.0 param set CA_ACT6_MIN 0.0 param set CA_ACT7_MIN 0.0 param set CA_ACT0_MAX 1.0 param set CA_ACT1_MAX 1.0 param set CA_ACT2_MAX 1.0 param set CA_ACT3_MAX 1.0 param set CA_ACT4_MAX 1.0 param set CA_ACT5_MAX 1.0 param set CA_ACT6_MAX 1.0 param set CA_ACT7_MAX 1.0 # KM: CCW: +ve, CW: -ve # PZ: 5.5cm / 22.0cm = 0.25 # https://docs.px4.io/master/en/airframes/airframe_reference.html#octorotor-coaxial param set CA_MC_R0_PX 0.7071068 param set CA_MC_R0_PY 0.7071068 param set CA_MC_R0_PZ -0.25 param set CA_MC_R0_CT 11.7 # 12.523 param set CA_MC_R0_KM 0.0137 # 0.0135 param set CA_MC_R1_PX 0.7071068 param set CA_MC_R1_PY -0.7071068 param set CA_MC_R1_PZ -0.25 param set CA_MC_R1_CT 11.7 # 12.523 param set CA_MC_R1_KM -0.0137 # -0.0135 param set CA_MC_R2_PX -0.7071068 param set CA_MC_R2_PY -0.7071068 param set CA_MC_R2_PZ -0.25 param set CA_MC_R2_CT 11.7 # 12.523 param set CA_MC_R2_KM 0.0137 # 0.0135 param set CA_MC_R3_PX -0.7071068 param set CA_MC_R3_PY 0.7071068 param set CA_MC_R3_PZ -0.25 param set CA_MC_R3_CT 11.7 # 12.523 param set CA_MC_R3_KM -0.0137 # -0.0135 param set CA_MC_R4_PX 0.7071068 param set CA_MC_R4_PY -0.7071068 param set CA_MC_R4_PZ 0.25 param set CA_MC_R4_CT 11.7 # 12.523 param set CA_MC_R4_KM 0.0137 # 0.0135 param set CA_MC_R5_PX 0.7071068 param set CA_MC_R5_PY 0.7071068 param set CA_MC_R5_PZ 0.25 param set CA_MC_R5_CT 11.7 # 12.523 param set CA_MC_R5_KM -0.0137 # -0.0135 param set CA_MC_R6_PX -0.7071068 param set CA_MC_R6_PY 0.7071068 param set CA_MC_R6_PZ 0.25 param set CA_MC_R6_CT 11.7 # 12.523 param set CA_MC_R6_KM 0.0137 # 0.0135 param set CA_MC_R7_PX -0.7071068 param set CA_MC_R7_PY -0.7071068 param set CA_MC_R7_PZ 0.25 param set CA_MC_R7_CT 11.7 # 12.523 param set CA_MC_R7_KM -0.0137 # -0.0135fi set MAV_TYPE 13 # set MIXER octo_coxset MIXER directset PWM_OUT 12345678 Copy "},{"title":"Problems","type":1,"pageTitle":"PX4 Dynamic Control Allocation","url":"docs/hardware/px4-firmware/px4-dynamic-control-allocation/#problems","content":"Currently the iris_cox simulation does not work as expected. The same gazebo model works with the old control allocation implementation (make px4_sitl gazebo_iris_cox) but not under the new control allocation structure. With the new implementation, as long as CA_ACT*_MIN is set to be >= CA_ACT*_MAX, the corresponding PWM output will be set to 1500. By default all CA_ACT*_MIN and CA_ACT*_MAX are set to 0.0, so any unconfigured motor will output 1500. Below are some simulation results: only configure R0-R3 → worksonly configure R4-R7 → accelerate upwards as soon as armconfigure all R0-R7 → accelerate upwards as soon as armconfigure R0 → okay to armconfigure R4 → upsidedownconfigure R0-R3 + R4 → okay to armconfigure R0-R3 + R5 → okay to armconfigure R0-R3 + R4 + R5 → when arm, front goes up → upsidedownconfigure R0-R3 + R6 → okay to armconfigure R0-R3 + R7 → okay to armconfigure R0-R3 + R6 + R7 → when arm, front goes up → upsidedown Below are my guesses of what went wrong: Even if the PWM output of a motor is 1500 (when left unconfigured), gazebo does not necessarily simulate the prop rotating at that speed, since the vehicle stays on the ground even if all lower 4 motors receive a PWM output of 1500.There is something wrong with the gazebo motor definition, since one side of the vehicle will tilt upwards if we enable 2 lower motors at the same side even if their PWM output is only a little above 1000. "},{"title":"How to Construct the iris_cox Gazebo Model:","type":1,"pageTitle":"PX4 Dynamic Control Allocation","url":"docs/hardware/px4-firmware/px4-dynamic-control-allocation/#how-to-construct-the-iris_cox-gazebo-model","content":"https://github.com/lirc572/PX4-SITL_gazebo/blob/c79686bd8d796bf4e50a0ffe521999059ac174ab/models/iris_cox_ctrlalloc/iris_cox_ctrlalloc.sdf The iris_cox model is based on the original iris model in PX4-SITL_gazebo.Duplicate the rotor_0 - rotor_3 links and joints, and update their name and position (as rotor_4 - rotor_7).Add 4 new libgazebo_motor_model plugins to match the lower 4 motors.Update the mavlink_interface plugin, edit the control_channels → rotor5 - rotor8 based on rotor1 - rotor4. "},{"title":"PX4 FAQ","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/px4-FAQ/","content":"","keywords":""},{"title":"Cannot Arm the Drone?","type":1,"pageTitle":"PX4 FAQ","url":"docs/hardware/px4-firmware/px4-FAQ/#cannot-arm-the-drone","content":"Check for any (Preflight checks fail) error messages from QGC. Check for the throttle position (it must be \"low\"). Check whether you are using Arming Gesture or Arming Buttom/Switch (they are either or). More info here Check the parameter COM_RC_IN_MODE, it should be the default value 0. more information regarding PX4 mixing and geometry filehttps://github.com/jlecoeur/servo_mix_matrix https://github.com/PX4/PX4-Devguide/issues/435 https://github.com/PX4/PX4-Devguide/issues/511 "},{"title":"Radio channel twitches in QGroundControl","type":1,"pageTitle":"PX4 FAQ","url":"docs/hardware/px4-firmware/px4-FAQ/#radio-channel-twitches-in-qgroundcontrol","content":"Check if you are using manually inverted SBUS signal. Try to use the original signal pad from the receiver. Past experience with frsky r-xsr is that if we manually solder the wire from the inverted SBUS pad, it may have the twitching problem. If we use the original SBUS signal wire connect to the flight controller SBUS pad then all is good. "},{"title":"Drone slowly spins after takeoff","type":1,"pageTitle":"PX4 FAQ","url":"docs/hardware/px4-firmware/px4-FAQ/#drone-slowly-spins-after-takeoff","content":"Check RC calibration. If you copy the param from another drone, you may encounter this. Just try to re-calibrate your RC from QGroundControl. "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/px4-overview/","content":"Overview PX4 firmware runs on our UAV platforms to support the basic flight control.","keywords":""},{"title":"Toolchain Installation & Setup","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/px4-toolchain-installation-setup/","content":"","keywords":""},{"title":"Toolchain installation","type":1,"pageTitle":"Toolchain Installation & Setup","url":"docs/hardware/px4-firmware/px4-toolchain-installation-setup/#toolchain-installation","content":"It is recommended to use Linux machine for PX4 development as it takes 🔥excruciatingly🔥 long time to compile in Windows. If you have Ubuntu 18.04 installed (recommended), go ahead to this link or follow the steps below. caution If you see errors while running any of the installation scripts, do not proceed to the next step. Check your internet connection and re-run the script until there is no error! Download PX4 Source Code and run the installation script: git clone https://github.com/PX4/PX4-Autopilot.git --recursivecd PX4-Autopilotbash ./Tools/setup/ubuntu.sh # re-run this script if there are errors Copy Acknowledge any prompts as the script progress. Download and run the ROS/Gazebo installation script: wget https://raw.githubusercontent.com/PX4/Devguide/master/build_scripts/ubuntu_sim_ros_melodic.shbash ubuntu_sim_ros_melodic.sh # re-run this script if there are errors Copy You may need to acknowledge some prompts as the script progresses. Reboot the system to complete installation. "},{"title":"Test run","type":1,"pageTitle":"Toolchain Installation & Setup","url":"docs/hardware/px4-firmware/px4-toolchain-installation-setup/#test-run","content":"To verify succesful toolchain installation and setup, we will try to compile the PX4 firmware for both simulation and fmu-v2. First, we try to compile PX4 firmware and run gazebo simulation, more details on simulation with gazebo here. cd PX4-Autopilotmake px4_sitl gazebo Copy To takeoff, run commander takeoff in the PX4 shell. caution Sometimes Gazebo fails to load up on the first run. When Gazebo shows a blank window, try running the above command again. Then, try to compile PX4 firmware for fmu-v2 target, more details on different targets here. make px4_fmu-v2 Copy tip If there is build error like this, follow the guide here to check compiler version.If you forget the make command, use make list_config_targets to remind yourself.After switching branch, submodules are not automatically updated, to update submodules, use git submodule update --init "},{"title":"FCU-OBC Time Synchronisation","type":0,"sectionRef":"#","url":"docs/hardware/px4-firmware/time-synchronisation/","content":"","keywords":""},{"title":"Motivation - Sensor Data Correspondence","type":1,"pageTitle":"FCU-OBC Time Synchronisation","url":"docs/hardware/px4-firmware/time-synchronisation/#motivation---sensor-data-correspondence","content":"PX4-based flight controller boards (FCU), such as Pixhawk series, have rich I/O interfaces, and at the same time provides a real-time operating system environment (POSIX-compliant Nuttx specifically). These capabilities enables us to have real-time guarantees, when interfacing any sensors that are time-critical. The necessity of time synchronisation comes into play due to the following facts: The flight controller board (FCU) and the on-board computer (OBC) operates on different hardware clocks (typically its respective boot time, in nanoseconds);Sensor data reception is physically distributed: some sensor data (e.g. built-in IMU) is received by the FCU, while others (e.g. cameras) are received by the OBC;Tight-coupled sensor fusion algorithm requires precise time correspondence of all sensor inputs involved, to perform inference on the state of the agent. One particular use case is to operate multiple cameras in a hardware-synchronised fashion, with a master IMU module sending electrical triggering signal. We are interested in determining the correspondence between all incoming image streams and the triggering IMU measurement. The solution is non-trivial as IMU measurement is taken with a timestamp in FCU-time domain, whereas the image reception (either over MIPI or USB data link) is timestamped in OBC-time domain. "},{"title":"Doing Without Time Synchronisation","type":1,"pageTitle":"FCU-OBC Time Synchronisation","url":"docs/hardware/px4-firmware/time-synchronisation/#doing-without-time-synchronisation","content":"It is technically possible to determine correspondence without any time synchronisation, but with a few necessary assumptions. In this setting, we need to assume: All sensor data to establish correspondence are strictly triggered by the same physical event (CPU instruction, electrical signal), up to the precision required (often sub-millisecond);The communication delays of respective sensor data to OBC are predicable / consistent;The sensor data is gathered by OBC, and the sensor fusion algorithms are only carried out on OBC. In this way, we could perform all processing in OBC-time domain;The OBC has close to realtime scheduling performance. (This is where things may break, particularly under high CPU load) Fortunately, this is often the case. Lets do some quick maths in the case of IMU-Cameras data correspondence. "},{"title":"Correspondance with Known Latency and Sampling Period","type":1,"pageTitle":"FCU-OBC Time Synchronisation","url":"docs/hardware/px4-firmware/time-synchronisation/#correspondance-with-known-latency-and-sampling-period","content":"Latency for IMU Data over UART# Typically, IMU data is transmitted over UART protocol from FCU / dedicated IMU to the OBC. The typical baud rate we have is 921600bps, which is ~90KB per second, which in turn is 90 byte per millisecond. It is typically safe to assume each IMU packet is less than a 100 byte, therefore the communication delay induced by the physical UART layer is about $t_{UART} \\approx 1$ ms. In reality, this number is still in the range of very few milliseconds (depending on the pulling rate of the OS; In Linux low-latency mode, the rate is about 1ms). Latency for Image Data over USB3.0# On the other hand, one image data frame is much much larger than an imu data frame. A 1-megapixel 16-bit raw image would give rise to a data size of 2MB. Provided a USB3.0 physical layer connection has a bandwidth of 5Gbit/s, the communication delay over USB is $t_{USB} \\approx \\frac{8*2}{5} = 3.4$ ms. In reality, the delay could be in terms of $10$ ms or more, heavily depends on how many devices present on the USB bus streaming. Putting into context, with a 20Hz rate of IMU-cameras correspondence frequency, the data correspondence happens every 50ms or so. Therefore, we could infer the IMU-cameras correspondence by just looking at the time of arrival (TOA) at OBC side. Why is that the case? Camera data is almost surely (we will see when this is not the case soon) arrived after IMU data, because of the communication delays;Compared to the 50ms sampling period in between each IMU-camera correspondence event, the expected slack time between the TOA of camera data and TOA of IMU data is much less, $10ms < 50ms$. This ensures that we do not have ambiguity of having camera data 'diffusing' into the next IMU-camera event. (We may term this as aliasing) In summary, the good stuff here is that we have the slack time (between IMU data and camera data) predicable and less than the sampling period. We could use a trivial algorithm to match the camera data to the closest IMU data which is strictly earlier in OBC-time domain. "},{"title":"Potential Issues Working without Time Synchronisation","type":1,"pageTitle":"FCU-OBC Time Synchronisation","url":"docs/hardware/px4-firmware/time-synchronisation/#potential-issues-working-without-time-synchronisation","content":"Pros: No software architectural dependency outside OBC Cons: All sensor data need to be triggered by the same physical event, and the precise timing of that event cannot be determined precisely (can only be estimated based on the communication delays etc.)Case-by-case analysis neededNon-realtime operating system may cause jittering which corrupts the correspondenceSubject to system CPU loading and other resource constraints The obvious advantage of this no-synchornisation approach is that there is no dependency outside the OBC, as we disregard all other time domains, and only work on time of arrival timings in the OBC-time domain. However, as we have seen, the correctness of such approach need to be analysed case-by-case: the nature of the communication channels, the data size, the time period of each correspondence event. All those piece must be compatible to make inference of correspondance work. Otherwise, issues like aliasing may happen. Known issue: In not-so rare cases, camera data may arrive to OBC earlier than the triggering. This may caused by multiple factors, such as the USB kernel was allocated the right time to transmit the data over, while the UART kernel is stucked waiting for scheduling. I would like to term this effect as jitter, as this normally have something to do on how the OS schedule kernel tasks. In addition, there is no way to know the actual time that the correspondence is happening, which hides behind the non-deterministic communication delay, buffering delay etc. For a reference on how the implementation may look like for a IMU-Camera synchronisation, without assuming time synchronisation, refer here camera_imu_sync.cpp. "},{"title":"Doing Time Synchronisation using MAVLink","type":1,"pageTitle":"FCU-OBC Time Synchronisation","url":"docs/hardware/px4-firmware/time-synchronisation/#doing-time-synchronisation-using-mavlink","content":"The more elaborated and proper way to achieve time synchronisation of different sensor data in different time-domain is discussed below. In the platform we used, an implementation has been done using MAVLink protocol, implemented in both PX4 Firmware and Mavros package. "},{"title":"Working Principle (Reference)","type":1,"pageTitle":"FCU-OBC Time Synchronisation","url":"docs/hardware/px4-firmware/time-synchronisation/#working-principle-reference","content":"The host (both side CAN be the host at the SAME time) sends a TIMESYNC message, which contain two fields tc1 (filled with zero) and ts1(filled with current hardware time in nanoseconds). As the receiver, when a TIMESYNC message is received, there are two cases: tc1 == 0, then the message just make a half-round trip. The receiver sends back tc1 filled with its own current hardware timestamp in nanoseconds, and the original ts1 fieldtc1 != 0, then the message has made the full round trip. Then the current hardware time subtracted by the ts1 field will give the round trip time, and tc1 - (current time + ts1) / 2 will give the offset. The logics are both implemented in mavros (sys_time.cpp) and in PX4 firmware. On mavros, the estimation is availalbe at topic /mavros/timesync_status, like example below: header: seq: 172 stamp: secs: 1013 nsecs: 301973092 frame_id: ''remote_timestamp_ns: 35922878000observed_offset_ns: 977378604936estimated_offset_ns: 977378507661round_trip_time_ms: 0.979619979858 Copy "},{"title":"Synchronising Timestamp","type":1,"pageTitle":"FCU-OBC Time Synchronisation","url":"docs/hardware/px4-firmware/time-synchronisation/#synchronising-timestamp","content":"note The content below is based on the modified version of mavros. The original mavros only allows the time synchronisation to be performed against OBC's realtime clock (Wallclock / UNIX Time). This is potentially problematic for us, as realtime clock can jump, whenever a NTP timesync happens over the network. clock_source: monotonic # realtime (ros::Time::now()) or monotonic (MONOTOMIC_TIME) option has been added to allow mavros to track against monotonic clock (hardware time) on the OBC, instead of the wallclock. (encapsulated in the get_time_now()) To turn on time synchronisation, just set the timesync_mode option to be MAVLINK, and clock_source to be monotonic. When mavros first starts up, it requires tens of seconds to stabilise the estimation. A info prompt will be shown after the synchronisation estimate has been completed. "},{"title":"Basics - Bash & File Systems","type":0,"sectionRef":"#","url":"docs/linux/getting-started/basics-bash-file-system/","content":"","keywords":""},{"title":"Bash Syntax","type":1,"pageTitle":"Basics - Bash & File Systems","url":"docs/linux/getting-started/basics-bash-file-system/#bash-syntax","content":"Watch the following video:  Keypoints of the video: ~ and $Bash syntax (command base): program-name argumentsthe search paths when executing: the PATH variable, the absolute path, denoted with leading /, or the relative path, the PWD (current working directory) Excercise: What is the different between executing foo and ./foo? Excercise: How to pass multiple arguments when executing a program? "},{"title":"Bash Pipeline, Redirection and Built-in Commands","type":1,"pageTitle":"Basics - Bash & File Systems","url":"docs/linux/getting-started/basics-bash-file-system/#bash-pipeline-redirection-and-built-in-commands","content":"Watch the video until 11:30  Watch the video above from 11:30 First few built-in commands: cd change directoryecho print arguments to stdout (try echo $PATH)export set environment variable (inheritable variables) "},{"title":"Bash Basic Navigation and File Manipulations","type":1,"pageTitle":"Basics - Bash & File Systems","url":"docs/linux/getting-started/basics-bash-file-system/#bash-basic-navigation-and-file-manipulations","content":"Access and Navigation  Keypoints of the video: Understand command cd,cd ..,ls, ls -a, mkdir, pwdUnderstand command touch, mv, rm Creating & Editing Text Files  Keypoints of the video: - Understand command `cat`,`less`,`tail`, - Use `nano` to edit files Privileges and Permissions  Keypoints of the video: - Using `sudo` to raise to root access - Set permissions and group using `chmod` and `chown` "},{"title":"Finding Documentation and Files","type":1,"pageTitle":"Basics - Bash & File Systems","url":"docs/linux/getting-started/basics-bash-file-system/#finding-documentation-and-files","content":" Keypoints of the video: Locate executable in PATH by whichLocate files using find or locate or whereisusing grep to find text in files (Reference) "},{"title":"Navigating through Directories and Locating Files","type":1,"pageTitle":"Basics - Bash & File Systems","url":"docs/linux/getting-started/basics-bash-file-system/#navigating-through-directories-and-locating-files","content":"tip Other Learning Materials: The Edx Link (Chapter 7)Tutorial Point (Unix for Beginners)Shell Tutorials "},{"title":"Questions for Thought","type":1,"pageTitle":"Basics - Bash & File Systems","url":"docs/linux/getting-started/basics-bash-file-system/#questions-for-thought","content":"What is the difference between ls and ls -a?How to differentiate between an absolute path and a relative path? What does environment variable PATH do? How to view, set, append environment variables?What can which whereis and locate do? What are their differences?  "},{"title":"Know More about a File","type":1,"pageTitle":"Basics - Bash & File Systems","url":"docs/linux/getting-started/basics-bash-file-system/#know-more-about-a-file","content":"stat to show the modification date etcfile to know the binary type (architecture)ldd to know the dynamic linkage (.so file links) "},{"title":"Uncompress Zip File","type":1,"pageTitle":"Basics - Bash & File Systems","url":"docs/linux/getting-started/basics-bash-file-system/#uncompress-zip-file","content":"Reference Tar file:tar -czvf archive.tar.gz /usr/local/something Unzip file with permission preserved:tar -xpf file "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/hardware/UAV-platform/Multirotor-overview/","content":"","keywords":""},{"title":"Platform","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#platform","content":"Solder ESC GND pads to AUAV Power Module   Place the flight control board on the layer above with buzzer and safety switch connected  Connect ESC servo to flight control board  Connect ESC servo to MAIN OUT (not AUX OUT) according to the labelled numbers Connect receiver to RC on flight control board  tip Always keep the cables neatly arranged and components fixed in place. It is recommended to use cable ties and double-sided tape to help with the practice. Try not to bundle the cable near the propellers for safety purposes. caution Always check the polarity.  "},{"title":"QGroundControl","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#qgroundcontrol","content":"Download QGroundControl on DroneCode (Code is available on GitHub)Run ./QGroundControlAppImage on terminalConnect the flight control board to computer via USB portUpdate the firmwareSelect correct airframeConnect the vehicle to battery and calibrate ESC PWM Minimum and MaximumCalibrate sensorsSet up and calibrate radio (controller) In order to connect the controller to receiver, press the LINK/MODE on receiver until the red light start blinking and then turn on the controller. When the light turns green, the connection is done. Set up flight mode note More detailed information can be found on Vehicle Setup. tip Safety switch: Double blinking suggests that vehicle can be armed while single blinking suggest that vehicle is not allowed to be armed. "},{"title":"Tuning","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#tuning","content":"Tuning is required when creating a new airframe type or significantly modifying an existing supported frame. Generally if you're using a supported configuration (e.g. using an airframe in QGroundControl > Airframe) the default tuning should be acceptable (particularly for larger frames). Reference from PX4UserGuide and dronecode. All tuning should be performed in the manual Stabilized flight mode. Go into settings > tuning > click on the advance at the topright corner  Mavlink Inspector could also be used to monitor the response from the drone. The response and the command graph should be rather close to one another for a good response from the drone. Check for overshoot or any sluggish response from the drone by feel or looking at the graph.  "},{"title":"Step 1: Preparation","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#step-1-preparation","content":"First of all set all parameters to initial values: Set all MCXXX_P to zero (ROLL, PITCH, YAW)_Set all MC_XXXRATE_P, MC_XXXRATE_I, MC_XXXRATE_D to zero, except MC_ROLLRATE_P and MC_PITCHRATE_PSet MC_ROLLRATE_P and MC_PITCHRATE_P to a small value, e.g. 0.02Set MC_YAW_FF to 0.5 All gains should be increased slowly, about 20%-30% per iteration, 10% for fine tuning. Large gains may cause very dangerous oscillations. "},{"title":"Step 2: Stabilize Roll and Pitch Rates","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#step-2-stabilize-roll-and-pitch-rates","content":"P Gain Tuning Parameters: MC_ROLLRATE_P, MC_PITCHRATE_P. Tilt it in roll or pitch direction, and observe the response. If it oscillates, tune down RATE_P. Once the control response is slow but correct, increase RATE_P until it starts to oscillate. Cut back RATE_P until it does only mildly oscillate or not oscillate any more (about 10% cutback), just over-shoots. Typical value is around 0.1. D Gain Tuning Parameters: MC_ROLLRATE_D, MC_PITCHRATE_D. Assuming the gains are in a state where the multi rotor oscillated and RATE_P was slightly reduced. Slowly increase RATE_D, starting from 0.01. Increase RATE_D to stop the last bit of oscillation. If the motors become twitchy, the RATE_D is too large, cut it back. By playing with the magnitudes of RATE_P and RATE_D the response can be fine-tuned. Typical value is around 0.01…0.02. I Gain Tuning If the roll and pitch rates never reach the setpoint but have an offset, add MC_ROLLRATE_I and MC_PITCHRATE_I gains, starting at 5-10% of the MC_ROLLRATE_P gain value. "},{"title":"Step 3: Stabilize Roll and Pitch Angles","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#step-3-stabilize-roll-and-pitch-angles","content":"P Gain Tuning Parameters: MC_ROLL_P, MC_PITCH_P. Set MC_ROLL_P and MC_PITCH_P to a small value, e.g. 3 Tilt it in roll or pitch direction, and observe the response. If it oscillates, tune down P. Once the control response is slow but correct, increase P until it starts to oscillate. Optimal response is some overshoot (~10-20%). After getting stable response fine tune RATE_P, RATE_D again. "},{"title":"Step 4: Stabilize Yaw Rate","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#step-4-stabilize-yaw-rate","content":"P Gain Tuning Parameters: MC_YAWRATE_P. Set MC_YAWRATE_P to small value, e.g. 0.1 If it oscillates or becomes twitchy, tune down RATE_P. If response is very large even on small movements (full throttle spinning vs idle spinning propellers) reduce RATE_P. Typical value is around 0.2…0.3. "},{"title":"Step 5: Stabilize Yaw Angle","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#step-5-stabilize-yaw-angle","content":"P Gain Tuning Parameters: MC_YAW_P. Set MC_YAW_P to a low value, e.g. 1 If it oscillates, tune down P. Once the control response is slow but correct, increase P until the response is firm, but it does not oscillate. Typical value is around 2…3.  "},{"title":"Pixhawk","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#pixhawk","content":"Check out the website to set up the TELEM2 port for companion computer.  "},{"title":"Nvidia Jetson Xavier NX","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#nvidia-jetson-xavier-nx","content":""},{"title":"1. Set up Jetson Xavier NX Developer Kit","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#1-set-up-jetson-xavier-nx-developer-kit","content":""},{"title":"2. Install software with SDK Manager","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#2-install-software-with-sdk-manager","content":"2.1. Download SDK Manager on a developer host machine 2.2. Connect Xavier to the developer host machine via micro USB port 2.3. Follow the instructions note Untick Jetson OS at Step 3 "},{"title":"3. Install ROS Melodic","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#3-install-ros-melodic","content":""},{"title":"4. Follow the source installation instructions at DroneCode to compile MAVROS","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#4-follow-the-source-installation-instructions-at-dronecode-to-compile-mavros","content":"note Add the following lines into .bashrc $ sudo apt install nano$ nano .bashrc Copy # Lines to be addedsource /opt/ros/melodic/setup.bashsource /home/yt/catkin_ws/devel/setup.bash Copy "},{"title":"5. Check serial port (TX/RX) using loopback test","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#5-check-serial-port-txrx-using-loopback-test","content":"5.1. Connect the TX and RX port using one cable in order to perform loopback test tip The pin configuration of Jetson Xavier NX Developer Kit can be found here. 5.2. Set up Minicom $ sudo apt-get install minicom$ minicom Copy 5.3. Open up Minicom and perform loopback test $ sudo minicom -D /dev/ttyTHS0 Copy Press Ctrl A-Z Press O to configure minicom Select serial port setupChange serial device  Or Open the minicom in terminal using different tty* note List of tty* can be found by typing the following code in terminal $ ls /dev/tty* Copy To verify the TX/RX ports, make sure the content you type in shows on the terminal. "},{"title":"6. Modify the line shown below in px4.launch file","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#6-modify-the-line-shown-below-in-px4launch-file","content":"# path: ~/catkin_ws/src/mavros/mavros/launch# line to be modified: <arg name =\"fcu_url\" default=\"/dev/ttyTHS0:921600\"> Copy Set tty to the tty verified in previous step (For example: ttyASM0 -> ttyTHS0)Change the baudrate (the number behind tty) to the baudrate of TELEM2 note Check the baudrate of TELEM2 on QGroundControl by searching the parameter SER_TEL2_BAUD. "},{"title":"7. Check the connection between PX4 and Jetson Xavier","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#7-check-the-connection-between-px4-and-jetson-xavier","content":"7.1. Connect RX, TX and GND pin on Jetson Xavier to TELEM2 on PX4 note Pin configuration of TELEM2 on PX4 (from left to right) is 5V, RX, TX, CTS, RTS, GND. 7.2. Boot up PX4 and check if anything shows up on the Minicom on Jetson Xavier "},{"title":"8. Run px4.launch file","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#8-run-px4launch-file","content":"8.1. In px4.launch file, make sure fcu_url is modified as stated in step 5 8.2. Change gcs_url into \"udp://@ip_address\" The IP address inserted is the IP address of the computer where QGroundControl will run on. Therefore, when px4.launch file runs, the QGroundControl will be automatically connected to the pixhawk. 8.3. Run the following command in terminal $ roslaunch mavros px4.launch Copy # Can try the following line if encounter error # DeviceError: serial: open: Permission denied$ sudo chmod a+rw /dev/ttyTHS0 Copy tip Compiled information can be found here. tip When UART port THS0 is used to connect to pixhawk, warnings keep showing up while px4.launch file is running. However, when USB port USB0 is used, px4.launch file runs normally.  "},{"title":"Vicon","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#vicon","content":"Check out website to setup vicon.  "},{"title":"Camera","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#camera","content":"1. Check CUDA version Jetson Xavier has CUDA 10.2 pre-installed by SDK Manager # insert the following lines in .bashrcexport PATH=/usr/local/cuda-10.2/bin${PATH:+:${PATH}}$export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRA$ # check CUDA version$ nvcc --version$ apt policy cuda # might have error Copy 2. Download ZED SDK for Jetpack 4.4 3.3.3 (Jetson Nano, NX, TX2, Xavier, CUDA 10.2) from Stereolabs 3. Follow the instructions to install ZED SDK note The SDK is located at /usr/local/zed/tools 4. Download ZED ROS Wrapper $ cd ~/catkin_ws/src$ git clone https://github.com/stereolabs/zed-ros-wrapper.git$ cd ../$ rosdep install --from-paths src --ignore-src -r -y$ catkin build -DCMAKE_BUILD_TYPE=Release$ source ./devel/setup.bash Copy 5. Open a terminal and use roslaunch to start the ZED node: # ZED Mini camera: $ roslaunch zed_wrapper zedm.launch Copy 6. Download cv_bridge from GitHub 7. Data display with Rviz $ roslaunch zed_display_rviz display_zedm.launch$ rosrun rqt_reconfigure rqt_reconfigure Copy note Reference code used for cpp  "},{"title":"VIO","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#vio","content":""},{"title":"Coordinate Frames","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#coordinate-frames","content":"Frame used in ZED Mini: map (NWU) -> odom (NWU) -> base_link (NWU) note map is the parent of odom and odom is the parent of base_link. The definition of a child frame is based on its parent frame. Both map and odom are world-fixed frame while base_link is attached to the mobile robot base. Frame conventions (odom) in ROS: Body frame: NWUWorld frame: ENU Frame used in PX4: NED note Reading REP 103REP 105 "},{"title":"Working principle","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#working-principle","content":"Feed /mavros/odometry/out with odom data of type nav_msgs::Odometry in the correct frame according to ROS conventions. Then, the functions in odom.cpp will transform the frame of odom data into the frame for MAVLINK (PX4). MAVLINK will fuse the odom data from /mavros/odometry/out with internal sensors using EKF. Eventually, the final position that is used by the flight controller will be published at /mavros/local_position/pose. About MAVROS odom.cpp odom_cb() : transforming and sending odometry to fcu handle_odom() : receiving and transforming odometry from fcu Frame_id required by fcu header.frame_id = \"odom_ned\" child_frame_id = \"base_link_frd\" "},{"title":"Steps","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#steps","content":"With zed-ros-wrapper and mavros running, open rviz to check the frames orientation and open rqt to check tf_tree.  Since the frames of map and odom in zed-ros-wrapper do not follow the ROS conventions, the transform by MAVROS from odom to odom_ned is incorrect because it assumes the provided odom data is in ENU (but actually is NWU by ZED). Therefore, changes are needed to be made in code that performs frame transformation. In mavros/mavros/src/lib/uas_data.cpp, line 54-68: std::vector<geometry_msgs::TransformStamped> transform_vector; # from ENU to NEDadd_static_transform(\"map\", \"map_ned\", Eigen::Affine3d(ftf::quaternion_from_rpy(M_PI, 0, M_PI_2)),transform_vector); # from ENU to NEDadd_static_transform(\"odom\", \"odom_ned\", Eigen::Affine3d(ftf::quaternion_from_rpy(M_PI, 0, M_PI_2)),transform_vector); # from NWU to NEDadd_static_transform(\"base_link\", \"base_link_frd\", Eigen::Affine3d(ftf::quaternion_from_rpy(M_PI, 0, 0)),transform_vector); Copy In order to match it with ZED frame settings, the following line has to be modified as shown. # from NWU to NEDadd_static_transform(\"odom\", \"odom_ned\", Eigen::Affine3d(ftf::quaternion_from_rpy(M_PI, 0, 0)),transform_vector); # Modify \"map\" frame tranformation line if \"map_ned\" is used.# The sequence is in (roll, pitch, yaw)# M_PI = 180 degree Copy Now check the orientation of odom_ned on Rviz. Then, add the following lines in MAVROS launch file to feed the zed camera odom data to /mavros/odometry/out <!-- You should only remap either vision_pose or odometry, but not both --><!-- /mavros/vision_pose/pose is posestamped, where /mavros/vision_pose/pose is posewithcovariancestamped --> <remap from=\"/mavros/odometry/out\" to=\"/zedm/zed_node/odom\" /> Copy Add estimator_type: 3 under odometry in px4_config.yaml. Run roscore, mavros and zed-ros-wrapper. Note that zed-ros-wrapper has to be launched after messages Time offset between FCU and OBC has been initialised show on the terminal. On QGC, set parameter MAV_ODOM_LP = 1 to loop back odometry. Then, check ODOMETRY at Mavlink Inspector. To verify the frame, perform the following actions. If all are true, it indicates that the frame is transformed correctly. Yaw clockwise: Quaternion[3] becomes positive/increasesRoll to the right: Quaternion[1] becomes positive/increasesPitch up: Quaternion[2] becomes positive/increasesAt initial position: The values of quaternion should be close to [1, 0, 0, 0] = [w, x, y, z] The second problem with zed-ros-wrapper is time synronisation between OBC and FCU. The time difference between FCU and OBC is too large. Therefore, zed-ros-wrapper is modified as following. In zed-ros-wrapper/zed_nodelets/src/zed_nodelet/src/zed_wrapper_nodelet.cpp, add the following function in public. // Huimin's uint64_t get_monotonic_now(void){ struct timespec spec; clock_gettime(CLOCK_MONOTONIC, &spec); return spec.tv_sec * 1000000000ULL + spec.tv_nsec;} Copy Then, modify the code accordingly. # line 3230-3240 # Original code:// Timestampif (mSvoMode) { mFrameTimestamp = ros::Time::now();} else { mFrameTimestamp = sl_tools::slTime2Ros(mZed.getTimestamp(sl::TIME_REFERENCE::IMAGE));} # Modified code:// Timestampif (mSvoMode) { mFrameTimestamp = ros::Time::now(); NODELET_INFO_STREAM(\"In SVO mode\");} else { uint64_t monotonic_time = get_monotonic_now(); mFrameTimestamp = ros::Time().fromNSec(monotonic_time); } # Note: publishOdom(mOdom2BaseTransf, mLastZedPose, mFrameTimestamp); Copy After catkin build, everything should be working fine. To further optimise VIO, fill the QRC parameter EKF2_EV_DELAY with the delay time (in milliseconds) between Vision and IMU by checking the Log file using FlightPlot.  "},{"title":"Camera Calibration","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#camera-calibration","content":"Install dependencies and Kalibr package $ sudo apt-get install python-setuptools python-rosinstall ipython libeigen3-dev libboost-all-dev doxygen libopencv-dev python-software-properties software-properties-common libpoco-dev python-matplotlib python-scipy python-git python-pip ipython libtbb-dev libblas-dev liblapack-dev python-catkin-tools libv4l-dev Copy Record the data which will be used for calibration later $ rosbag record -O Kalibr_data.bag /zedm/zed_node/imu/data_raw /zedm/zed_node/left/image_rect_color /zedm/zed_node/right/image_rect_color Copy Add april_gridl.yaml and imu-params.yaml in the folder Run calibratin file $ kalibr_calibrate_cameras --bag /home/safmc/zed-kalibr/Kalibr_data.bag --topics /zedm/zed_node/left/image_rect_color /zedm/zed_node/right/image_rect_color --models pinhole-radtan pinhole-radtan --target /home/safmc/kalibr_workspace/src/kalibr/april_grid.yaml $ kalibr_calibrate_imu_camera --bag /home/safmc/zed-kalibr/Kalibr_data.bag --cam camchain-homesafmczed-kalibrKalibr_data.yaml --imu /home/safmc/kalibr_workspace/src/kalibr/imu-params.yaml --target /home/safmc/kalibr_workspace/src/kalibr/april_grid.yaml # enter full path of Kalibr_data.bag and april_grid.yaml Copy  "},{"title":"Teraranger Evo 60m","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#teraranger-evo-60m","content":"Connect the sensor to pixhawk autopilothttps://www.terabee.com/connection-to-pixhawk-autopilots-teraranger-evo/ Enable the parameter SENS_EN_TRANGER in QGC (Parameters->sensors) If SENS_EN_TRANGER cannot be found, follow the steps below: Insert distance_sensor/teraranger under DRIVERS in Firmware/boards/px4/fmu-v2/default.cmake file. Type make px4_fmu-v2 under Firmware directory in terminal. Flash the firmware file(Firmware/build/px4_fmu-v2_default/px4_fmu-v2_Default.px4) to the board via QGC. Now SENS_EN_TRANGER is visible in the parameters menu. Remove distance_sensor from blacklist in ~/catkin_ws/src/mavros/mavros/launch/px4_pluginlists.yaml. Modify px4_config.yaml to configure the sensor. Check that id under publisher is the same as the value found in QGC for DISTANCE_SENSOR. Type this command to check /mavros/distance_sensor/NAME_OF_THE_PUB is running. $ rostopic list Copy Type this command to view the readings of the sensor. $ rostopic echo /mavros/distance_sensor/NAME_OF_THE_PUB Copy  "},{"title":"GitHub","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#github","content":"# Push commits$ git add .$ git commit -m \"description\"$ git push origin master Copy  "},{"title":"Remote desktop","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#remote-desktop","content":"FYI: Local host is the current host in which you are logged in. Remote host is the host to which you are trying to connect from the local host . "},{"title":"Access Ubuntu remotely from Ubuntu","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#access-ubuntu-remotely-from-ubuntu","content":"SSH Make sure both local host and remote host is connected to the same network On local host, run the following command in terminal $ ssh desktop_name@ip_address_of_remote_host$ ssh yt@192.168.1.124 #example Copy Enter exit to end SSH session VNC Make sure Remmina is installed in both hostMake sure both local host and remote host is connected to the same network On remote host, open the Settings, under Sharing, turn on both screen sharing and remote loginOpen Remmina, select VNC and enter the IP address of remote host  "},{"title":"Access Ubuntu remotely from Macos","type":1,"pageTitle":"Overview","url":"docs/hardware/UAV-platform/Multirotor-overview/#access-ubuntu-remotely-from-macos","content":"VNC Make sure both local host and remote host is connected to the same network Finder > Go > Connect to server > Browse > Select > Share screen (at top right of the window) "},{"title":"Basics - More Bash Scripting","type":0,"sectionRef":"#","url":"docs/linux/getting-started/basics-bash-scripting/","content":"","keywords":""},{"title":"Reading Documentation about Commands","type":1,"pageTitle":"Basics - More Bash Scripting","url":"docs/linux/getting-started/basics-bash-scripting/#reading-documentation-about-commands","content":"The Edx Link (Chapter 8) man COMMANDCOMMAND -h or COMMAND --help "},{"title":"Quick Reference - Basic Commands","type":1,"pageTitle":"Basics - More Bash Scripting","url":"docs/linux/getting-started/basics-bash-scripting/#quick-reference---basic-commands","content":"cat,less,more to print files,head, tail to print only the two ends of the filegrep \"text\" to filter the output of the stdout pipe (Ref)check network by ping,ifconfig,iwconfigcheck USB devices by lsusbcheck harddrive (block device) by lsblkcheck device messages by dmesg or dmesg -wcheck services by sudo systemctl status and similar command of start, stop, restart "},{"title":".bashrc and .profile file","type":1,"pageTitle":"Basics - More Bash Scripting","url":"docs/linux/getting-started/basics-bash-scripting/#bashrc-and-profile-file","content":"They are scripts that are automatically run by the shell when you open a new terminal. They can be quite useful in setting up environment variables like PATH, and running scripts. https://serverfault.com/questions/261802/what-are-the-functional-differences-between-profile-bash-profile-and-bashrc "},{"title":"Apt Package Manager (Ubuntu)","type":0,"sectionRef":"#","url":"docs/linux/getting-started/basics-package-mgt/","content":"","keywords":""},{"title":"Introduction","type":1,"pageTitle":"Apt Package Manager (Ubuntu)","url":"docs/linux/getting-started/basics-package-mgt/#introduction","content":"Explained! Difference between apt update and apt upgrade in Ubuntu YouTube  If there are some missing dependency errors, try sudo apt install -f to try to fix it. Managing Software  "},{"title":"Check Installed Versions","type":1,"pageTitle":"Apt Package Manager (Ubuntu)","url":"docs/linux/getting-started/basics-package-mgt/#check-installed-versions","content":"apt policy package_name "},{"title":"Check Content of the Package","type":1,"pageTitle":"Apt Package Manager (Ubuntu)","url":"docs/linux/getting-started/basics-package-mgt/#check-content-of-the-package","content":"Installed package:dpkg -L package_name deb file:dpkg -c package_file.deb without installing and without the deb file sudo apt-file update apt-file list package_name Copy "},{"title":"Basics - Linux OS Concepts","type":0,"sectionRef":"#","url":"docs/linux/getting-started/basics-linux-os/","content":"","keywords":""},{"title":"Why An Opertating System?","type":1,"pageTitle":"Basics - Linux OS Concepts","url":"docs/linux/getting-started/basics-linux-os/#why-an-opertating-system","content":"What is an operation system and why do we need them in the first place? This is a very important question to ask yourself, hopefully even before you getting into the myriad of Linux knowledge. The answers are not as trivial as one might expect, and everyone might have a slightly different interpretation on this matter. But that's okay, as operating system is truly multifacaded. View the following video to get another overview of the operating system: Operating Systems: Crash Course (history)  Operating System Basics (device driver, scheduler, memory management)  Question: Why do we need an operating system? When do we not needing them? "},{"title":"Hardware Abstraction through API and \"Files\"","type":1,"pageTitle":"Basics - Linux OS Concepts","url":"docs/linux/getting-started/basics-linux-os/#hardware-abstraction-through-api-and-files","content":"The necessity of an operating system arise when a single piece of user program need to run on possibly different hardware. Without an operating system, the programmer need to write the program in such a way that directly interfaces with the hardware (bare-metal programming). This makes it an almost impossible task to write a generic program that could run on wide variaty of machines, which is a pretty bad situation. Fortunately, an operating system is able to provide an hardware-neutral abstraction layer for the programmer to work with. For Linux specifically, this abstraction is achieved through Linux API, or more specifically the System Call Interface. Here is a list of Syscalls for your reference, some of the good examples are read, write, poll and ioctl. (Many of them are actually POSIX-compliant, for example poll from the Standard). Within the operating system, the intricate dealing with the specific hardware devices is handled by device drivers, written by the hardware vendor or the open source community.  As you can tell, most of these function calls (API) are pretty high-level (which means we don't have to fuss around the register-level configuration of hardware), and most objects and devices to operate upon are abstrated as files (or more specifically file descriptors, or fd for short). This is a pretty powerful idea for the abstraction to be done this way, as it turns out to be a very flexible design to treat vitually everything as file objects. Most devices should be found by one of the following: navigating in the /dev/ directorylsusb for USB deviceslspci for PCI-e devices Exercice: Obtain as much as information about your hardware devices using the commands above. \"Everything is a file\" in UNIX  Lastly, here is a more in-depth video regarding the under-the-hood operations behind some of the syscalls: UNIX System Calls (YouTube, 50min). "},{"title":"Multi-threading","type":1,"pageTitle":"Basics - Linux OS Concepts","url":"docs/linux/getting-started/basics-linux-os/#multi-threading","content":"In addition, for the longest time, our CPU-based machines runs program code in a sequential manner (per CPU thread), which does not have a concept of running multiple program code in a time-sharing manner. Therefore, another important role an operating system plays is to implement a scheduler to allow multiple program code to run on a single physical CPU thread, by time-sharing. This is the key feature that enable the user to run almost unlimited program on a machine, regardless the number of CPU cores and threads the physical machine has. "},{"title":"User Interface","type":1,"pageTitle":"Basics - Linux OS Concepts","url":"docs/linux/getting-started/basics-linux-os/#user-interface","content":"Operating system also provides user interfaces through either terminals (command prompt) or a graphical interface (desktop environment, the X Window system in Linux). It is capable of handling multiple user inputs at the same time. Getting to use terminal (and technically, its underlying shell program, e.g. Bash) is important for using Linux. Why? Immediately after this section, we will jump straight into using the Bash shell. The interface through the commandline is normally achieved through stdin / stdout, where text data (and possibly raw binary data) can flow in and out. Unix terminals and shells (history of terminal, terminal character device file, stdin/stdout, terminal emulator, X Window system)  The most important thing to note here is that virtually all program on Linux natively support user interface through terminals. On Linux, the glue between the terminal and the actual running program is the shell, which is responsible for parsing the commandline line input obtained from the terminal like cd or mkdir new_directory to execute the right commands. Later on we will learn extensively about the Bash shell. Exercies: List all the tty devices on your machine, and can you identify some of their usage? Exercies: Could you stop your graphical interface and obtain a pure tty terminal? (How to stop the X Server?) "},{"title":"Memory Vitualisation and Protection","type":1,"pageTitle":"Basics - Linux OS Concepts","url":"docs/linux/getting-started/basics-linux-os/#memory-vitualisation-and-protection","content":"We don't need to get very in-depth here, but the important concept is that every program executed by the OS is run on its own virtual memory. This design provides security between program processes. The second thing is that as the memory is virtual, some of the memory address can actually exist on hard disks (or SSDs)! That is precisely the use of the swap partition. It will prevent hanging when the physical memory runs out under high load. "},{"title":"Knowing Liunx OS","type":1,"pageTitle":"Basics - Linux OS Concepts","url":"docs/linux/getting-started/basics-linux-os/#knowing-liunx-os","content":"Linux Foundation - Chapter 2 (Edx) Boot Process from LinuxJourney "},{"title":"Questions for Thought","type":1,"pageTitle":"Basics - Linux OS Concepts","url":"docs/linux/getting-started/basics-linux-os/#questions-for-thought","content":"Try explaining in your own words: Where can we find Linux being used in everyday life?Why is filesystem concept so important in Linux? What can a file represent?What role does Linux Kernel play within a computer system? More specific terminologies: What are the main distributions of Linux? How is each Linux distribution related to Linux kernel?What does a bootloader do?What is the relationship between BIOS(MBR), UEFI and GRUB?What is X Window system? Further readings: CS5250 Advanced Operating Systems (Lecture 3 - Boot Process) "},{"title":"Install Ubuntu from USB Drive","type":0,"sectionRef":"#","url":"docs/linux/getting-started/installation/","content":"","keywords":""},{"title":"Installation Guide","type":1,"pageTitle":"Install Ubuntu from USB Drive","url":"docs/linux/getting-started/installation/#installation-guide","content":"Ask for the USB Flash Drive with the Ubuntu 18.04 Intaller Image. Boot your laptop / computer into the USB Drive to start the installation. A few helpful guides: Installation guide (linuxtechi)More on dual boot (itzgeek)Regarding boot options (phoenixnap). caution Always try to boot using UEFI mode, instead of the legacy BIOS mode. This will make sure the Ubuntu uses its UEFI bootloader after installation. Make sure to disable legacy mode or secure boot, if your system has this option. ReferenceRemember to give sufficient swap space during installation's disk partitioning, to protect the OS from hanging when the physical memory running low. ReferenceConnect to a LAN Internet connection if possible, so update downloads could be much faster "},{"title":"Prepare the USB Flash Drive Yourself","type":1,"pageTitle":"Install Ubuntu from USB Drive","url":"docs/linux/getting-started/installation/#prepare-the-usb-flash-drive-yourself","content":"We currently use Ubuntu 18.04 LTS as the standard OS, and the download link is available here: choose ubuntu-18.04.x-desktop-amd64.isoUse any USB image flashing tool to do it, for example the universal usb installer on Windows, or the cross-platform Etcher.A good reference tutorial is available here: https://www.guru99.com/install-linux.html "},{"title":"Gnome Extensions","type":0,"sectionRef":"#","url":"docs/linux/getting-started/gnome-shell-extensions/","content":"","keywords":""},{"title":"System Monitor Extension","type":1,"pageTitle":"Gnome Extensions","url":"docs/linux/getting-started/gnome-shell-extensions/#system-monitor-extension","content":"Install gnome-tweak by sudo apt install gnome-tweak-toolInstall sudo apt install gnome-shell-extension-system-monitorEnable it in gnome tweaks GUI Reference: browse to https://extensions.gnome.org/, system-monitor "},{"title":"Tutorials Overview","type":0,"sectionRef":"#","url":"docs/linux/getting-started/overview/","content":"Tutorials Overview The tutorials are organised as the following: Day\tTopics\tDescription 1\tLinux OS Concepts\tLinux API, Philosophy, Distributions, File Descriptor, Terminal, Boot Process 2\tBash & File Systems\tDescription 3\tMore Bash Shell\tDescription","keywords":""},{"title":"Grub","type":0,"sectionRef":"#","url":"docs/linux/kernel/grub-default-kernel/","content":"","keywords":""},{"title":"Change Default Kernel Version","type":1,"pageTitle":"Grub","url":"docs/linux/kernel/grub-default-kernel/#change-default-kernel-version","content":"Reference Obtain the kernel id string from grep gnulinux /boot/grub/grub.cfgEdit /etc/default/grub GRUB_DEFAULT=<the kernel you want>sudo update-grub "},{"title":"Setup Samba Server","type":0,"sectionRef":"#","url":"docs/linux/getting-started/intermediate-network-drive-samba/","content":"Setup Samba Server You need to create UNIX account (adduser), before adding the samba usersudo smbpasswd -a <user> Restart the service by sudo systemctl restart smbd.service Sample Samba Setup could be found below /etc/samba/smb.conf ## Sample configuration file for the Samba suite for Debian GNU/Linux.### This is the main Samba configuration file. You should read the# smb.conf(5) manual page in order to understand the options listed# here. Samba has a huge number of configurable options most of which# are not shown in this example## Some options that are often worth tuning have been included as# commented-out examples in this file.# - When such options are commented with \";\", the proposed setting# differs from the default Samba behaviour# - When commented with \"#\", the proposed setting is the default# behaviour of Samba but the option is considered important# enough to be mentioned here## NOTE: Whenever you modify this file you should run the command# \"testparm\" to check that you have not made any basic syntactic# errors. #======================= Global Settings ======================= [global] ## Browsing/Identification ### # Change this to the workgroup/NT-domain name your Samba server will part of workgroup = WORKGROUP # server string is the equivalent of the NT Description field server string = %h server (Samba, Ubuntu) # Windows Internet Name Serving Support Section:# WINS Support - Tells the NMBD component of Samba to enable its WINS Server# wins support = no # WINS Server - Tells the NMBD components of Samba to be a WINS Client# Note: Samba can be either a WINS Server, or a WINS Client, but NOT both; wins server = w.x.y.z # This will prevent nmbd to search for NetBIOS names through DNS. dns proxy = no #### Networking #### # The specific set of interfaces / networks to bind to# This can be either the interface name or an IP address/netmask;# interface names are normally preferred; interfaces = 127.0.0.0/8 eth0 # Only bind to the named interfaces and/or networks; you must use the# 'interfaces' option above to use this.# It is recommended that you enable this feature if your Samba machine is# not protected by a firewall or is a firewall itself. However, this# option cannot handle dynamic or non-broadcast interfaces correctly.; bind interfaces only = yes #### Debugging/Accounting #### # This tells Samba to use a separate log file for each machine# that connects log file = /var/log/samba/log.%m # Cap the size of the individual log files (in KiB). max log size = 1000 # If you want Samba to only log through syslog then set the following# parameter to 'yes'.# syslog only = no # We want Samba to log a minimum amount of information to syslog. Everything# should go to /var/log/samba/log.{smbd,nmbd} instead. If you want to log# through syslog you should set the following parameter to something higher. syslog = 0 # Do something sensible when Samba crashes: mail the admin a backtrace panic action = /usr/share/samba/panic-action %d ####### Authentication ####### # Server role. Defines in which mode Samba will operate. Possible# values are \"standalone server\", \"member server\", \"classic primary# domain controller\", \"classic backup domain controller\", \"active# directory domain controller\".## Most people will want \"standalone sever\" or \"member server\".# Running as \"active directory domain controller\" will require first# running \"samba-tool domain provision\" to wipe databases and create a# new domain. server role = standalone server # If you are using encrypted passwords, Samba will need to know what# password database type you are using. passdb backend = tdbsam obey pam restrictions = yes # This boolean parameter controls whether Samba attempts to sync the Unix# password with the SMB password when the encrypted SMB password in the# passdb is changed. unix password sync = yes # For Unix password sync to work on a Debian GNU/Linux system, the following# parameters must be set (thanks to Ian Kahan <<kahan@informatik.tu-muenchen.de> for# sending the correct chat script for the passwd program in Debian Sarge). passwd program = /usr/bin/passwd %u passwd chat = *Enter\\snew\\s*\\spassword:* %n\\n *Retype\\snew\\s*\\spassword:* %n\\n *password\\supdated\\ssuccessfully* . # This boolean controls whether PAM will be used for password changes# when requested by an SMB client instead of the program listed in# 'passwd program'. The default is 'no'. pam password change = yes # This option controls how unsuccessful authentication attempts are mapped# to anonymous connections map to guest = bad user ########## Domains ########### ## The following settings only takes effect if 'server role = primary# classic domain controller', 'server role = backup domain controller'# or 'domain logons' is set# # It specifies the location of the user's# profile directory from the client point of view) The following# required a [profiles] share to be setup on the samba server (see# below); logon path = \\\\%N\\profiles\\%U# Another common choice is storing the profile in the user's home directory# (this is Samba's default)# logon path = \\\\%N\\%U\\profile # The following setting only takes effect if 'domain logons' is set# It specifies the location of a user's home directory (from the client# point of view); logon drive = H:# logon home = \\\\%N\\%U # The following setting only takes effect if 'domain logons' is set# It specifies the script to run during logon. The script must be stored# in the [netlogon] share# NOTE: Must be store in 'DOS' file format convention; logon script = logon.cmd # This allows Unix users to be created on the domain controller via the SAMR# RPC pipe. The example command creates a user account with a disabled Unix# password; please adapt to your needs; add user script = /usr/sbin/adduser --quiet --disabled-password --gecos \"\" %u # This allows machine accounts to be created on the domain controller via the# SAMR RPC pipe.# The following assumes a \"machines\" group exists on the system; add machine script = /usr/sbin/useradd -g machines -c \"%u machine account\" -d /var/lib/samba -s /bin/false %u # This allows Unix groups to be created on the domain controller via the SAMR# RPC pipe.; add group script = /usr/sbin/addgroup --force-badname %g ############ Misc ############ # Using the following line enables you to customise your configuration# on a per machine basis. The %m gets replaced with the netbios name# of the machine that is connecting; include = /home/samba/etc/smb.conf.%m # Some defaults for winbind (make sure you're not using the ranges# for something else.); idmap uid = 10000-20000; idmap gid = 10000-20000; template shell = /bin/bash # Setup usershare options to enable non-root users to share folders# with the net usershare command. # Maximum number of usershare. 0 (default) means that usershare is disabled.; usershare max shares = 100 # Allow users who've been granted usershare privileges to create# public shares, not just authenticated ones usershare allow guests = yes #======================= Share Definitions ======================= # Un-comment the following (and tweak the other settings below to suit)# to enable the default home directory shares. This will share each# user's home directory as \\\\server\\username;[homes]; comment = Home Directories; browseable = no # By default, the home directories are exported read-only. Change the# next parameter to 'no' if you want to be able to write to them.; read only = yes # File creation mask is set to 0700 for security reasons. If you want to# create files with group=rw permissions, set next parameter to 0775.; create mask = 0700 # Directory creation mask is set to 0700 for security reasons. If you want to# create dirs. with group=rw permissions, set next parameter to 0775.; directory mask = 0700 # By default, \\\\server\\username shares can be connected to by anyone# with access to the samba server.# Un-comment the following parameter to make sure that only \"username\"# can connect to \\\\server\\username# This might need tweaking when using external authentication schemes; valid users = %S # Un-comment the following and create the netlogon directory for Domain Logons# (you need to configure Samba to act as a domain controller too.);[netlogon]; comment = Network Logon Service; path = /home/samba/netlogon; guest ok = yes; read only = yes # Un-comment the following and create the profiles directory to store# users profiles (see the \"logon path\" option above)# (you need to configure Samba to act as a domain controller too.)# The path below should be writable by all users so that their# profile directory may be created the first time they log on;[profiles]; comment = Users profiles; path = /home/samba/profiles; guest ok = no; browseable = no; create mask = 0600; directory mask = 0700 [printers] comment = All Printers browseable = no path = /var/spool/samba printable = yes guest ok = no read only = yes create mask = 0700 # Windows clients look for this share name as a source of downloadable# printer drivers[print$] comment = Printer Drivers path = /var/lib/samba/printers browseable = yes read only = yes guest ok = no# Uncomment to allow remote administration of Windows print drivers.# You may need to replace 'lpadmin' with the name of the group your# admin users are members of.# Please note that you also need to set appropriate Unix permissions# to the drivers directory for these users to have write rights in it; write list = root, @lpadmin [home]path = /home/huiminpcvalid users = huiminpccreate mask = 0664directory mask = 0775browseable = yeswritable = yesguest ok = nofollow symlinks = yes [homes]comment = Home Directoriesvalid users = %Sread only = Nocreate mask = 0664directory mask = 0775browseable = No Copy","keywords":""},{"title":"Nvidia Intel Graphics Driver","type":0,"sectionRef":"#","url":"docs/linux/kernel/nvidia-intel-driver/","content":"","keywords":""},{"title":"Option 1","type":1,"pageTitle":"Nvidia Intel Graphics Driver","url":"docs/linux/kernel/nvidia-intel-driver/#option-1","content":"go to Nvidia to download the latest driver in .run formatgo to single-user mode and uninstall existing nvidia drivers sudo apt-get purge nvidia-*You may need to perform apt autoremove to remove stray nvidia related packages.  $ init 1$ sudo apt-get purge nvidia-* Copy Verify That Nouveau Driver is Disabled: remove nvidia related config files under /etc/modprobe.d/ and /lib/modprobe.d/ excute the downloaded run file $ ./NVIDIA-Linux-x86_64-440.44.run Copy update the following lines in grub file $ nano /etc/default/grub GRUB_CMDLINE_LINUX_DEFAULT=\"modprobe.blacklist=nouveau\"GRUB_CMDLINE_LINUX=\"\" Copy $ sudo update-grub verify the running kernel module$ prime-select query . Switch to nvidia if the current version is intel: $ prime-select nvidia Verify That Nouveau Driver is Disabled grep nouveau /var/log/Xorg.0.log Copy "},{"title":"Option 2","type":1,"pageTitle":"Nvidia Intel Graphics Driver","url":"docs/linux/kernel/nvidia-intel-driver/#option-2","content":"Open Software & UpdatesTick first four optionsNVIDIA device drivers can be found in 'Additional Drivers'After the drivers have been installed, reboot "},{"title":"Install CUDA if necessary","type":1,"pageTitle":"Nvidia Intel Graphics Driver","url":"docs/linux/kernel/nvidia-intel-driver/#install-cuda-if-necessary","content":"install CUDA 10 from official website (.deb), and follow the instructions to add apt-key. Lastly: sudo apt install cuda In .bashrc add: # CUDAexport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}$export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Copy Install CUDA DNN sudo apt install libcudnn7 Copy Official Installation Guide (CUDA) "},{"title":"USB","type":0,"sectionRef":"#","url":"docs/linux/kernel/usb/","content":"","keywords":""},{"title":"Buffer Setup","type":1,"pageTitle":"USB","url":"docs/linux/kernel/usb/#buffer-setup","content":"Temporarysudo sh -c 'echo 1000 > /sys/module/usbcore/parameters/usbfs_memory_mb'Permanent in /etc/default/grubGRUB_CMDLINE_LINUX_DEFAULT=\"usbcore.usbfs_memory_mb=1000\" for TX2, this should lie in /boot/extlinux/extlinux.conf. Add after ${cbootargs} usbcore.usbfs_memory_mb=1000 "},{"title":"GStreamer","type":0,"sectionRef":"#","url":"docs/linux/packages/gstreamer/","content":"","keywords":""},{"title":"Installation","type":1,"pageTitle":"GStreamer","url":"docs/linux/packages/gstreamer/#installation","content":"sudo apt install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio sudo apt install libgstreamer-plugins-base1.0-dev Copy "},{"title":"Tutorials","type":1,"pageTitle":"GStreamer","url":"docs/linux/packages/gstreamer/#tutorials","content":"Using GStreamer (YouTube) About UVC mechanism "},{"title":"File Systems","type":0,"sectionRef":"#","url":"docs/linux/packages/file-systems/","content":"","keywords":""},{"title":"exFAT","type":1,"pageTitle":"File Systems","url":"docs/linux/packages/file-systems/#exfat","content":"sudo apt install exfat-fuse exfat-utils "},{"title":"NTFS","type":1,"pageTitle":"File Systems","url":"docs/linux/packages/file-systems/#ntfs","content":"sudo apt install ntfs-3g "},{"title":"Latex","type":0,"sectionRef":"#","url":"docs/linux/packages/latex/","content":"","keywords":""},{"title":"Installation on Linux","type":1,"pageTitle":"Latex","url":"docs/linux/packages/latex/#installation-on-linux","content":"Install latex-workshop plugin in VS CodeInstall TexLive by sudo apt install texlive-latex-extra latexmk. reference "},{"title":"Ngnix","type":0,"sectionRef":"#","url":"docs/linux/packages/nginx/","content":"Ngnix sudo apt install nginx Copy https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-ubuntu-18-04","keywords":""},{"title":"Node.js","type":0,"sectionRef":"#","url":"docs/linux/packages/nodejs/","content":"","keywords":""},{"title":"Installation on Ubuntu (18.04/16.04)","type":1,"pageTitle":"Node.js","url":"docs/linux/packages/nodejs/#installation-on-ubuntu-18041604","content":"Reference ## the following line is not needed for Ubuntu 18.04 as the dependencies are installed already# sudo apt-get install curl python-software-propertiescurl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -sudo apt-get install nodejs Copy To verify version 12 has been installed successfully: node -v npm -v Copy "},{"title":"Install yarn package manager","type":1,"pageTitle":"Node.js","url":"docs/linux/packages/nodejs/#install-yarn-package-manager","content":"NOTE: Many have said yarn is no longer relevant, as it no longer has much advantages over Node.jshttps://yarnpkg.com/lang/en/docs/install/#debian-stable curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.listsudo apt-get update && sudo apt-get install yarn Copy history below  "},{"title":"Fix npm Global Install Permissions","type":1,"pageTitle":"Node.js","url":"docs/linux/packages/nodejs/#fix-npm-global-install-permissions","content":"First check, where npm point to, if you call: npm config get prefix If /usr is returned, you can do the following: mkdir ~/.npm-globalexport NPM_CONFIG_PREFIX=~/.npm-globalexport PATH=$PATH:~/.npm-global/bin Copy To make it permanent, add the export items in the .bashrc "},{"title":"OpenCV (with CUDA)","type":0,"sectionRef":"#","url":"docs/linux/packages/opencv/","content":"","keywords":""},{"title":"Dependencies","type":1,"pageTitle":"OpenCV (with CUDA)","url":"docs/linux/packages/opencv/#dependencies","content":"apt install the following libopenjpip7 libopenjp2-toolslibatlas-base-dev gfortranliblapacke-dev libatlas-base-dev "},{"title":"Install OpenCV 4 from Source","type":1,"pageTitle":"OpenCV (with CUDA)","url":"docs/linux/packages/opencv/#install-opencv-4-from-source","content":"Clone repos of OpenCV and OpenCV Contrib, andCheckout the matching release (e.g. 4.1.0) cd ~/opencvmkdir buildcd build cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D INSTALL_C_EXAMPLES=OFF \\ -D OPENCV_ENABLE_NONFREE=ON \\ -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules \\ -D PYTHON3_EXECUTABLE=/usr/bin/python3 \\ -D PYTHON2_EXECUTABLE=/usr/bin/python2 \\ -D BUILD_EXAMPLES=ON \\ -D WITH_CUDA=ON \\ -D CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda \\ -D CUDA_ARCH_BIN=\"6.2\" \\ -D BUILD_opencv_cudacodec=OFF \\ .. Copy TX2 uses 6.2; For CUDA_ARCH_BIN versions, please refer to GPU Compute CapabilityNote: cuda10以上没有dynlink_nvcuvid.h和nvcuvid.h,所以要将BUILD_opencv_cudacodec=OFF Reference 1 Reference 2 "},{"title":"ROSlib.js","type":0,"sectionRef":"#","url":"docs/linux/ros/roslib.js/","content":"ROSlib.js https://msadowski.github.io/ros-web-tutorial-pt1/","keywords":""},{"title":"Installing Catkin","type":0,"sectionRef":"#","url":"docs/linux/ros/using-catkin-build/","content":"","keywords":""},{"title":"Install Missing Dependencies Automatically","type":1,"pageTitle":"Installing Catkin","url":"docs/linux/ros/using-catkin-build/#install-missing-dependencies-automatically","content":"# Navigate to the root path of the caktin workspace, e.g. cd /home/user/catkin_ws/rosdep install --from-paths src --ignore-src -r -y Copy "},{"title":"Source catkin_ws in bashrc","type":1,"pageTitle":"Installing Catkin","url":"docs/linux/ros/using-catkin-build/#source-catkin_ws-in-bashrc","content":"# Navigate to the root path of the catkin workspace, e.g. cd /home/bashrc, CTRL+h to show the bashrc filesource /home/catkin_ws/devel/setup.bash Copy Reference:http://wiki.ros.org/rosdep "},{"title":"Installation of ROS","type":0,"sectionRef":"#","url":"docs/linux/ros/installation/","content":"","keywords":""},{"title":"Installation","type":1,"pageTitle":"Installation of ROS","url":"docs/linux/ros/installation/#installation","content":""},{"title":"1.1 Setup your sources.list","type":1,"pageTitle":"Installation of ROS","url":"docs/linux/ros/installation/#11-setup-your-sourceslist","content":"sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' Copy "},{"title":"1.2 Setup your keys","type":1,"pageTitle":"Installation of ROS","url":"docs/linux/ros/installation/#12-setup-your-keys","content":"sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 Copy tip If you experience issues connecting to the keyserver, you can try substituting hkp://pgp.mit.edu:80 or hkp://keyserver.ubuntu.com:80 in the previous command. "},{"title":"2.0 Make sure Debian package index is up-to-date","type":1,"pageTitle":"Installation of ROS","url":"docs/linux/ros/installation/#20-make-sure-debian-package-index-is-up-to-date","content":"sudo apt update Copy "},{"title":"2.1 Install ROS (Full Desktop Version)","type":1,"pageTitle":"Installation of ROS","url":"docs/linux/ros/installation/#21-install-ros-full-desktop-version","content":"Desktop-Full Install: (Recommended) : ROS, rqt, rviz, robot-generic libraries, 2D/3D simulators and 2D/3D perception sudo apt install ros-melodic-desktop-full Copy "},{"title":"2.2 Environment setup","type":1,"pageTitle":"Installation of ROS","url":"docs/linux/ros/installation/#22-environment-setup","content":"It's convenient if the ROS environment variables are automatically added to your bash session every time a new shell is launched: echo \"source /opt/ros/melodic/setup.bash\" >> ~/.bashrcsource ~/.bashrc Copy tip If you have more than one ROS distribution installed, ~/.bashrc must only source the setup.bash for the version you are currently using. If you just want to change the environment of your current shell, instead of the above you can type source /opt/ros/melodic/setup.bash Copy "},{"title":"3.0 Dependencies for building packages","type":1,"pageTitle":"Installation of ROS","url":"docs/linux/ros/installation/#30-dependencies-for-building-packages","content":"To install this tool and other dependencies for building ROS packages, run: sudo apt install python-rosdep python-rosinstall python-rosinstall-generator python-wstool build-essential Copy "},{"title":"3.1 Initializing rosdep","type":1,"pageTitle":"Installation of ROS","url":"docs/linux/ros/installation/#31-initializing-rosdep","content":"Before you can use many ROS tools, you will need to initialize rosdep. rosdep enables you to easily install system dependencies for source you want to compile and is required to run some core components in ROS. If you have not yet installed rosdep, do so as follows: sudo apt install python-rosdep Copy With the following, you can initialize rosdep. sudo rosdep initrosdep update Copy @yongtian "},{"title":"cmake","type":0,"sectionRef":"#","url":"docs/productivity/cmake_debug/cmake/","content":"cmake","keywords":""},{"title":"Static and Dynamic Libraries in Linux","type":0,"sectionRef":"#","url":"docs/productivity/cmake_debug/debugging/","content":"Static and Dynamic Libraries in Linux http://www.techytalk.info/c-cplusplus-library-programming-on-linux-part-one-static-libraries/ use of ldd command","keywords":""},{"title":"Git Practices","type":0,"sectionRef":"#","url":"docs/productivity/git/git/","content":"","keywords":""},{"title":"Git Submodule","type":1,"pageTitle":"Git Practices","url":"docs/productivity/git/git/#git-submodule","content":""},{"title":"To make the git push work","type":1,"pageTitle":"Git Practices","url":"docs/productivity/git/git/#to-make-the-git-push-work","content":"git config --global push.default matching "},{"title":"Modify the .gitmodules file correctly","type":1,"pageTitle":"Git Practices","url":"docs/productivity/git/git/#modify-the-gitmodules-file-correctly","content":"git config -f .gitmodules submodule.edt.branch linear_dt "},{"title":"To convert the existing submodule to the correct branch","type":1,"pageTitle":"Git Practices","url":"docs/productivity/git/git/#to-convert-the-existing-submodule-to-the-correct-branch","content":"if the local branch does no exist yet: git checkout -b linear_dt --track origin/linear_dt else, the local branch with the same name already exist, do git branch -u origin/linear_dt "},{"title":"To update submodules after checkingout to a different branch (with different versions of submodules)","type":1,"pageTitle":"Git Practices","url":"docs/productivity/git/git/#to-update-submodules-after-checkingout-to-a-different-branch-with-different-versions-of-submodules","content":"git checkout <branch>git reset --hardgit submodule foreach --recursive 'git checkout -- . || :'git submodule update --init --recursivegit clean -d -f -f -xgit submodule foreach --recursive git clean -d -f -f -x Copy "},{"title":"Alias","type":1,"pageTitle":"Git Practices","url":"docs/productivity/git/git/#alias","content":"add the following file content to /.git/config add git sm-trackbranch [alias]#git sm-trackbranch : places all submodules on their respective branch specified in .gitmodules#This works if submodules are configured to track a branch, i.e if .gitmodules looks like :#[submodule \"my-submodule\"]# path = my-submodule# url = git@wherever.you.like/my-submodule.git# branch = my-branchsm-trackbranch = \"! git submodule foreach --recursive 'branch=\\\"$(git config -f $toplevel/.gitmodules submodule.$name.branch)\\\"; git checkout $branch'\" #sm-pullrebase :# - pull --rebase on the master repo# - sm-trackbranch on every submodule# - pull --rebase on each submodule## Important note :#- have a clean master repo and subrepos before doing this !#- this is *not* equivalent to getting the last committed # master repo + its submodules: if some submodules are tracking branches # that have evolved since the last commit in the master repo,# they will be using those more recent commits !## (Note : On the contrary, git submodule update will stick #to the last committed SHA1 in the master repo)#sm-pullrebase = \"! git pull --rebase; git submodule update; git sm-trackbranch ; git submodule foreach 'git pull --rebase' \" # git sm-diff will diff the master repo *and* its submodulessm-diff = \"! git diff && git submodule foreach 'git diff' \" #git sm-push will ask to push also submodulessm-push = push --recurse-submodules=on-demand #git alias : list all aliasesalias = \"!git config -l | grep alias | cut -c 7-\" Copy "},{"title":"pangolin","type":0,"sectionRef":"#","url":"docs/productivity/uiux/pangolin/","content":"pangolin","keywords":""},{"title":"roslibjs","type":0,"sectionRef":"#","url":"docs/productivity/uiux/roslibjs/","content":"roslibjs","keywords":""},{"title":"VuePress","type":0,"sectionRef":"#","url":"docs/random/web-documentation/vuepress/","content":"","keywords":""},{"title":"Installation","type":1,"pageTitle":"VuePress","url":"docs/random/web-documentation/vuepress/#installation","content":"(Make sure Node.js is installed, like here) To install VuePress 1.x globally is simple: npm install vuepress -g Copy vuepress dev docs starts a development server with automatic reload and all the dev goodies. This is the command you'll use when developing your website.vuepress build docs builds your website. The generated assets will be ready to deploy on any hosting service. "},{"title":"Automatic Generation of Sidebar","type":1,"pageTitle":"VuePress","url":"docs/random/web-documentation/vuepress/#automatic-generation-of-sidebar","content":"npm install vuepress-bar -g Copy Modify docs/.vuepress/config.js file: const getConfig = require(\"vuepress-bar\");const barConfig = getConfig(`${__dirname}/..`,{skipEmptySidebar: false, addReadMeToFirstGroup:false}) //// Full Automatic// module.exports = {// themeConfig: {// ...barConfig,// displayAllHeaders: true // Default: false// }// }; //// Mixed stylemodule.exports = { themeConfig: { nav: [ { text: 'Home', link: '/' }, // { text: 'Tutorials', link: '/1-tutorials/' }, // { text: 'External', link: 'https://google.com' }, ...barConfig.nav], sidebar: barConfig.sidebar, lastUpdated: 'Last Updated', editLinks: false, displayAllHeaders: true // Default: false }}; Copy "},{"title":"Serve The Website Locally","type":1,"pageTitle":"VuePress","url":"docs/random/web-documentation/vuepress/#serve-the-website-locally","content":"You have a few options: vuepress dev docs Copy or python3 -m http.server Copy or npm install http-server -g # first-time installhttp-server Copy "},{"title":"Gitbook Cli","type":0,"sectionRef":"#","url":"docs/random/web-documentation/gitbook-cli/","content":"","keywords":""},{"title":"How to install Gitbook Legacy","type":1,"pageTitle":"Gitbook Cli","url":"docs/random/web-documentation/gitbook-cli/#how-to-install-gitbook-legacy","content":"To install the command line interface: Install Nodejs (with NPM)npm install gitbook-cli -gnpm install gitbook-pdf -g To verify installation: gitbook -V "},{"title":"Install LaTex Support","type":1,"pageTitle":"Gitbook Cli","url":"docs/random/web-documentation/gitbook-cli/#install-latex-support","content":"Reference For KaTex { \"plugins\": [\"katex\"]} Copy For MathJax { \"plugins\": [\"mathjax\"]} Copy gitbook install "},{"title":"Publish Static Pages to GitHub","type":1,"pageTitle":"Gitbook Cli","url":"docs/random/web-documentation/gitbook-cli/#publish-static-pages-to-github","content":"To avoid troublesome branch switching and files copying, simply use a public npm package gh-pages. To setup: sudo npm install -g gh-pages # to install globallyecho \"require('gh-pages').publish('_book', function(err) {});\" > publish.js # to create script file for node Copy To update the static site, simply do (it will not commit the master for you): node publish Copy Previous Non-Working Insturctions Another npm package tried gitbook-publish to update gh-pages branch in GitHub. The problem is the script does not switch back to master branch after running. To update the static site, simply do: gitpub Copy Try to use worktree to resolve the problem, refer to this gist git worktree add -b gh-pages ../gh-pages origin/gh-pages Copy Try to use subtree to resolve the problem gist gitbook build # build html to _book directorygit add . # add all changes to commit on master branch git subtree push --prefix _book origin gh-pages # put _book into a subtree on gh-pages branch Copy Tip: to remove ignored files that are previously tracked git rm -r --cached _book Copy "},{"title":"Calibration results","type":0,"sectionRef":"#","url":"docs/research/calibration/Calibration results/Calibration results/","content":"","keywords":""},{"title":"Cam 1","type":1,"pageTitle":"Calibration results","url":"docs/research/calibration/Calibration results/Calibration results/#cam-1","content":"Calibration results yaml Calibration results txt Calibration report Calibration results yaml (17 Feb 2021) Calibration results txt (17 Feb 2021) Calibration report (17 Feb 2021) "},{"title":"Cam 2 (28 Jan 2021)","type":1,"pageTitle":"Calibration results","url":"docs/research/calibration/Calibration results/Calibration results/#cam-2-28-jan-2021","content":"Calibration results yaml (cameras)Calibration results yaml (cameras-imu) "},{"title":"Cam 3","type":1,"pageTitle":"Calibration results","url":"docs/research/calibration/Calibration results/Calibration results/#cam-3","content":"Calibration results yamlCalibration results txtCalibration report "},{"title":"Cam 4","type":1,"pageTitle":"Calibration results","url":"docs/research/calibration/Calibration results/Calibration results/#cam-4","content":"Calibration results yamlCalibration results txtCalibration reportVerification result "},{"title":"Calibration Procedures 123","type":0,"sectionRef":"#","url":"docs/research/calibration/calibration-procedures/","content":"","keywords":""},{"title":"For Multi-camera calibration","type":1,"pageTitle":"Calibration Procedures 123","url":"docs/research/calibration/calibration-procedures/#for-multi-camera-calibration","content":"Changing the pixhawk IMU frequency On the sd card that is to be inserted into the pixhawk device: Create a new folder named \"etc\" Create a new .txt file named \"extras.txt\" mavlink stream -d/dev/ttyACM0 -s ATTITUDE -r 100 mavlink stream -d/dev/ttyACM0 -s ATTITUDEQUATERNION -r 100 mavlink stream -d/dev/ttyACM0 -s HIGHRES_IMU -r 100 Save the .txt file, insert into pixhawk device, reboot device. PX4 IMU will be changed to 100Hz. Comfirm in mavlink inspector (Qgroundcontrol) or imu/data (RQT). Changing the trigger frequencyNavigate to Qgroundcontrol->settings->cameras Adjust the follow settings Triger mode: Time based always on Trigger interface: GPIO Time interval: 250 ms Trigger Pin Polarity: High (3.3V) AUX Pin Assignment: Aux pin number Setting the focus of the camera.After mounting the cameras to the backplace, loosen the allen screw securing the lens element. Focus the camera against a target of further than 5m away. Operate the viewfinder through the camera interface tcam-capture Copy Zoom in to focus on a far away object. Once in focus secure allen screw and ensure that the lens element is immune to loosening, as adjusting the focus of the camera after the calibration process produces undesirable results. Identify the serial number of both cameras through tcam interface. Before calibrationLaunch roscore Roscore Copy Launch mavros roslaunch mavros px4.launch Copy Refer to device_list.yaml (catkin_ws/src/tiscamera_ros/launch/device_list.yaml). Update the camera serial number under fisheye_left, SN and fisheye_right, SN. Make the edited files in the catkin folder catkin make Copy Launch tiscamera roslaunch tiscamera_ros tiscamera_ros_drone23.launch Copy Launch RQT to access viewfinder during calibration rqt_image_view Copy Start the imaging capture by running tisbag script ./tisbag Copy If script is unavailable:  > rosbag record --lz4 /mavros/imu/data /tiscamera_ros/fisheye_left/camera_info /tiscamera_ros/fisheye_left/image_rect_raw /tiscamera_ros/fisheye_right/camera_info /tiscamera_ros/fisheye_right/image_rect_raw Copy If the script cannot be run due to a lack of permission, permission canbe added to file chmod +x tisbag Copy To review the bag(optional) Rosbag info bagname.bag Copy Recording processOrientate the board and the camera rig such that the april5x5 target board is 2-3 meters away and fills up around 30% of the screen size. Slowly orientate the rig/board such that the big is positioned (and still fully visible) at the outer edge of the camera vision. Slowly rotate the board/camera to cover the entire circumference of the camera vision. At each position briefly tilt the angle of the board that is facing the camera. Position the board/rig such that the board is in the center of the camera vision. Slowly move the board in a circular motion to ensure maximum coverage at the center of vision. Repeat the above processes for both left and right cameras. End the recording via ctrl-c. The recording will be saved in a date-time.bag file. Calibrating the cameras.If kalibr isnt install on the computer, you can process the calibration through another terminal that has kalibr installed. Update camera info in target, run-ds-none.sh with those in tisbag -/tiscamera_ros/fisheye_right/image_rect_raw -/tiscamera_ros/fisheye_left/image_rect_raw If script is unavailable kalibr_calibrate_cameras --target april_5x5.yaml --bag $1 --models ds-none ds-none --topics /tiscamera_ros/fisheye_left/image_rect_raw /tiscamera_ros/fisheye_right/image_rect_raw Run the tisbag script  ./run-ds-none.sh ./.bag file directory/bag-name.bag Copy Calibration report and camchain file will be generated. "},{"title":"Calibration of IMU-Camera(Optional)","type":1,"pageTitle":"Calibration Procedures 123","url":"docs/research/calibration/calibration-procedures/#calibration-of-imu-cameraoptional","content":"This step is usually not needed, as the calibration result is generally not accurate, especially in the relative position between the IMU and the cameras. It might only be useful to serve as a way to determine the mounting position and coordinate systems used in both IMU and the cameras. Update the parameters inside the imu_run.sh --bag <data-time.bag> --cam <camchain.date-time.yaml> --imu imu.yaml --target april_5x5.yaml Copy If script is unavailable #!/bin/bash kalibr_calibrate_imu_camera --bag $1 --cam $2 --imu $3 --target $4 "},{"title":"Calibration Targets & Params","type":0,"sectionRef":"#","url":"docs/research/calibration/calibration-target-params/","content":"","keywords":""},{"title":"Our Calibration Target","type":1,"pageTitle":"Calibration Targets & Params","url":"docs/research/calibration/calibration-target-params/#our-calibration-target","content":"It is strongly recommended to use Aprilgrid as the calibration target as partially visible calibration boards can be usedpose of the target is fully resolved (no flips) More details can be found on the Kalibr's wiki page, and a sample grid starting from detection id 0 is available here. Below is the current Aprilgrid calibration board we are using and its corresponding .yaml file.  april_5x5.yaml target_type: 'aprilgrid' #gridtype tagCols: 5 #number of apriltags tagRows: 5 #number of apriltags tagSize: 0.1500 #size of apriltag, edge to edge [m] tagSpacing: 0.30001 #ratio of space between tags to tagSize, for our apriltag spacing = 0.029 low_id: 25 #the lowest detection id within the grid, normally on the bottom-left Copy "},{"title":"About low_id","type":1,"pageTitle":"Calibration Targets & Params","url":"docs/research/calibration/calibration-target-params/#about-low_id","content":"Observe that the first grid (closest to the coordinate marker) in the board above has the same encoding as the 26-th grid in a typical board. This is because the board we are using is purpose-made to start from a different id other than the default zero. Therefore, the low_id field is to be added in the yaml file to tell the calibration algorithm about this fact. The stock Kalibr pacakges does not support defining the lowest id natively (it always assumes the id starts from 0), hence we have added the low_id feature, refering to the commit. "},{"title":"The IMU Config File","type":1,"pageTitle":"Calibration Targets & Params","url":"docs/research/calibration/calibration-target-params/#the-imu-config-file","content":"If camera-imu calibration is needed, then an additional IMU config file like below is needed. imu.yaml #Accelerometersaccelerometer_noise_density: 1.86e-03 #Noise density (continuous-time)accelerometer_random_walk: 4.33e-04 #Bias random walk #Gyroscopesgyroscope_noise_density: 1.87e-04 #Noise density (continuous-time)gyroscope_random_walk: 2.66e-05 #Bias random walk rostopic: /mavros/imu/data #the IMU ROS topicupdate_rate: 360.0 #Hz (for discretization of the values above) Copy "},{"title":"Getting Started with Kalibr","type":0,"sectionRef":"#","url":"docs/research/calibration/getting-started/","content":"","keywords":""},{"title":"Download and Build","type":1,"pageTitle":"Getting Started with Kalibr","url":"docs/research/calibration/getting-started/#download-and-build","content":"warning Do not download from official repository! This is because our calibration board is custom-made with different Apriltag (QR Code) IDs, the official code will not detect the corners correctly. Git Repository: https://github.com/chengguizi/kalibr (default master branch) Kalibr is a ROS package, so it is to be cloned into the src/ directory of the caktin workspace. This version builds on top of the official repository, which: works with a calibration board with non-zero low_idworks correctly with Double Sphere Camera Model Installation processs Ensure that ROS melodic and catkin are installed. Install the build and run dependencies: sudo apt-get install python-setuptools python-rosinstall ipython libeigen3-dev libboost-all-dev doxygen libopencv-dev ros-melodic-vision-opencv ros-melodic-image-transport-plugins ros-melodic-cmake-modules python-software-properties software-properties-common libpoco-dev python-matplotlib python-scipy python-git python-pip ipython libtbb-dev libblas-dev liblapack-dev python-catkin-tools libv4l-dev sudo pip install python-igraph --upgrade Copy Create a catkin workspace inside catkin folder mkdir -p ~/kalibr_workspace/src cd ~/kalibr_workspace source/opt/ros/melodic/setup.bash catkin init catkin config --extend /opt/ros/melodic catkin config --merge-devel # Necessary for catkin_tools >= 0.4. catkin config --cmake-args -DCMAKE_BUILD_TYPE=Release Copy Navigate to catkin's src folder, clone custom repo cd ~/kalibr_workspace/src git clone https://github.com/chengguizi/kalibr.git Copy Make kalibr catkin make kalibr Copy It takes a while to compile. Install Sophus sudo apt-get install ros-melodic-sophus Copy Once the build is finished you have to source the catkin workspace setup to use Kalibr source ~/kalibr_workspace/devel/setup.bash More on catkin build, refer here. To verify the installation, command kalibr_calibrate_cameras should exist. More on catkin build, refer here. To verify the installation, command kalibr_calibrate_cameras should exist. It is recommanded to use our calibration server instead. "},{"title":"Using Our Calibration Server","type":1,"pageTitle":"Getting Started with Kalibr","url":"docs/research/calibration/getting-started/#using-our-calibration-server","content":"However, we have a local server dedicated for calibration at IP address: 172.16.141.132 (NUS network) For detailed calibration steps, please follow these instructions. "},{"title":"Knowledge Required for Camera Calibration Work","type":1,"pageTitle":"Getting Started with Kalibr","url":"docs/research/calibration/getting-started/#knowledge-required-for-camera-calibration-work","content":"Familiar with Rotation group $SO(3)$ (special othorgnonal group in dimension 3) and its two common representations, including Rotation matrix $R$ (dimensions of $\\mathbb{R}^{3 \\times 3}$), and Quaternion $q$ (dimensions of $\\mathbb{R}^{4}$) Faimiliar with special Euclidean group $SE(3)$ and its representation as Homogenous transformation matrix $T$ (dimensions of $\\mathbb{R}^{4 \\times 4}$) Ability to express the rotation relationship between two coordinate frames, and be confortable with the notations like $Ti^c$ or $T{ic}$ or T_i_c (denotes camera in imu frame). "},{"title":"Reading Materials","type":1,"pageTitle":"Getting Started with Kalibr","url":"docs/research/calibration/getting-started/#reading-materials","content":"Basics on rotations and rigid-body motions Lecture 8 - 10 from http://vision.stanford.edu/teaching/cs131_fall1415/schedule.htmlNote on Camera Models https://web.stanford.edu/class/cs231a/course_notes/01-camera-models.pdfModern Robotics video series, first three videos of chapter 3 YouTubeAn Invitation to 3-D Vision, the first three chapters More regarding quaternions: Standarised Use of Quaternions [PDF] "},{"title":"Installation of Pangolin","type":0,"sectionRef":"#","url":"docs/research/calibration/Installing_Panglolin/","content":"","keywords":""},{"title":"Git repo","type":1,"pageTitle":"Installation of Pangolin","url":"docs/research/calibration/Installing_Panglolin/#git-repo","content":"Find the latest version on [Github] git clone https://github.com/stevenlovegrove/Pangolin.git Copy "},{"title":"Installing dependencies","type":1,"pageTitle":"Installation of Pangolin","url":"docs/research/calibration/Installing_Panglolin/#installing-dependencies","content":"C++11 OpenGL (Desktop / ES / ES2)sudo apt install libgl1-mesa-devGlewsudo apt install libglew-devPython3 sudo apt install libpython2.7-devsudo python -mpip install numpy pyopengl Pillow pybind11 Copy "},{"title":"Build","type":1,"pageTitle":"Installation of Pangolin","url":"docs/research/calibration/Installing_Panglolin/#build","content":"Pangolin uses the CMake portable pre-build tool. To checkout and build pangolin in the directory 'build', execute the following at a shell (or the equivelent using a GUI): git clone https://github.com/stevenlovegrove/Pangolin.gitcd Pangolinmkdir buildcd buildcmake ..cmake --build . Copy @yanfeng "},{"title":"Verify Camera Calibration by Kalibr Validator","type":0,"sectionRef":"#","url":"docs/research/calibration/verify-by-kalibr-validator/","content":"Verify Camera Calibration by Kalibr Validator Launch roscore Roscore Copy Launch tiscamera roslaunch tiscamera_ros tiscamera_ros_drone23.launch Copy Run the kalibr validator kalibr_camera_validator --cam camchain.yaml --target target.yaml Copy @yanfeng Reference:https://github.com/ethz-asl/kalibr/wiki/calibration-validator","keywords":""},{"title":"Take note","type":0,"sectionRef":"#","url":"docs/research/calibration/verify-by-triangulation-depth/","content":"","keywords":""},{"title":"Verify Calibration by Triangulation","type":1,"pageTitle":"Take note","url":"docs/research/calibration/verify-by-triangulation-depth/#verify-calibration-by-triangulation","content":"by triangulation @jalvin Verify the camera calibration by checking the depth of different points in the images. Ensure that you have the yaml calibration file ready and take a few photos with the cameras. Git Repository:https://github.com/STEMO-CS5340/position_proposal_estimator Clone and Build git clone (source link)# go to cloned directorygit submodule update --init --recursivemkdir buildcd buildcmake .. Copy Usage # open file src/depth_verification_test.cpp# set imageFileNames and yaml file# go to build directorymake ./depth_verification_test Copy Click on the same point in both images. Terminal will display the direction unit vertor and the distance between the point and origin, and also displays the world coordinates of the point in the next line. Verify that the distance is correct.  Take note Try to click on the point as precisely as possible. The algorithm is sensitive to slight changes, so selecting the point slightly off can give unexpected result.  "},{"title":"Kalibr Result Conventions","type":0,"sectionRef":"#","url":"docs/research/calibration/kalibr-result-conventions/","content":"","keywords":""},{"title":"Camera Model (Intrinsics)","type":1,"pageTitle":"Kalibr Result Conventions","url":"docs/research/calibration/kalibr-result-conventions/#camera-model-intrinsics","content":"Pinhole [fu fv pu pv]Double Sphere (ds): [xi alpha fu fv pu pv] "},{"title":"Transformation Matrices","type":1,"pageTitle":"Kalibr Result Conventions","url":"docs/research/calibration/kalibr-result-conventions/#transformation-matrices","content":"T_cn_cnm1: Transform cam0 (cnm1?) coordinates to cam1 (last camera, denoted as cn) coordinates (i.e. T_cn_cnm1 = T_c1_c0). Therefore it consists of: $R_{c_1c_0}$ or R_c1_c0 formed by basis vectors of cam0 frame in cam1 coordinates$t_{c_1c_0}$ or t_c1_c0 which is the position of the cam0 frame's origin in cam1 coordinates T_cam_imu Transform imu coordinates to camera coordnates. It consists of: $R_{ci}$ or R_c_i formed by basis vectors of imu frame in cam coordinates$t_{ci}$ or t_c_i which is the position of the imu frame's origin in the cam coordinates "},{"title":"Example Files","type":1,"pageTitle":"Kalibr Result Conventions","url":"docs/research/calibration/kalibr-result-conventions/#example-files","content":"camchain-2020-08-05-16-24-56.yaml cam0: cam_overlaps: [1] camera_model: ds distortion_coeffs: [] distortion_model: none intrinsics: [-0.19963086018860057, 0.5924590037637837, 317.552559595926, 316.1745713358917, 724.8551471226007, 546.5708052642561] resolution: [1440, 1080] rostopic: /tiscamera_ros/fisheye_left/image_rect_rawcam1: T_cn_cnm1: - [0.9994397899015358, 0.01583231520610129, 0.029486338477008038, -0.1982100882850154] - [-0.014069141538300084, 0.9981545353408604, -0.05907269110882186, -0.003226884424352984] - [-0.03036717994712894, 0.05862475052124023, 0.9978181061738522, 0.0003909535695235701] - [0.0, 0.0, 0.0, 1.0] cam_overlaps: [0] camera_model: ds distortion_coeffs: [] distortion_model: none intrinsics: [-0.19567833352036393, 0.5934771139662725, 318.54033139830346, 317.0520130252999, 712.7496608380312, 557.2224352962965] resolution: [1440, 1080] rostopic: /tiscamera_ros/fisheye_right/image_rect_raw Copy camchain-imucam-2020-08-05-16-54-55.yaml cam0: T_cam_imu: - [0.025462230671673247, -0.9992630992296517, -0.02872165258433318, 0.004244697665851643] - [0.04993886668011138, 0.029966538262921255, -0.9983026175360077, 0.003320133241428618] - [0.9984276560692467, 0.023984684748595442, 0.050665081600466455, 0.015700839664599384] - [0.0, 0.0, 0.0, 1.0] cam_overlaps: [1] camera_model: ds distortion_coeffs: [] distortion_model: none intrinsics: [-0.21697771106770586, 0.5920493574492033, 312.2320475101821, 311.05174019378734, 725.0950280082754, 545.2634687518469] resolution: [1440, 1080] rostopic: /tiscamera_ros/fisheye_left/image_rect_raw timeshift_cam_imu: 0.07650780716660849cam1: T_cam_imu: - [0.05736274985627546, -0.9977239680238397, -0.03544571285346676, -0.1921825607480451] - [-0.02404388231864263, 0.03411324897586468, -0.999128709410033, -0.003544617933287773] - [0.9980638289468132, 0.05816502278066027, -0.02203232790586701, 0.013584152644111497] - [0.0, 0.0, 0.0, 1.0] T_cn_cnm1: - [0.9994673874821907, 0.00835184517590277, 0.03154660111141055, -0.1969480349995947] - [-0.006003694095546954, 0.9972543376065331, -0.0738088191353987, -0.005671290908341935] - [-0.032076424625132306, 0.0735801114915753, 0.9967733293864537, -0.002174166631870157] - [0.0, 0.0, 0.0, 1.0] cam_overlaps: [0] camera_model: ds distortion_coeffs: [] distortion_model: none intrinsics: [-0.24136341506123252, 0.581315342400733, 301.55643035207885, 300.1877223496746, 714.9295565564267, 560.0563142309148] resolution: [1440, 1080] rostopic: /tiscamera_ros/fisheye_right/image_rect_raw timeshift_cam_imu: 0.04030756527247593 Copy "},{"title":"EDT Framework Overview","type":0,"sectionRef":"#","url":"docs/research/edt/edt-overview/","content":"","keywords":""},{"title":"Installation","type":1,"pageTitle":"EDT Framework Overview","url":"docs/research/edt/edt-overview/#installation","content":""},{"title":"ROS main function （edt_node_laser_realdrone.cpp）","type":1,"pageTitle":"EDT Framework Overview","url":"docs/research/edt/edt-overview/#ros-main-function-（edt_node_laser_realdronecpp）","content":"ROS basics# subscribe 4 topics: pose, laser scan, depth from ZED, confidence from ZED  scan_sub = nh.subscribe(\"/scan\", 1, &EDTNODE::get_scan, this); o_sub = nh.subscribe(\"/mavros/position/local\", 1, &EDTNODE::get_pose, this); depth_sub = nh.subscribe(\"/revised_sensor/image\",1,&EDTNODE::get_depth, this); subConf = nh.subscribe(\"zed/confidence/confidence_map\", 1, &EDTNODE::get_confi_map, this); Copy publish 1 topic:  mapTimer = nh.createTimer(ros::Duration(0.1), &EDTNODE::publishMap, this); pub_map = nh.advertise<edt::CostMap> (\"cost_map\", 1); Copy node param  nh.param<bool>(\"/edt/simulation\", simulation, false); nh.param<int>(\"/edt/crop\", crop, depth_rows); nh.param<float>(\"/edt/clear_range\", clear_range, 10); nh.param<double>(\"/nndp_cpp/fly_height\",FLYHEIGHT,1.0); Copy Code structure# while receive a depth measurement, update the probatility map (call function from stereo.cpp)  if (got_depth) { su->makeStereoPt(trans,depth_ptr,confidence_ptr); } Copy Update the cost Map (call function from MapUpdater.cpp)  su->updateEDTMap(FLYHEIGHT-0.4, FLYHEIGHT+0.4, center); Copy Publish the distance map  pub_map.publish (cost_map_msg); Copy "},{"title":"ROS main function （edt_node_dsstereo.cpp）","type":1,"pageTitle":"EDT Framework Overview","url":"docs/research/edt/edt-overview/#ros-main-function-（edt_node_dsstereocpp）","content":"Difference with edt_node_laser_realdrone.cpp# set params instead of hard coding, such as the physical size of a grid in meter, gridSize, how many grids in the map, map_x, map_y, map_zhow many grids will be updated in each step, update_x, update_y, update_zfly height of UAV, FLYHEIGHTheight range of the map, HEIGHT_EXPANSIONwhether there's a confidence map together with depth map, use_confidencethe params used in map update, d_min, d_max, d_thre "},{"title":"Map Updater","type":0,"sectionRef":"#","url":"docs/research/edt/MapUpdater/","content":"","keywords":""},{"title":"Overview & Class Names","type":1,"pageTitle":"Map Updater","url":"docs/research/edt/MapUpdater/#overview--class-names","content":"class name: MapUpdater, MapUpdater::MapUpdater(LinDistMap *dmap, DevMap *dev_map) Copy "},{"title":"MapUpdater.cpp","type":1,"pageTitle":"Map Updater","url":"docs/research/edt/MapUpdater/#mapupdatercpp","content":"get camera pose from tf, and update projection cudaMat::SE3<float> MapUpdater::getCamPos(const tf::Transform &trans) constvoid MapUpdater::updateProjection(const tf::Transform &trans) update EDT Mapvoid MapUpdater::updateEDTMap(const double &min_z_pos, const double &max_z_pos, const DevGeo::pos &center) Get the vehicle's global coordinates  DevGeo::coord c = _dev_map->pos2coord(center); Copy "},{"title":"FED_ROS Framework Overview","type":0,"sectionRef":"#","url":"docs/research/fed_ros/fed_ros-overview/","content":"","keywords":""},{"title":"Installation","type":1,"pageTitle":"FED_ROS Framework Overview","url":"docs/research/fed_ros/fed_ros-overview/#installation","content":""},{"title":"catkin make error:","type":1,"pageTitle":"FED_ROS Framework Overview","url":"docs/research/fed_ros/fed_ros-overview/#catkin-make-error","content":"Project 'cv_bridge' specifies '/usr/include/opencv' as an include dir, which is not found. solution:# sudo gedit /opt/ros/melodic/share/cv_bridge/cmake/cv_bridgeConfig.cmake sudo gedit /opt/ros/melodic/share/image_geometry/cmake/image_geometryConfig.cmake sudo gedit /opt/ros/melodic/share/image_proc/cmake/image_procConfig.cmake find and replace \"include/opencv\" to \"include/opencv4\" reference: https://www.cnblogs.com/long5683/p/12390807.html "},{"title":"stereo.cpp","type":0,"sectionRef":"#","url":"docs/research/edt/stereo/","content":"stereo.cpp implement a class StereoUpdater with param StereoUpdater::StereoUpdater(LinDistMap *dmap, DevMap *dev_map, StereoParams p): MapUpdater(dmap,dev_map), _sp(p) { allocMem(_sp.rows,_sp.cols); } Copy allocate, free, reallocate memory for depth and confidence on device, _D_depth, _D_confi_map implement a function makeStereoPt: (1) get the current camera pose (call updateProjection(trans) in MapUpdater.cpp); (2) copy depthmap and confidence into device (although confidence not used in linear distance map optimization currently); (3) call the GPU function to construct probablity map in kernel/stereo_kernel.cu // Get the current camera pose updateProjection(trans); // Copty the depthmap, etc into the device topic2Dmem(depthPoint,confidence_ptr); // Start the GPU kernel to construct the probability map stereo::stereoKernelWrapper(_sp,_mp,_D_depth,_D_confi_map,_dev_map); Copy kernel/stereo_kernel.cu in function void stereoKernelWrapper(const StereoParams &sp, const ProjParams &mp, float *d_depth, float* d_confi_map, DevMap *dev_map) call the GPU kernel function __global__ void stereoMapSingleUpdater(StereoParams sp, ProjParams mp,float *d_depth, DevMap dev_map, DevGeo::coord shift), blks=range of z, threads=range of y const int gridSize = dev_map->updateRange.z; const int blkSize = dev_map->updateRange.y; // calculate the shift DevGeo::coord shift = dev_map->pos2coord(mp.center); shift.x -= dev_map->updateRange.x/2; shift.y -= dev_map->updateRange.y/2; shift.z -= dev_map->updateRange.z/2; stereoMapSingleUpdater<<<gridSize,blkSize>>>(sp,mp,d_depth,*dev_map,shift); Copy in GPU kernel function stereoMapSingleUpdater, in each thread, loop all the range of x, convert grid coordinate to global position (call function coord2pos in kernel/helper_mapop.cuh), then transform to body frame, finally project into pixel through calling the function void global2project(const DevGeo::coord &grid_crd,const ProjParams &mp, const DevMap &dev_map, const StereoParams &sp, int2 &pix, float &depth), global frame using NWU. in coord2pos for (s.x = 0; s.x < dev_map.updateRange.x; ++s.x) { c = s + shift; global2project(c,mp,dev_map,sp,pix,depth); Copy do nothing for too large or too small depth and projected pixels out of image range. if (depth <= 0 || depth >4.5) continue; if (pix.x < 0 || pix.x >= sp.cols || pix.y < 0 || pix.y >= sp.rows || pix.y >= sp.crop) continue; Copy Here, compare two values, depth and img_depth. depth is the existing grid after projection, img_depth is the sensor measured depth in current frame. if grid is in front of the sensor measured depth, it should be not occupied (value = 0), then send to low pass filter to update. if grid is far behind the sensor measured depth, it cannot be seen by sensor, do nothing. if the grid is near the sensor measured depth, it should be occupied (value = 1), send to low pass filter to update. the low pass filter is a function lowpassOccupancy in kernel/helper_mapop.cuh. img_depth=d_depth[sp.cols*pix.y+pix.x]; if (isnan(img_depth) || img_depth <= 0.21) continue; if (depth < img_depth - 0.21) { lowpassOccupancy(c, 0, 0.5, dev_map); } else if (depth > img_depth + 0.21) { // not seen do nothing } else { lowpassOccupancy(c, 255, 0.5, dev_map); } Copy kernel/helper_mapop.cuh low pass filter __device__ __forceinline__ void lowpassOccupancy(const DevGeo::coord &c, float val, float beta,const DevMap &dev_map) global position to grid map coordinate __device__ __forceinline__ DevGeo::coord pos2coord(const DevGeo::pos &p,const DevGeo::pos &origin, const float &gridstep) grid map coordinate to global position __device__ __forceinline__ DevGeo::pos coord2pos(const DevGeo::coord & c,const DevGeo::pos &origin, const float &gridstep) dsstereo_kernel.cu difference with stereo_kernel.cuin the function global2project, the projection of 3D point to image pixel using the double sphere fisheye camera model; add params, d_min, d_max, d_thre, sweep_mode","keywords":""},{"title":"loop_closure Overview","type":0,"sectionRef":"#","url":"docs/research/msckf_vio/loop_closure/","content":"","keywords":""},{"title":"Installation","type":1,"pageTitle":"loop_closure Overview","url":"docs/research/msckf_vio/loop_closure/#installation","content":""},{"title":"Basalt Backend Walkthrough","type":0,"sectionRef":"#","url":"docs/research/vio/basalt-backend/","content":"","keywords":""},{"title":"Overview & Class Names","type":1,"pageTitle":"Basalt Backend Walkthrough","url":"docs/research/vio/basalt-backend/#overview--class-names","content":"The backend of the VIO framework is contained with the base class VioEstimatorBase (defined in vio_estimator.h) which is the parent class of two type of the estimator possible: KeypointVioEstimator(defined in keypoint_vio.h), andKeypointVoEstimator(defined in keypoint_vo.h) // how the factory class initialise to different type of estimator depends on use_imu variableVioEstimatorBase::Ptr VioEstimatorFactory::getVioEstimator( const VioConfig& config, const Calibration<double>& cam, const Eigen::Vector3d& g, bool use_imu) { VioEstimatorBase::Ptr res; if (use_imu) { res.reset(new KeypointVioEstimator(g, cam, config)); } else { res.reset(new KeypointVoEstimator(cam, config)); } return res;} Copy For the purpose of this walkthrough honly KeypointVioEstimator is conerned. We are interested in understanding how optical flow observations (keypoints) are used to form keyframes, and 3D landmarks. "},{"title":"I/O of Base Class of VioEstimator","type":1,"pageTitle":"Basalt Backend Walkthrough","url":"docs/research/vio/basalt-backend/#io-of-base-class-of-vioestimator","content":"The inputs are optical results (observations of keypoints) and imu inputs: A TBB queue, vision_data_queue of type OpticalFlowResult::Ptr, where each optical flow result contains the 64-bit timestamp,a vector named observations, containing a map between the keypoint ID KeypointId and its 2D transformations Eigen::AffineCompact2f (2D transformation literally represent the keypoint 2D location in the observing camera frame, aka. the uv coordinates)and a pointer to the original image data (std::vector<ImageData>), for the current frame A TBB queue, imu_data_queue of type ImuData::Ptr, storing the timestamp, and the accelerometer and gyroscope information The outputs are output states, ⚠️ marginalised data?, and vio visualisation data: A pointer to TBB queue out_state_queue of type PoseVelBiasState::Ptr containing all the states which are: t_ns timestamp of the state in nanoseconds; T_w_i pose of the state; vel_w_i linear velocity of the state; bias_gyro gyroscope bias; bias_accel accelerometer bias.A pointer to TBB queue out_marg_queue of type MargData::PtrA pointer to TBB queueout_vis_queue of type VioVisualizationData::Ptr "},{"title":"Initialisation","type":1,"pageTitle":"Basalt Backend Walkthrough","url":"docs/research/vio/basalt-backend/#initialisation","content":"With the incoming flow of visual optical flow observations and IMU data, the backend has a way to initialise itself with a proper initial pose (and velocity) and bias estimate. We can choose to initialise either just the biases and attitude (quaternion), or with the transformation and velocity as well. Key things to note: The current implementation in Basalt only does bias and quaternion initialisation (all others set): bias is set them to zero, and attitude is done by two-vector calculation from single data point of imu -> TODO [Improvement]The initial IMU-world transformation T_w_i_init has a very special way to initialise, which is yaw ignorant: First the accelerometer raw reading is used (one data point), it is assumed to be the gravity vectorThe gravity vector is rotated to be aligned with the positive Z axis, this rotation action is recorded as a quaternion. This quaternion is essentially performing basis changing from body frame to global frame.That is to say, the transformation is only defined by the plane that the gravity vector and the +ve z-axis make as well as the angle itself. This does not take account into the correct yaw, a.k.a the initial heading is not consistent, which is a function of how the IMU is mounted and its initial orientation.NOTE: Therefore, with a fixed mounting between the IMU and camera, and with a fixed up direction of the whole rig (e.g. the drone), we can pre-calculate the rotation matrix to make the initialised $T$ transformation matrix to face our desired coordinate convention (e.g. NWU). The pre-integration buffer imu_meas is also initialised here, taken account of the bias (which is always set to zeros in the implementation)The bundle adjustment states frame_states of type Eigen::aligned_map<int64_t, PoseVelBiasStateWithLin>marg_order??The processing thread is created within the initialize() function, which is basically a while loop, by the means of lamda function and std::thread. "},{"title":"The Processing Thread (within KeypointVioEstimator)","type":1,"pageTitle":"Basalt Backend Walkthrough","url":"docs/research/vio/basalt-backend/#the-processing-thread-within-keypointvioestimator","content":"Configs: vio_enforce_realtime is used to set when the backend cannot catch up with optical flow results. It will throw away all previous results, except the latest one in the queue. if (config.vio_enforce_realtime) { // drop current frame if another frame is already in the queue. while (vision_data_queue.try_pop(curr_frame)) {skipped_image++;} if(skipped_image) std::cerr<< \"[Warning] skipped opt flow size: \"<<skipped_image<<std::endl; } if (!curr_frame.get()) { break; } Copy vio_min_triangulation_dist is used to determine whether the two camera frames are suitable for triangulation in generating the landmarks, during a keyframe initialisation vio_new_kf_keypoints_thresh is the threshold for the ratio between tracked landmarks and total tracked keypoints (e.g. raito of 0.7) vio_min_frames_after_kf Calibs: cam_time_offset_ns offset of the camera in time, should be normally 0 // Correct camera time offset curr_frame->t_ns += calib.cam_time_offset_ns; Copy After initialisation, the actual processing will only start with two concecutive frames (when both prev_frame and curr_frame are defined): Pre-integration is performed, between the two frames, using the latest bias estimation. Blocking may happen, as it reads IMU buffer all the way until one pass the current frame's timestamp (while (data->t_ns <= curr_frame->t_ns)) Note: the integration will ensure the upper integral timestamp is at least the current frame timestamp (it will make fake IMU measurement by shifting the last IMU readings, when needed) At the same time, bias correction getCalibrated() are also done for both gyro and accelerometerThe bulk of the calculation is doen within the measure(curr_frame, meas) function, which is discussed next "},{"title":"The Measure() Routine","type":1,"pageTitle":"Basalt Backend Walkthrough","url":"docs/research/vio/basalt-backend/#the-measure-routine","content":"The measure() routine takes in the current frame optical flow observations, as well as the IMU pre-integration results. "},{"title":"IMU measurements Processing","type":1,"pageTitle":"Basalt Backend Walkthrough","url":"docs/research/vio/basalt-backend/#imu-measurements-processing","content":"frame_states is updated with a entry key last_state_t_ns (current frame timestamp), to be stored with the IMU predicted stateimu_meas is also updated by indexed by the previous frame's timestampThis whole step might be skipped if IMU measurement is not passed in "},{"title":"Optical Flow Results Processing","type":1,"pageTitle":"Basalt Backend Walkthrough","url":"docs/research/vio/basalt-backend/#optical-flow-results-processing","content":"Pre-processing: prev_opt_flow_res stores, with key in timestamp, the optical flow measurement struct pointer, up to the current time frame For each camera frame, iterate through all observations. If that observed keypoint is already a landmark, add the observation to the landmark database; if not put it to the unconnected observation set. // skeletonfor (size_t i = 0; i < opt_flow_meas->observations.size(); i++) for (const auto& kv_obs : opt_flow_meas->observations[i]) if (lmdb.landmarkExists(kpt_id)){ num_points_connected[tcid_host.frame_id]++; }else{ unconnected_obs0.emplace(kpt_id); } Copy Key-framing: "},{"title":"Basalt VIO Framework Overview","type":0,"sectionRef":"#","url":"docs/research/vio/basalt-overview/","content":"","keywords":""},{"title":"Installation","type":1,"pageTitle":"Basalt VIO Framework Overview","url":"docs/research/vio/basalt-overview/#installation","content":""},{"title":"Install Camera Dependency","type":1,"pageTitle":"Basalt VIO Framework Overview","url":"docs/research/vio/basalt-overview/#install-camera-dependency","content":"First, install the camera driver (from .deb will do):Tiscamera Driver Second, install the ROS wrapper:Tiscamera ROS Wrapper "},{"title":"Install mavros Dependency","type":1,"pageTitle":"Basalt VIO Framework Overview","url":"docs/research/vio/basalt-overview/#install-mavros-dependency","content":"Modified mavros for monotonic time sync "},{"title":"Basalt Main Code","type":1,"pageTitle":"Basalt VIO Framework Overview","url":"docs/research/vio/basalt-overview/#basalt-main-code","content":"Git Repository: https://github.com/chengguizi/basalt-mirrorBranch: Master # in src/ directory of the catkin workspace, dogit clone --recursive https://github.com/chengguizi/basalt-mirror# git submodule update --recursive --initcd basalt-mirror ./scripts/install_deps.sh catkin build basalt Copy "},{"title":"Running Basalt VIO","type":0,"sectionRef":"#","url":"docs/research/vio/basalt-tests/","content":"","keywords":""},{"title":"Modes to Run In","type":1,"pageTitle":"Running Basalt VIO","url":"docs/research/vio/basalt-tests/#modes-to-run-in","content":"Refer here for the full list of fusion modes:https://docs.px4.io/master/en/advanced_config/tuning_the_ecl_ekf.html#ekf2_extvis Modes to test and compare with: GPS + EV_VEL + ROTATE_EV (recommended)GPS + EV_POS + ROTATE_EV (not recommened?)EV_POS + EV_VEL + ROTATE_EVEV_POS + ROTATE_EV "},{"title":"ROS to Mavros to PX4 (Odometry / vehicle_visual_odometry)","type":1,"pageTitle":"Running Basalt VIO","url":"docs/research/vio/basalt-tests/#ros-to-mavros-to-px4-odometry--vehicle_visual_odometry","content":"The mavros received external vision (VIO) position estimate through either POSE or ODOMETRY messages, done through remapping. <!-- You should only remap either vision_pose or odometry, but not both --> <!-- /mavros/vision_pose/pose is posestamped, where /mavros/vision_pose/pose is posewithcovariancestamped --><!-- <remap from=\"/mavros/vision_pose/pose\" to=\"/basalt/pose_enu\" /> --> <remap from=\"/mavros/odometry/out\" to=\"/basalt/odom_ned\" /> Copy For more documentation please find at https://dev.px4.io/master/en/ros/external_position_estimation.html System Level Tests "},{"title":"26 Oct 2020 Major Algorithmic Fixes","type":1,"pageTitle":"Running Basalt VIO","url":"docs/research/vio/basalt-tests/#26-oct-2020-major-algorithmic-fixes","content":"Bug Fixes: Fix bugs in LM optimisation steps, for convergence check and rollback stepFix a exception error in keyframe marginalisation logics (indexing non-existing frame / pose entries) Improvements: Improve KF creation logics, add throttling to prevent keep creating KF under low feature environments "},{"title":"Major Improvement over Outdoor Stability","type":1,"pageTitle":"Running Basalt VIO","url":"docs/research/vio/basalt-tests/#major-improvement-over-outdoor-stability","content":"Sep 2020 Update: GPS interference by USB / Fan is hugeShort of J120 boards Potential issues of the current system: Initialisation may fail / jitter a lot, if the drone starts at a very open field with no close features ( < 10m ?)When GPS is used, VIO degraded to relative measurement. It may not correct GPS jumps effectivelyPossible to explore VIO as velocity measurement, built into ECL EKF onboard PX4 "},{"title":"Fixed Crashing Issues","type":1,"pageTitle":"Running Basalt VIO","url":"docs/research/vio/basalt-tests/#fixed-crashing-issues","content":"Frontend - Boundary Check Using Actual Camera Model# Instead of rectangular boundary check, use the actual fisheye lens boundary for optical flow tracking checks, as well as keypoint detection: Example if (!calib.intrinsics[0].inBound(transform.translation())) patch_valid = false; Copy Backend - Triangulation Checks (Numerical)# Ignore points behind the camera: if (p0_triangulated[2] < 0.0){ if (config.vio_debug) std::cout << \"point \" << p0_triangulated.transpose() <<\" is behind the camera, throw away\" << std::endl; continue;} Copy Backend - Keyframe Selection Criteria Optimisation (Initialisation)#  // hm: check if keyframe is needed( // hm: criteria 1: landmarks in the database is low (indexed by current key frames), and there are available unconnected ones // hm: criteria 2: only a small ratio of landmarks are observed, time to marginalise old key frames! if ( ( (lmdb.numLandmarks() < 12 && lmdb.numLandmarks() / opt_flow_meas->num_good_ids < 0.4 ) || double(connected0) / (lmdb.numLandmarks() + 1) < config.vio_new_kf_keypoints_thresh) && (frames_after_kf > config.vio_min_frames_after_kf)) take_kf = true; static bool initialise_baseline = false; if (!initialise_baseline){ // latest state translation to the first pose position try { double moved_dist = (frame_states.at(last_state_t_ns).getState().T_w_i.translation()).norm(); if ( moved_dist > config.vio_min_triangulation_dist * 1.1 && frames_after_kf > config.vio_min_frames_after_kf){ take_kf = true; // take a keyframe when the time is right after start; initialise_baseline = true; } }catch (const std::out_of_range& e) { std::cout << \"Out of Range error at initialise_baseline\" << std::endl; } } Copy Frontend - Good Features To Use# Define good observation points, as the points tracked at least in one consecutive pair of frames. And only those observations are used for back-end {int good_ids = 0;for (auto& obs : transforms->observations){ for (auto& kv : obs){ // unsigned if (kv.first < pre_last_keypoint_id) good_ids++; }} transforms->num_good_ids = good_ids;} Copy Backend - Marginalise Far Frames# Detect distance between any keyframes and the latest keyframes if (id_to_marg < 0) { int64_t last_kf = *kf_ids.crbegin(); // the last keyframe always has a state for (int64_t id : kf_ids) { auto dist = (frame_poses.at(id).getPose().translation() - frame_states.at(last_kf).getState().T_w_i.translation()).norm(); if (dist > 10){ id_to_marg = id; std::cout << \"keyframe too far (> 10 m) removing\" << std::endl; } } } Copy Detect Speed > 15m/s, and crash automatically#  if (last_t_ns > 0){ auto delta_t = t_ns - last_t_ns; auto delta_t_w_i = T_w_i.translation() - vio_t_w_i.back(); // hm: greater than 15 m/s if (delta_t_w_i.norm() / delta_t > 15 ) { std::cout << \"detect speed to fast > 15 m/s\" << std::endl; abort(); } }else{ first_t_ns = t_ns; } last_t_ns = t_ns; Copy "},{"title":"Video Demos","type":1,"pageTitle":"Running Basalt VIO","url":"docs/research/vio/basalt-tests/#video-demos","content":" "},{"title":"Major Improvement over Stability","type":1,"pageTitle":"Running Basalt VIO","url":"docs/research/vio/basalt-tests/#major-improvement-over-stability","content":"Work Done: Redesign the mounting plate (credit Yanfeng)Recalibrated the Pixhawk (credit Yanfeng)Recalibrating the prototype rig, by covering all corners of the image Calibration verified by Kalibr's validator, and Jalvin's triangulation code Increase the IMU rates from 100Hz to 200 Hz, to make the pre-integration more stable (20ms is too long an interval) Use HIGHRES_IMU aka /mavros/imu/data_raw topicIt has certain filtering delay, but should be negligible (few ms?) Discovered incorrect time sync matches between camera-imu. Fixed it (50ms delay on laptop) Do not confuse the mean delay between camera and imu, and the OS delay (a measure between kernel v4l2 image reception and ros image reception). The mean delay on laptop is normally smaller than 80ms.Observe the visualisation on basalt, when there is large rotation motion. If the camera is out of sync, the optimisation will shift the observing point by a lot. THIS INDICATES THE ERROR. Fixed various crashing bugs shrinking the viewing angle to < 165 degree, by adding alpha_offsetfixed std::map error which occured in visualising the keypoints' original observation locationsfixed a divide by zero bug in front-end note This framework rely highly upon calibration and correct hardware synchronisation. All sensor measurement timings should be precise at few-milliseconds level. "},{"title":"Some Video Demos","type":1,"pageTitle":"Running Basalt VIO","url":"docs/research/vio/basalt-tests/#some-video-demos","content":"Indoor#   Outdoor#  TODO: The config vio_filter_iteration effectiveness is still to be determinedPerformance of GN and LM optimisation method testTest vio_enforce_realtime optionPort over to TX2 to verify the performance again, try to hit >10Hz update rate. Full Config File tis_config.json { \"value0\": { \"config.optical_flow_type\": \"frame_to_frame\", \"config.optical_flow_detection_grid_size\": 60, \"config.optical_flow_max_recovered_dist2\": 5.0, \"config.optical_flow_pattern\": 51, \"config.optical_flow_max_iterations\": 5, \"config.optical_flow_epipolar_error\": 0.05, \"config.optical_flow_levels\": 4, \"config.optical_flow_skip_frames\": 1, \"config.feature_match_show\": false, \"config.vio_max_states\": 3, \"config.vio_max_kfs\": 7, \"config.vio_min_frames_after_kf\": 3, \"config.vio_new_kf_keypoints_thresh\": 0.6, \"config.vio_debug\": false, \"config.vio_obs_std_dev\": 0.5, \"config.vio_obs_huber_thresh\": 1.0, \"config.vio_min_triangulation_dist\": 0.15, \"config.vio_outlier_threshold\": 10, \"config.vio_filter_iteration\": 4, \"config.vio_max_iterations\": 7, \"config.vio_enforce_realtime\": false, \"config.vio_use_lm\": true, \"config.vio_lm_lambda_min\": 1e-32, \"config.vio_lm_lambda_max\": 1e2, \"config.vio_init_pose_weight\": 1e12, \"config.vio_init_ba_weight\": 1e1, \"config.vio_init_bg_weight\": 1e2, \"config.mapper_obs_std_dev\": 0.25, \"config.mapper_obs_huber_thresh\": 1.5, \"config.mapper_detection_num_points\": 800, \"config.mapper_num_frames_to_match\": 30, \"config.mapper_frames_to_match_threshold\": 0.04, \"config.mapper_min_matches\": 20, \"config.mapper_ransac_threshold\": 5e-5, \"config.mapper_min_track_length\": 5, \"config.mapper_max_hamming_distance\": 70, \"config.mapper_second_best_test_ratio\": 1.2, \"config.mapper_bow_num_bits\": 16, \"config.mapper_min_triangulation_dist\": 0.07, \"config.mapper_no_factor_weights\": false, \"config.mapper_use_factors\": true, \"config.mapper_use_lm\": true, \"config.mapper_lm_lambda_min\": 1e-32, \"config.mapper_lm_lambda_max\": 1e3 }} Copy "},{"title":"Basalt VIO Tuning & Configs","type":0,"sectionRef":"#","url":"docs/research/vio/basalt-tuning/","content":"","keywords":""},{"title":"Update Camera Calibration","type":1,"pageTitle":"Basalt VIO Tuning & Configs","url":"docs/research/vio/basalt-tuning/#update-camera-calibration","content":"calib_file should be a .json file, typicaly located in ./data folder. It is different from Kalibr format. Therefore, a convenient python script is provided in the source code. (To run, it has scipy and sophus as dependencies) Convert the Kalibr's yaml file to json file like this ./ds_kalibr_camimu.py camchain-imucam-2021-01-27-14-52-36.yaml tis_23 Copy IMU-Camera Time offset \"cam_time_offset_ns\": -1897133 in the json file indicate the time offset in nanosecond. Meaning: imu time = camera time + cam_time_offset_ns. A negative value means image is head of imu data. "},{"title":"Tuning Basalt Configrations","type":1,"pageTitle":"Basalt VIO Tuning & Configs","url":"docs/research/vio/basalt-tuning/#tuning-basalt-configrations","content":""},{"title":"Front End Optical Flow:","type":1,"pageTitle":"Basalt VIO Tuning & Configs","url":"docs/research/vio/basalt-tuning/#front-end-optical-flow","content":"optical_flow_epipolar_error could be smaller if the calibration is good, author's default is 0.01, can be as large as 0.05 if calibration is offoptical_flow_skip_frames will control how frequent the flow tracking results to be send to backend optimisation. For example, a number 2 means only have of the frames are sent to optimisation: If the camera are capturing at 20Hz, the pose output will be at 10Hz. Front End Debugging config.feature_match_show could be set to true to evaluate optical flow matching performance. Connected lines means successful left-right matching "},{"title":"Back End Optimisation:","type":1,"pageTitle":"Basalt VIO Tuning & Configs","url":"docs/research/vio/basalt-tuning/#back-end-optimisation","content":"vio_max_states number of latest imu-pose states (shown in red in gui)vio_max_kfs number of keyframes (shown as blue in gui)vio_min_frames_after_kf the minimum frames apart, where two keyframes are taken. This is to avoid taking keyframes too frequentlyvio_new_kf_keypoints_thresh the higher the threshold, the more frequent the keyframe is taken. Three more important parameters: vio_obs_std_dev the lower the standard deviation, the higher weight the back end will trust the optical flow results (hence less trust on imu prior). Typical value: 0.1 - 0.5vio_obs_huber_thresh to bound the optical flow loss, huber loss is used. The higher the value, the more weights is put on the optical results overall. Typical value 5.0 - 1.0vio_min_triangulation_dist this should be set just slightly smaller than the baseline of the cameras. This avoid triangulation of two frames that are physically too close in space. (Too close gives more error) "},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/systems/pixhawk_v1/getting-started/","content":"","keywords":""},{"title":"Setup","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#setup","content":""},{"title":"Drone system","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#drone-system","content":"Prepare the downloading script Running following command to download the script for installation, replace the YOURNAME with your account. $ wget --ask-password --user=YOURNAME -c https://bitbucket.org/nusuav/pixhawk_v1/raw/a6268c9207d70f689612ac2a5d5759749897c07f/pixhawk_setup.sh For example:wget --ask-password --user=Yu_ZHOU -c https://bitbucket.org/nusuav/pixhawk_v1/raw/a6268c9207d70f689612ac2a5d5759749897c07f/pixhawk_setup.sh Copy Installation Set up working directory and install the repository. Note that the script will auto create the directory catkin_ws under pixhawk_DIR folder. $ export pixhawk_DIR=\"Your directory\" (The default directory is `/media/nvidia/SD/`.)$ sh pixhawk_setup.sh Copy Compiling The step 2 will auto compile the package, if you want to do it mannully again, please run the following command. $ cd catkin_ws$ catkin build --cmake-args -DCMAKE_BUILD_TYPE=Release Copy "},{"title":"Web-UI","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#web-ui","content":"Please Refer to the Web-UI page. "},{"title":"Usage","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#usage","content":""},{"title":"Step 1: Common setting before testing","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#step-1-common-setting-before-testing","content":"Waypoints file Put waypoints in pixhawk_v1/param/waypoints.txt or change the file location and name in usr_console.launch (waypoints_file param) based on your requirement. Flight parameters(optional) /pixhawk_v1/us_ws/nndp_cpp/include/optimization/OptimizationUtilities.h Safety distance: rTooClose(default: 1.0)Vehicle size: rVehicle(default: 0.4) /pixhawk_v1/us_ws/edt/src/edt_node_laser_realdrone.cpp compress range for 2d mapping: (default: FLYHEIGHT-0.4, FLYHEIGHT+0.2) su->updateEDTMap(FLYHEIGHT-0.4, FLYHEIGHT+0.2, center) /pixhawk_v1/us_ws/nndp_cpp/src/nndp_cpp_node.cpp flight speed: ran.loc_h(default: 2), ran.loc_v(default: 1). Modify from 1(low speed) to 5(high speed). zed camera parameters (optional) Please put calibration param under /usr/local/zed/settings with SNxxxx.conf as it's nameChange camera param in edt.yaml file under edt package.Change frequency of zed from tools/ZED Exlplorer Remember to change frequency param in zed.launch fileRemember to change the noise param in vision_sensor_fix_pixhawk.yaml as well If you want to use zed only instead of VIO, change the use_zed_only param in pixhawk.launch to true and change the the param file under edt from edt.yaml to edt_mini.yaml zed auto explosure (optional) rosrun rqt_reconfigure rqt_reconfigure "},{"title":"Step 2: Running","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#step-2-running","content":"simulator testing $ sh v2_pixhawk.sh Copy Real flight testing $ sh v2_pixhawk_simulation.sh *if you want to test pixhawk only without other high level modules, then just run: \"roslaunch mavros px4.launch system_id:=6(for example)\"* Copy "},{"title":"Step 3: System checking","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#step-3-system-checking","content":"check vio status, whether the location and heading is correctcheck mapping status, whether or not good to fly "},{"title":"Step 4: Take off","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#step-4-take-off","content":"Method 1: web UI# Check the ip adress of the droneSearch the address in your web browser You will be able to see the UI if everything goes wellSend command by clicking each buttons Engine0TakeoffMission Method 2: previous user console# $ roslaunch usr_console usr_console.launch Copy Clear previous reference on pixhawk wait till all the plugins have been loaded and them input engine0 command in the terminal. take off Input Engage command in the terminal. Send waypoints command. Global waypoints: mission* This command will load the waypoints.txt where all the points are in global NWU frame (True North). Local waypoints: waypoints* This command will load the waypoints.txt file but all the waypoints is in local NWU frame. "},{"title":"step 5: landing","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#step-5-landing","content":""},{"title":"Useful Command","type":1,"pageTitle":"Getting Started","url":"docs/systems/pixhawk_v1/getting-started/#useful-command","content":"Screen Re-attach screen: screen -r nameOfSessionDetach screen: ctrl+A+DKill all sessions Check GPU usage $ sudo ~/tegrastats --interval <int> --logfile <out_file> &For example:$ sudo ~/tegrastats --interval 5000 --logfile ~/pixhawk #This command probes every 5 seconds Copy The out put is similar to this: RAM 2135/7855MB (lfb 1057x4MB) CPU [2%@2035,0%@2035,0%@2035,2%@2035,2%@2036,2%@2035] EMC_FREQ 1%@1866 GR3D_FREQ 0%@1300 APE 150 MTS fg 0% bg 0% BCPU@30C MCPU@30C GPU@29C PLL@30C Tboard@28C Tdiode@28.25C PMIC@100C thermal@29.4C VDD_IN 3104/4029 VDD_CPU 291/730 VDD_GPU 145/207 VDD_SOC 727/746 VDD_WIFI 0/180 VDD_DDR 1248/1375 Copy Please check tegrastats utility reports to get more info about the stats. The data we normally care about are: RAM X/Y: X is the amount of RAM in use in MB.CPU [X%@Z, Y%@Z,...]: Load statistics for each of the CPU cores relative to the current running frequency Z.GR3D_FREQ X%@Y: Percent of the GR3D that is being used, relative to the current running frequency. GR3D is the GPU engine. Hotspot setup Enable broadcast of SSID (self wifi mode) $ echo 2 > /sys/module/bcmdhd/parameters/op_mode Copy Change back to normal wifi mode $ echo 0 > /sys/module/bcmdhd/parameters/op_mode Copy To make it persistent Add line options bcmdhd op_mode=2 in nano /etc/modprobe.d/bcmdhd.conf Change ip address Add line address1=192.168.1.150/24 Fan stop fan $ sudo -s $ cd /sys/kernel/debug/tegra_fan/$ echo 0 > target_pwm Copy Check temperature $ watch -n 1 cat /sys/class/thermal/thermal_zone?/temp Copy Submodule Add a new submodule with a branch name $ git submodule add -b branch_name https://bitbucket.org/nusuav/repo_name.git Copy Remove a submodule $ git submodule deinit -f submodule_name (The name of the submodule is the same as the repo's name) $ rm -rf .git/modules/submodule_name$ git rm -f submodule_name Copy "},{"title":"Motion Planning","type":0,"sectionRef":"#","url":"docs/systems/pixhawk_v1/motion-planning/","content":"Motion Planning","keywords":""},{"title":"Basalt Front End","type":0,"sectionRef":"#","url":"docs/research/vio/basalt-frontend/","content":"","keywords":""},{"title":"Initialization","type":1,"pageTitle":"Basalt Front End","url":"docs/research/vio/basalt-frontend/#initialization","content":"The front end is initialized in basalt::OpticalFlowFactory::getOpticalFlow(vio_config, calib) Copy func param: vio_config: parameters in config.json file; calib: camera info. return: OpticalFlowBase::Ptr, which is reset (initialized) by config.optical_flow_type and config.optical_flow_pattern parameter shown in the table. class OpticalFlowBase { public: using Ptr = std::shared_ptr<OpticalFlowBase>; tbb::concurrent_bounded_queue<OpticalFlowInput::Ptr> input_queue; tbb::concurrent_bounded_queue<OpticalFlowResult::Ptr>* output_queue = nullptr; Eigen::MatrixXf patch_coord;}; Copy The input_queue is set to be the stereo image buffer queue with tbb::concurrent_bounded_queue to handle multithread sharing. The output_queue passes to vio vision_data_queue. "},{"title":"Parameters","type":1,"pageTitle":"Basalt Front End","url":"docs/research/vio/basalt-frontend/#parameters","content":"param\ttype\tvalue used\tfunctionconfig.optical_flow_type\t\"frame_to_frame\" / \"patch\"\t\"frame_to_frame\"\tdecide optical flow method: \"patch\" to use PatchOpticalFlow and \"frame_to_frame\" to use FrameToFrameOpticalFlow config.optical_flow_pattern\t24/52/51/50\t50\toptical flow pattern shown in the below image  Optical flow patterns:The pattern is used for each key point to buid a patch when comparing matchespattern24: 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 pattern52: 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 Weights of pattern52: {-3, 7}, {-1, 7}, {1, 7}, {3, 7}, {-5, 5}, {-3, 5}, {-1, 5}, {1, 5}, {3, 5}, {5, 5}, {-7, 3}, {-5, 3}, {-3, 3}, {-1, 3}, {1, 3}, {3, 3}, {5, 3}, {7, 3}, {-7, 1}, {-5, 1}, {-3, 1}, {-1, 1}, {1, 1}, {3, 1}, {5, 1}, {7, 1}, {-7, -1}, {-5, -1}, {-3, -1}, {-1, -1}, {1, -1}, {3, -1}, {5, -1}, {7, -1}, {-7, -3}, {-5, -3}, {-3, -3}, {-1, -3}, {1, -3}, {3, -3}, {5, -3}, {7, -3}, {-5, -5}, {-3, -5}, {-1, -5}, {1, -5}, {3, -5}, {5, -5}, {-3, -7}, {-1, -7}, {1, -7}, {3, -7} pattern51 is the same as Pattern52 but twice smaller in terms of weight.pattern50 is the same as Pattern52 but 0.75 smaller. Copy "},{"title":"data structure","type":1,"pageTitle":"Basalt Front End","url":"docs/research/vio/basalt-frontend/#data-structure","content":"struct OpticalFlowInput { using Ptr = std::shared_ptr<OpticalFlowInput>; int64_t t_ns; std::vector<ImageData> img_data;}; Copy struct OpticalFlowResult { using Ptr = std::shared_ptr<OpticalFlowResult>; int64_t t_ns; //observation size depends on the number of cameras, 2 in our case //KeypointId: //AffineCompact2f: std::vector<Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>> observations; OpticalFlowInput::Ptr input_images; // one image frame from the input_queue of images}; Copy /// ids for 2D features detected in imagesusing FeatureId = int;/// identifies a frame of multiple images (stereo pair)using FrameId = int64_t;/// identifies the camera (left or right)using CamId = std::size_t; struct KeypointsData { /// collection of 2d corner points: pixel location (indexed by FeatureId) std::vector<Eigen::Vector2d, Eigen::aligned_allocator<Eigen::Vector2d>> corners; /// collection of feature orientation (in radian) with same index as `corners`(indexed by FeatureId) std::vector<double> corner_angles; /// collection of feature descriptors with same index as `corners` (indexed by FeatureId) std::vector<std::bitset<256>> corner_descriptors; Eigen::aligned_vector<Eigen::Vector4d> corners_3d; std::vector<FeatureHash> hashes; HashBowVector bow_vector;}; Copy Patch Copy "},{"title":"FrameToFrameOpticalFlow","type":1,"pageTitle":"Basalt Front End","url":"docs/research/vio/basalt-frontend/#frametoframeopticalflow","content":""},{"title":"Constructor","type":1,"pageTitle":"Basalt Front End","url":"docs/research/vio/basalt-frontend/#constructor","content":"calculate R and T between if there is more than one cameras.initialize the processingLoop() thread and use a smart pointer to handle resource. "},{"title":"processingLoop","type":1,"pageTitle":"Basalt Front End","url":"docs/research/vio/basalt-frontend/#processingloop","content":"Keep poping OpticalFlowInput from the input_queue and process it with processFrame(input_ptr->t_ns, input_ptr) function until we get a nullptr from the input_queue. "},{"title":"processFrame","type":1,"pageTitle":"Basalt Front End","url":"docs/research/vio/basalt-frontend/#processframe","content":"Input(int64_t curr_t_ns, OpticalFlowInput::Ptr& new_img_vec) first frame initialization# initialize transforms (OpticalFlowResult::Ptr) transforms.reset(new OpticalFlowResult);transforms->input_images = new_img_vec; Copy initialize pyramid(std::shared_ptr<std::vector<basalt::ManagedImagePyr<u_int16_t>>>) pyramid->resize(calib.intrinsics.size()); Copy running pyramid for all images in parallel tbb::parallel_for(tbb::blocked_range<size_t>(0, calib.intrinsics.size()), [&](const tbb::blocked_range<size_t>& r) { for (size_t i = r.begin(); i != r.end(); ++i) { pyramid->at(i).setFromImage( *new_img_vec->img_data[i].img, config.optical_flow_levels); } }); //[tbb.parallel_for(blocked_range<T>(begin,end,grainsize), func(const tbb::blocked_range<size_t>& r))](https://software.intel.com/en-us/node/506057)//[&] means \"capture by reference\", [=] means \"capture by value\"//setFromImage(const ManagedImage<T>& other, size_t num_levels) sets image pyramid from other image// @param other image to use for the pyramid level 0 Copy The ManagedImagePyr class is used to calculate pyramid of the input image. Noted that the pyramid is stored as a (1+1/2)w x h image shown as the zebra image. A gaussian kernel is used to subsampling the image and every even-numbered row and colum have to be removed. The kernel is shown as this template. 1/256 * cov[image x kernel] + 0.51 & 4 & 6 & 4 & 14 & 16 & 24 & 16 & 46 & 24 & 36 & 24 & 64 & 16 & 24 & 16 & 41 & 4 & 6 & 4 & 1 Copy // only left important function heretemplate <typename T, class Allocator = DefaultImageAllocator<T>>class ManagedImagePyr { public: using Ptr = std::shared_ptr<ManagedImagePyr<T, Allocator>>; /// @brief Set image pyramid from other image. inline void setFromImage(const ManagedImage<T>& other, size_t num_levels) { // call \"Image<const T> lvl(size_t lvl)\" to build subimage on the image, i.e. given x, y index as well as w and h for each pyramid // call \"void subsample(const Image<const T>& img, Image<T>& img_sub)\" to return const image of the certain level, one by convolution with Gaussian kernel(ubsample the image twice in vertical and horizontal direction) and removing every even-numbered row and column } protected: size_t orig_w; ///< Width of the original image (level 0) ManagedImage<T> image; ///< Pyramid // (1+1/2)w x h }; Copy The paramid 0,1...n are stored from left to right and upper to bottom as shown in the image.  addPoints add previously points from the Left image to pts0(vector<Eigen::Vector2d>) variabledetect points at level 0  void detectKeypoints( const basalt::Image<const uint16_t>& img_raw, KeypointsData& kd, int PATCH_SIZE, int num_points_cell, const Eigen::aligned_vector<Eigen::Vector2d>& current_points){ devide image into cells based on PATCH_SIZE put existing points into each cell for each cell: if the current cell already has previously tracked feature, skip this cell else detect fast feature in the grid (default: one feature per grid): cv::FAST(subImg, points, threshold); threshold is from 40->20->10->5 and make sure the point is with in EDGE_THRESHOLD = 19 of the whole image } Copy put corners with keypoint_id in transform(local variable) and initialize new_poses0(same type as transform: Eigen::AffineCompact2f). note: transfrom is just the keypoint location on imagetrack points from left to right  void trackPoints(const basalt::ManagedImagePyr<u_int16_t>& pyr_1, const basalt::ManagedImagePyr<u_int16_t>& pyr_2, const Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>& transform_map_1, Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>& transform_map_2, bool leftToRight = false){/* 1. trackPointAtLevel: Check one pyr level: if the residul = patch - data, is valid valid means that 1.1 at least PATTERN_SIZE / 2 number of points that both valid in template and residuals patch 1.2 the points transformed by the updated and optimized T can still be seen in the image 2. trackPoint: check if the traking is valid at all pyr levels 3. This Func: square norm diff from \"left to right transform\" and \"right to left transform\" should be less than config.optical_flow_max_recovered_dist2 */} Copy filterPoints supporting functions# trackPoints Track pointss between two images pyramid. Images could be from left to right/ right to left and previous to current. Noted that parameter without const here means output of the function. Also pay attention to the name, transform_map and transforms are map and vector, while transform is a single keypoint from the containers. It also applies to functions such as trackpoints and trackpoint.  void trackPoints(const basalt::ManagedImagePyr<u_int16_t>& pyr_1, const basalt::ManagedImagePyr<u_int16_t>& pyr_2, const Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>& transform_map_1, Eigen::aligned_map<KeypointId, Eigen::AffineCompact2f>& transform_map_2) const { for each point in transform_map_1: if trackPoint(pyr_1, pyr_2, transform_1, transform_2) is valid (means more than half of the points in template and tracking patch at each leavel are within image boundries during optimization iterations) track points from pyr_2 to pyr_1 (valid = trackPoint(pyr_2, pyr_1, transform_2, transform_1_recovered)) if the norm between the transform_1 and transform_1_recovered are within in config.optical_flow_max_recovered_dist2, then add this point into the transform_map_2 } Copy trackpoint Track point from coarse to fine at all levels. The point is tracked from old_pyramid to current pyramid. inline bool trackPoint(const basalt::ManagedImagePyr<uint16_t>& old_pyr, const basalt::ManagedImagePyr<uint16_t>& pyr, const Eigen::AffineCompact2f& old_transform, Eigen::AffineCompact2f& transform) { for each level of the pyramid(start from the highest one): scale the keypoint with respect to the transform (keypoint coordinate at the lowest layer of the pyramid) build a template patch around that scaled keypoint (see the patch section to see more details) perform tracking on current level (trackPointAtLevel(pyr.lvl(level), p, transform)) and check if the result is valid (check the definition in trackPointAtLevel func). update transform return patch_valid (True when all levels are valid) } Copy trackPointAtLevel Track one point at one pyramid level. Return patch_valid, which means  inline bool trackPointAtLevel(const Image<const u_int16_t>& img_2, const PatchT& dp, Eigen::AffineCompact2f& transform) const { for patch_valid && iteration < config.optical_flow_max_iterations: transform designed pattern with transform matrix(R * (x;y) + t, its linear/rotation part is initialized as an identity matrix, and translation part is initialized as the keypoint's coordinate) calculate residual between the template(dp) and the transformed patch(transformed_pat) check out patch section for more details in terms of residual calculation if the residual is valid: calculate increment of the rotation: inc= inv(JT*J)*JT*res update transfrom with inc if transform.translation() is within the img_2, then the patch is still valid, otherwise break the loop with patch_valid = false else: patch_valid = false; break the loop } Copy OpticalFlowPatch structure This structure is used to buid a patch around a point in image, and provides function to calculate residual used for transform itertation. Noted that below are pseudo code only. Using the inverse compositional algorithm discribed in section 3.2.1 in Lucas-Kanade 20 Years On: A Unifying Framework The original problem is to minimize the sum of squard error between two images, the template $$T$$ and the image $$I$$ warped back on to the coordinate frame of the template: $$\\sum_x[I(W(x;p))-T(x)]^2$$ We could assume that a current estimate of p is known and then iteratively solves for increments to the parameters $$\\Delta p$$, i.e. the following expression is minimized: Forwards additive(Lucas-Kanade) algorithm: $$\\sum_x[I(W(x;p+\\Delta p))-T(x)]^2$$ or Frowards compositional algorithm: $$\\sum_x[I(W(W(x;\\Delta p);p))-T(x)]^2$$ However, in order to avoid a huge computational cost in re-evaluating the Hessian in every iteration, we switch the role of the image and the template to form the inverse problem, where the goal is to minimize: $$\\sum_x[T(W(x;\\Delta p)-I(W(x;p)))]^2$$ with respect to $$\\Delta p$$ and then update the warp: $$W(x;p) \\leftarrow W(x;p) \\circ W(x;\\Delta p)^{-1}$$ Performing a first order Taylor expansion gives: $$\\sum_x[T(W(x;0))+ \\triangledown T \\frac{\\partial W}{\\partial p} - I(W(x;p))]^2$$ The solution to this least-squares problem is: $$\\Delta p = H^{-1} \\sum_x[\\triangledown T\\frac{\\partial W}{\\partial p} ]^T[I(W(x;p))-T(x)]$$ where H is: $$H = \\sum_x [\\triangledown T\\frac{\\partial W}{\\partial p} ]^T [\\triangledown T\\frac{\\partial W}{\\partial p} ]$$ Noted that the Jacobian $$\\frac{\\partial W}{\\partial p}$$ is evaluated at (x;0) and since there is nothing in the Hssian that depends on p, it is constant across iterations and can be pre-computed. The detail diagram of the algorithm is as follow.  In Baslt, W is group SE(2),which is rigid transformations in 2D space. It has 3 degree of freedom: $$p = [t_x, t_y, \\theta]$$. The derivative of SE(2) is a 2x3 matrix $$Jw _ se2 = \\triangledown T\\frac{\\partial W}{\\partial p}$$ , where $$\\triangledown T$$ means template gradient and $$\\frac{\\partial W}{\\partial p}$$ means the change of x or y with respect to SE2 parameters $$[t_x, t_y, \\theta]$$. For details can check out Equ. 117 in SE2. SE2 can be represented by: $$SE2=\\begin{bmatrix} 0 & -\\theta & t_x\\ \\theta & 0 & t_y\\ 0 & 0 & 0 \\end{bmatrix}$$ $$\\bm{x} = [x, y, 1]^T$$ And $$W(x;p) = SE(2) \\cdot \\bm{x} = [- \\theta y + t_x; \\theta x + t_y; 0]$$. Thus we have: $$\\frac{\\partial W}{\\partial p}=\\begin{bmatrix} 1 & 0 & -y\\ 0 & 1 & x \\end{bmatrix}$$  struct OpticalFlowPatch { // build a patch around the center point pos in the given image OpticalFlowPatch(const Image<const uint16_t> &img, const Vector2 &pos) { setFromImage(img, pos); } void setFromImage(const Image<const uint16_t> &img, const Vector2 &pos) { MatrixP2 grad; // matrix of patter_size * 2, stores the gradient, with respect to x (right) and y (down), for data! for i in range(pattern) Vector2 p = pos + pattern2.col(i); if (img.InBounds(p, 2)) { // see below images of the interGrad to know what it means // valGrad[0] is the interplated pixel value, valGrad[1] is grad in x direction, valGrad[2] is grad in y dirextion Vector3 valGrad = img.interpGrad<Scalar>(p); obtain intensity value for each point in the patch: data[i] = valGrad[0]; update sum for with each intensity value: sum += valGrad[0]; calculate each row of grad and update grad sum from the last two data of valGrad num_valid_points++; } else { data[i] = -1; } mean = sum / num_valid_points; Scalar mean_inv = num_valid_points / sum; // Yu: SE2: http://www.ethaneade.org/gradlie.pdf, see Equ. 119 // http://www.ncorr.com/download/publications/bakerunify.pdf Eigen::Matrix<Scalar, 2, 3> Jw_se2; // hm: each column, x, y, theta. x points right, y points down Jw_se2.template topLeftCorner<2, 2>().setIdentity(); MatrixP3 J_se2; // hm: pattern_size * 3, account for 2 translation and 1 rotation. reflects how each residual component changes, with regards to SE(2) for (int i = 0; i < PATTERN_SIZE; i++) { if (data[i] >= 0) { const Scalar data_i = data[i]; const Vector2 grad_i = grad.row(i); // hm: grad_i is the current pixel's gradient // hm: effectively = grad_i / averaged_intensity - averaged_grad * data_i / averaged_intensity ///Yu: grad.row(i) = num_valid_points * (grad_i * sum - grad_sum * data_i) / (sum * sum); data[i] *= mean_inv; // hm: divide by mean for all pixels } else { grad.row(i).setZero(); } // Fill jacobians with respect to SE2 warp // hm: the image is store in a row-major fashion, starting from the top-left corner. x correspon /// Yu: SE2 param [x,y,theta] Jw_se2(0, 2) = -pattern2(1, i); // hm: change of x, respect to rotation. Apply small angle differentiation, give dx/dtheta = -y Jw_se2(1, 2) = pattern2(0, i); // hm: change of y, respect to rotation. Similarly dy/dtheta = x // hm: J_se2 = di/dw * dw/dse2 = grad * Jw_se2 J_se2.row(i) = grad.row(i) * Jw_se2; // hm: jecobian of each pixel = } // hm: formulation refer to Usenko(2019), Preliminaries, with W = identity Matrix3 H_se2 = J_se2.transpose() * J_se2; Matrix3 H_se2_inv; H_se2_inv.setIdentity(); H_se2.ldlt().solveInPlace(H_se2_inv); // hm: this is to solve the inverse. This is because H_se2_inv is set to identity. so the result x IS THE INVERSE H_se2_inv_J_se2_T = H_se2_inv * J_se2.transpose(); } inline bool residual(const Image<const uint16_t> &img, const Matrix2P &transformed_pattern, VectorP &residual) const { Scalar sum = 0; // hm: stores the sum of image pixels, using valid points in the transformed pattern Vector2 grad_sum(0, 0); int num_valid_points = 0; for (int i = 0; i < PATTERN_SIZE; i++) { if (img.InBounds(transformed_pattern.col(i), 2)) { residual[i] = img.interp<Scalar>(transformed_pattern.col(i)); // Yu: bilinear interplatation sum += residual[i]; num_valid_points++; } else { residual[i] = -1; } } int num_residuals = 0; for (int i = 0; i < PATTERN_SIZE; i++) { if (residual[i] >= 0 && data[i] >= 0) { const Scalar val = residual[i]; // hm: sum / num_valid_points gives the averaged intensity of the patch residual[i] = num_valid_points * val / sum - data[i]; num_residuals++; } else { residual[i] = 0; } } // hm: num_residuals indicates the number of points that both valid in data and residuals return num_residuals > PATTERN_SIZE / 2; } Vector2 pos; //patch center VectorP data; // template patch, negative if the point is not valid Matrix3P H_se2_inv_J_se2_T; //inv(JT*J)*JT, typical calculation for gaussian newton optimization Scalar mean; // mean of data, before deviding by mean}; Copy  frame processing# Draft section  \"config.optical_flow_detection_grid_size\": 50, // Patch size \"config.optical_flow_max_recovered_dist2\": 1.0, //max distance set for optical flow \"config.optical_flow_pattern\": 51, \"config.optical_flow_max_iterations\": 5, \"config.optical_flow_epipolar_error\": 0.5, // remove flow that valates the epipolar constrain \"config.optical_flow_levels\": 5, \"config.optical_flow_skip_frames\": 1, // 1 means do not skip \"config.feature_match_show\": true, // show detection and tracking result between left and right \"config.vio_max_states\": 3, \"config.vio_max_kfs\": 5, \"config.vio_min_frames_after_kf\": 5, \"config.vio_new_kf_keypoints_thresh\": 0.7, \"config.vio_debug\": true, \"config.vio_obs_std_dev\": 0.5, // dev for 3d vision reprojection error \"config.vio_obs_huber_thresh\": 1.0, // Another threshod for vision weights \"config.vio_min_triangulation_dist\": 0.05, // the translation has to be bigger than square of this value to do trangulation \"config.vio_outlier_threshold\": 1.0, // outlier if the norm of the residual is larger than this value \"config.vio_filter_iteration\": 4, // filter outliers in this iter during optimization \"config.vio_max_iterations\": 7, // vio optimization iteration number \"config.vio_enforce_realtime\": false, \"config.vio_use_lm\": false, // use gaussian newton otherwise \"config.vio_lm_lambda_min\": 1e-32, \"config.vio_lm_lambda_max\": 1e2, \"config.vio_init_pose_weight\": 1e8, // margH postion and yaw weight Copy pitch; ///Yu: Bytes per unit data. e.g: one row contain pitch * w bytes. Update repo git submodule sync // fix mismatch remote repo issuegit submodule update Copy "},{"title":"Welcome to pixhawk Page","type":0,"sectionRef":"#","url":"docs/systems/pixhawk_v1/pixhawk/","content":"","keywords":""},{"title":"Hardware Design","type":1,"pageTitle":"Welcome to pixhawk Page","url":"docs/systems/pixhawk_v1/pixhawk/#hardware-design","content":"The final platform was a 2.6kg octo-rotor configuration utilizing a 2213 size brushless dc motor rated at 920kV which was paired with a 9.5” in diameter by 4.5” pitch propeller with self-locking propeller hubs together with electronic speed controllers rated for 20A continuous current usage at up to 500Hz. Each propulsor was capable of providing up to 1kg of thrust when used with a 4S lithium polymer battery. "},{"title":"Software Architecture","type":1,"pageTitle":"Welcome to pixhawk Page","url":"docs/systems/pixhawk_v1/pixhawk/#software-architecture","content":"ROS was chosen as our communication framework because it provides a convenient way to realize the interaction between different modules. The modules can be categorized into the following main parts: state estimation, mapping and motion planning, target detection, and tracking, etc.  "},{"title":"Simulator","type":0,"sectionRef":"#","url":"docs/systems/pixhawk_v1/simulator/","content":"Simulator","keywords":""},{"title":"Tracking and Detection","type":0,"sectionRef":"#","url":"docs/systems/pixhawk_v1/tracking-and-detection/","content":"Tracking and Detection","keywords":""},{"title":"Visual-Inertial Odometry","type":0,"sectionRef":"#","url":"docs/systems/pixhawk_v1/VIO/","content":"","keywords":""},{"title":"System Overview","type":1,"pageTitle":"Visual-Inertial Odometry","url":"docs/systems/pixhawk_v1/VIO/#system-overview","content":"The ekf_sensor_fusion package performs loose-coupled sensor fusion from stereo camera's visual odometry and IMU's inertial measurement to produce UAV pose estimation. The filter is formulated to take in body-frame velocity measurement from the visual odometry module (ZED camera), instead of the pose measurement with respect to the initial body-frame. This architecture allows temporal VO measurement errors to be rejected effectively, when an inconsistency between IMU integration and VO measurement is detected. \tpixhawk Coordinate System\tState Estimation Diagram The output of this module is the pose estimation, and only the X-Y coordinates of the pose information is fed to Pixhawk's internal state filter. "},{"title":"Inputs and Outputs","type":1,"pageTitle":"Visual-Inertial Odometry","url":"docs/systems/pixhawk_v1/VIO/#inputs-and-outputs","content":"Inputs: Description\tROS Topic Type\tROS Topics (remapped in .launch files)\tRecommended FrequencyIMU Input (accel + gyro)\tsensor_msgs/Imu\tekf_fusion/imu_state_input\t>= 100Hz Magnetometer Input\tsensor_msgs/MagneticField\tekf_fusion/mag_state_input\tsame as IMU Velocity Odometry\tgeometry_msgs/PoseStamped\tekf_fusion/visionpose_measurement\t>= 15Hz Delta-Pose Odometry\tgeometry_msgs/PoseStamped\tekf_fusion/zedpose_measurement\tnot tested Time Synchronisation of Inputs Input ROS messages are time-synchronised internally, to the messages' header stamp. Therefore, it is important to ensure the timestamp of the inputs are sufficiently accurate. Best Practices IMU sensor has drift over time, even between each power cycle. Therefore, pixhawk's Pixhawk firmware is programmed to perform gyro calibration at power on. DO NOT move the UAV for the first 20 second or so, until the second boot up success beeping sound is heard (indicating completion of gyro calibration).Magnetometer has significant drift indoor and it almost always requires re-calibration in new environments. Therefore, magnetometer input is only used during the filter initialisation, and essentially ignored when the UAV is flying. (by giving large variance in meas_noise2 parameter) Delta-Pose Odometry should not be used in general, as it is not tested. (keep velocity_measurement as true) Outputs: Description\tROS Topic Type\tROS TopicsIMU-rate pose output in world ekf_frame (default NWU)\tgeometry_msgs/PoseWithCovarianceStamped\t/ekf_fusion/pose IMU-rate pose output, same as above but yaw is with respect to initial heading, instead of magnetic north\tgeometry_msgs/PoseWithCovarianceStamped\t/ekf_fusion/pose_local The same as /ekf_fusion/pose but at VO-rate\tgeometry_msgs/PoseWithCovarianceStamped\t/ekf_fusion/pose_corrected FOR DEBUG ONLY (Integrated pose from IMU inputs only)\tgeometry_msgs/PoseWithCovarianceStamped\t/ekf_fusino/pose_integrated Generally, Pixhawk prefers to take in pose estimation with respected to magnetic north, at highest frequency possible, therefore /ekf_fusion/pose is currently used. "},{"title":"Initialisation","type":1,"pageTitle":"Visual-Inertial Odometry","url":"docs/systems/pixhawk_v1/VIO/#initialisation","content":"Prior to the start of the system, the UAV should be in a stationary state, "},{"title":"Configurable Parameters (.yaml)","type":1,"pageTitle":"Visual-Inertial Odometry","url":"docs/systems/pixhawk_v1/VIO/#configurable-parameters-yaml","content":"Parameters\tDescription\tRecommended Valuesvelocity_measurement\tAlways set true, as the filter is designed and tested to receive velocity measurement\ttrue pose_of_camera_not_imu\tSet if the pose estimation is for camera-body or imu-body\tfalse use_imu_internal_q\tIf IMU has internal estimation of attitude, this could be set to true. Otherwise, the initial pose would be estimated from accelerometer (gravity-align) and magnetometer (north-align)\ttrue do_init_gyro_bias_estimate\tWhen set true, the filter assumes the UAV is absolutely stationary, and all the finite gyro readings are fed as gyro bias in the states\ttrue imu_frame\tThe coordinate frame that IMU input is using\tNWU ekf_frame\tThe coordinate frame that the filter pose output is using\tNWU sigma_distance_scale\tThis parameter set how tolerant the filter is towards deviation of VO measured velocity / rotation agains IMU's integrated value. Smaller the value, the more sensitive the rejection\t3 scale_init\tStereo's visual frame scale respect to world frame. This value could be obtained from real-world test of the stereo camera\t1.0 fixed_scale\tFix visual frame scale\ttrue fixed_bias\tSet whether IMU bias online estimation is disabled\tfalse fixed_calib\tSet whether Camera-IMU calibration estimation is disabled\tfalse noise_acc\tNoise of accelerometer measurement\t0.01 noise_accbias\tDrift rate of accelerometer measurement\t0.0004 noise_gyr\tNoise of gyroscope measurement (much smaller than accel)\t0.008 noise_gyrbias\tDrift rate of gyroscope\t0.00002 noise_scale\tNot used\t0.0 noise_qwv\tNot used\t0.0 noise_qci\tNot used\t0.0 noise_pic\tNot used\t0.0 delay\tHow much time in second VO is lagging IMU timestamps\t0.0 meas_noise1\tNoise of velocity measurement. The noise should be smaller when frequency increases\t0.03 for 15Hz; 0.008 for 30Hz meas_noise2\tNoise of IMU quaternion measurement\t9999 init/q_ci\tattitude of camera with respect to IMU\t- init/p_ci\tposition of camera with respect to IMU\t- init/q_ib\tmounting position of IMU with respect to UAV\t- "},{"title":"Initialisation Failures","type":1,"pageTitle":"Visual-Inertial Odometry","url":"docs/systems/pixhawk_v1/VIO/#initialisation-failures","content":"The filter implemented a relatively robust initialisation check, so it is not uncommon to encounter the situation where the filter refuses to initialise. Most of the failures are reported as ROS messages to give clue for debugging. For the convenience, common failures are listed below: IMU input failed to be received Symptom: \"Waiting for IMU inputs...\"Possible causes: Pixhawk / mavros failure, check system_id etc. Some IMU input received, but not all (failed to synchronise IMU / magnetometer) Symptom: \"[ssf_core] Low number of synchronized imu/magnetometer tuples received.\"Possible causes: Magnetometer not connected properly IMU inputs are received in sync, but IMU Variance Test cannot pass Symptom: Keep receiving \"=============IMU Variance Statitics==============\" messagePossible causes: IMU Gravity estimate deviate too much from referenceVariance of accel, gyro, or magnetometer is too large over the measurement window. IMU input data received, but no VO measurement Symptom: ekf_fusion/pose keep drifting, and ekf_fusion/pose_local is not publishingPossible causes: ZED camera does not start correctly, check USB connection etc "},{"title":"ZED Stereo Camera Calibration","type":1,"pageTitle":"Visual-Inertial Odometry","url":"docs/systems/pixhawk_v1/VIO/#zed-stereo-camera-calibration","content":"During our flight test, we confirmed that ZED stock calibration is not accurate, even with re-calibration using their official tools. Therefore, calibration using a dedicate calibration tool like Kalibr is strongly recommended. After the Kalibr calibration, modify the factory calibration provided by ZED company (could be obtained by SDK, or visit http://calib.stereolabs.com/?SN=1010) with the values of the Kalibr output. The modified file should be stored in pixhawk_v1/param/zed/settings/SNxxxxx.conf A sample calibration file could look like this (calibrated under VGA resolution) [LEFT_CAM_VGA]cx = 333.25292570048475cy = 191.39765841045107fx = 340.7373436904007fy = 340.83066466998304k1 = -0.17236577491455016k2 = 0.025798283381817392p1 = 0.0p2 = -0.00013 [RIGHT_CAM_VGA]cx = 340.91855014058245cy = 184.2264715537507fx = 340.8878322679929fy = 340.89265557837626k1 = -0.17317038234955687k2 = 0.026135241148153184p1 = -0.00026p2 = -0.0005221063830678553 [STEREO]Baseline = 120.899CV_2K = 0.00850606CV_FHD = 0.00850606CV_HD = 0.00850606CV_VGA = 0.0032RX_2K = -0.00420175RX_FHD = -0.00420175RX_HD = -0.00420175RX_VGA = -0.0023RZ_2K = -0.0014632RZ_FHD = -0.0014632RZ_HD = -0.0014632RZ_VGA = -0.0006 Copy "},{"title":"Web-based UI","type":0,"sectionRef":"#","url":"docs/systems/pixhawk_v1/web-ui/","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Web-based UI","url":"docs/systems/pixhawk_v1/web-ui/#overview","content":"This Robot web-based UI package is built based on robotwebtools. It has the following functions: Showing the status of the robot, such as position, real-time image, path, and map, etcIntegrated RVIZ allowing further potential functionsCommand (takeoff, mission, land, etc.) can be sent to the robot directly from the UIConsole output from ROSA mission tab allowing the user to choose multiple waypoints directly from the floor plan   "},{"title":"Installation","type":1,"pageTitle":"Web-based UI","url":"docs/systems/pixhawk_v1/web-ui/#installation","content":""},{"title":"User","type":1,"pageTitle":"Web-based UI","url":"docs/systems/pixhawk_v1/web-ui/#user","content":"You will have to deploy the web page to access it by IP address. It should be deployed on the robot for now. 1. Packages Installation# nginx $ sudo apt-get update$ sudo apt-get install nginx Copy 2. Setup# Make sure the full path of your UI folder have 775 permission $ chmod 775 folder_name Usually you will have to give SD card 775 permission if you put the UI inside it. Copy Config nginx $ sudo gedit /etc/nginx/sites-enabled/defaultfind the line:$ root /var/www/html;and change the value /var/www/html to the path to your UI folder. For example:root /media/nvidia/SD/catkin_ws/src/pixhawk_v1/pixhawk_ui; Copy Restart Nginx: $ sudo systemctl restart nginx Copy 3. Usage: access by IP# Try 'Open Incognito Window' in your web browser and put the pixhawk's IP, you will be able to see the UI if you set everything correctly. Copy You are good to stop here if you want to use the UI only. "},{"title":"Developer (Optional)","type":1,"pageTitle":"Web-based UI","url":"docs/systems/pixhawk_v1/web-ui/#developer-optional","content":"1. Download tools in Vscode on host PC# Bootstrap 4, Font awesome 4, Font Awesome 5 Free & Pro snippets. This is the framework itself with couple of additional features which contain ready-to-use templates and other stuff useful while developing in bootstrap. Live Server To see your web UI changes in realtime Prettier Code formatter To keep your source code clean vscode-icons To make your workspace look good 2. Onboard PC setup# Download ros web server packages# web_video_server $ sudo apt-get install ros-kinetic-web-video-server It launches the server for streaming ROS image messages as video through the web. Copy rosbridge_server $ sudo apt-get install ros-kinetic-rosbridge-suite It launches the web sockets to allow web apps to publish or subscribe ROS messages. Copy tf2-web-republisher $ sudo apt-get install ros-kinetic-tf2-web-republisher Copy Put the following contents in ui.launch under your package# <!-- --><launch> <node pkg=\"web_video_server\" type=\"web_video_server\" name=\"web_video_server\"/> <include file=\"$(find rosbridge_server)/launch/rosbridge_websocket.launch\" /> <node pkg=\"tf2_web_republisher\" type=\"tf2_web_republisher\" name=\"tf2_web_republisher\" /></launch> Copy 3. Usage# Run UI launch file on the onboard PC $ roslaunch your_packages ui.launch Copy Access UI web page on local ground station From Vscode Open your workspace with Vscode; Right click index.html in your Vscode; Click \"Open with Live Server\" and then the web will show up in your browser "},{"title":"MISC","type":1,"pageTitle":"Web-based UI","url":"docs/systems/pixhawk_v1/web-ui/#misc","content":""},{"title":"Tutorials and useful websites","type":1,"pageTitle":"Web-based UI","url":"docs/systems/pixhawk_v1/web-ui/#tutorials-and-useful-websites","content":"Bootstrap 4 + ROSBootstrap 4 TutorialROBOTWEBTOOLSThemeHTML color code "},{"title":"Software Installation","type":0,"sectionRef":"#","url":"docs/systems/pixhawk/getting-started/","content":"","keywords":""},{"title":"Pre-requisite","type":1,"pageTitle":"Software Installation","url":"docs/systems/pixhawk/getting-started/#pre-requisite","content":"Xavier NX Development Kit Hardware Flashed with the lastest OS from nvidia, as well as CUDA (recommand to install using recovery mode + SDK manager). Remarks: Install NVIDIA SDK manager in a Linux machine; Use jumper wire to short RCV pin with GND pin on Xavier module. Assume username to be nvidia "},{"title":"Install tiscamera Camera Driver","type":1,"pageTitle":"Software Installation","url":"docs/systems/pixhawk/getting-started/#install-tiscamera-camera-driver","content":"Build from source and install tiscamera core driverInstall from .deb file the tiscamera-dutils package, insturctions here "},{"title":"Install ROS Melodic","type":1,"pageTitle":"Software Installation","url":"docs/systems/pixhawk/getting-started/#install-ros-melodic","content":"Install ROSInstall Catkin ToolsCreate the workspace, for example: cd /home/nvidiamkdir catkin_wscd catkin_wsmkdir srccatkin build # this command should suceed Copy Done, from now on all source code should be place within the ./src folder "},{"title":"Install Basic ROS Dependencies","type":1,"pageTitle":"Software Installation","url":"docs/systems/pixhawk/getting-started/#install-basic-ros-dependencies","content":"Mavlink, install by apt manager: ros-melodic-mavlinkMavros, the modified monotonic version is needed: (https://github.com/chengguizi/mavros, monotonic branch, Notes: clone the source code, the source code should be place within the ./src folder. Then go to catkin_ws folder to build the code using cmd catkin build)Install cv_bridge from source, refer here "},{"title":"Install Camera ROS Wrapper","type":1,"pageTitle":"Software Installation","url":"docs/systems/pixhawk/getting-started/#install-camera-ros-wrapper","content":"Installation refer to here. Take special note of the device_list.yaml file, which is called by the corresponding .launch file. If the serial number of the cameras do not match, the driver will not run. "},{"title":"Install VIO Code","type":1,"pageTitle":"Software Installation","url":"docs/systems/pixhawk/getting-started/#install-vio-code","content":"Refer here. "},{"title":"README","type":0,"sectionRef":"#","url":"docs/systems/SAFMC/assets/README/","content":"README","keywords":""},{"title":"ROS Coordinate Systems","type":0,"sectionRef":"#","url":"docs/systems/ros-coordinate-systems/","content":"","keywords":""},{"title":"Frame ID and Coordinate Frames","type":1,"pageTitle":"ROS Coordinate Systems","url":"docs/systems/ros-coordinate-systems/#frame-id-and-coordinate-frames","content":"Naming convention: REP 105 Rule of thumb: sensor data should be in NED frame (depth, VIO), global position and orientation should be in ENU frame (Motion Capture, Code). "},{"title":"From Mavros to ROS (Planning)","type":1,"pageTitle":"ROS Coordinate Systems","url":"docs/systems/ros-coordinate-systems/#from-mavros-to-ros-planning","content":"Frame ID\tCoordinate Frame\tRemark\tExamplemap\tENU\tTrue North Aligned\t/mavros/local_position/pose (configurable in launch file) map_ned\tNED\tBy Mavros base_link\tlocal ENU\tINCONSISTENT\t/mavros/local_position/odom, and static tf sent by mavros base_link_frd\tlocal NED\tBy Mavros\tShould use this as the pointcloud measurement frame base_link_enu shall we define this?  More Info It appears that although ROS recommand NWU as the convention for local frame, but base_link in mavros uses ENU. "},{"title":"From Mavros to ROS (VIO)","type":1,"pageTitle":"ROS Coordinate Systems","url":"docs/systems/ros-coordinate-systems/#from-mavros-to-ros-vio","content":"Frame ID\tCoordinate Frame\tRemark\tExamplebase_link\tENU /mavros/imu/data/orientation base_link\tlocal NWU /mavros/imu/data/angular_velocity, /mavros/imu/data/linear_acceleration base_link\tlocal NWU\tINCONSISTENT\t/mavros/imu/data_raw/* "},{"title":"From ROS to Mavros (VIO)","type":1,"pageTitle":"ROS Coordinate Systems","url":"docs/systems/ros-coordinate-systems/#from-ros-to-mavros-vio","content":"Frame ID\tCoordinate Frame\tRemark\tExampleodom\tENU\tTrue North Aligned odome_ned\tNED\tBy Mavros\t/mavros/odometry/out with frame_id = \"odom_ned\"  // Publish helper TFs used for frame transformation in the odometry plugin std::vector<geometry_msgs::TransformStamped> transform_vector; add_static_transform(\"map\", \"map_ned\", Eigen::Affine3d(ftf::quaternion_from_rpy(M_PI, 0, M_PI_2)),transform_vector); add_static_transform(\"odom\", \"odom_ned\", Eigen::Affine3d(ftf::quaternion_from_rpy(M_PI, 0, M_PI_2)),transform_vector); add_static_transform(\"base_link\", \"base_link_frd\", Eigen::Affine3d(ftf::quaternion_from_rpy(M_PI, 0, 0)),transform_vector); tf2_static_broadcaster.sendTransform(transform_vector); Copy "},{"title":"Case Study /mavros/local_position","type":1,"pageTitle":"ROS Coordinate Systems","url":"docs/systems/ros-coordinate-systems/#case-study-mavroslocal_position","content":"All message below assumes map as the frame id, and the child frame id is base_link. This means that map is representing ENU coordinates. Also, the orientation is also converted to ENU frame, which means base_link is also in ENU frame. "},{"title":"tf","type":1,"pageTitle":"ROS Coordinate Systems","url":"docs/systems/ros-coordinate-systems/#tf","content":"The tf transformation is optionally sent by setting the parameter local_position/tf/send to true, located at px4_config.yaml "},{"title":"Unity Simulator : Outline","type":0,"sectionRef":"#","url":"docs/systems/unity-simulator/","content":"","keywords":""},{"title":"Summary","type":1,"pageTitle":"Unity Simulator : Outline","url":"docs/systems/unity-simulator/#summary","content":"summary: The simulator is introduced in this post. Without the presence of the real vehicle and further ado, algorithm developers can swiftly test, debug and revise their codes in the provided mock-up UAV in the simulated environment which is modelled comparable to the actual test field. "},{"title":"Overview","type":1,"pageTitle":"Unity Simulator : Outline","url":"docs/systems/unity-simulator/#overview","content":"Ahead of the physical UAV platform being built up, software codes and algorithms to be implemented on the drone later on can be validated and substantiated by means of Unity3D simulation. Without the presence of the real vehicle and further ado, algorithm developers can swiftly test, debug and revise their codes in the provided mock-up UAV in the simulated environment which is modelled comparable to the actual test field. As for the whole simulation structure, all the sections can be categorized and divided into two sides: Unity3D side, which resembles the information and data flow for the UAV hardware excluding the on-board CPU, and the ROS side, which acts as the `brain' of the UAV, processes acquired sensor data and distributes commands and tasks. All the ROS codes are run on an actual TX2. As a result, the flow of data transmission is as follows: firstly, sensors on the UAV (Unity3D side) acquire data of the surroundings; secondly, the data are sent to the on-board CPU, in other words, TX2 (ROS side) to be processed and transformed into task commands; thirdly, task commands are sent to the motion controller Pixhawk (Unity3D side) and control signals in PWM are generated; fourthly, PWM signals are transmitted to the motors and propellers (Unity3D side) for them to spin and generate upward thrust for the drone; fifthly, the drone therefore can manoeuvre in the built-up environment (Unity3D side) and the on-board sensors can collect data accordingly; lastly, the data goes to the TX2 again in the first step and that closes the loop for data flow. In this method, a hardware-in-the-loop simulation is realized, because the TX2 is to be installed onto the actual drone afterwards. The full simulation is conducted in the following manner: the drone takes off outside of the building, flies through the gate and avoids the obstacles, detects and tracks human targets; when all the requirements are achieved, the simulation is considered completed. Below is about the excutable file for the simulator and the communicate socket between ROS and Unity simulator supporting UDP protocol. "},{"title":"Unity Socket","type":1,"pageTitle":"Unity Simulator : Outline","url":"docs/systems/unity-simulator/#unity-socket","content":""},{"title":"Subscribed Topics","type":1,"pageTitle":"Unity Simulator : Outline","url":"docs/systems/unity-simulator/#subscribed-topics","content":"/mavlink/from_pixhawk (mavros::Mavlink) MAVLink messages, which will be send to simulator. "},{"title":"Published Topics","type":1,"pageTitle":"Unity Simulator : Outline","url":"docs/systems/unity-simulator/#published-topics","content":"/hil/sensor/laser_altitude (sensor_msgs::Range) Altitude data measured by laser from simulator. /hil/sensor/imu (sensor_msgs::Imu) IMU data from simulator. /hil/sensor/magnetic_field (sensor_msgs::MagneticField) Magnetic field data from simulator. /hil/sensor/absolute_pressure (sensor_msgs::FluidPressure) Absolute pressure data from simulator. /hil/sensor/differential_pressure (sensor_msgs::FluidPressure) Differential pressue data from simulator. /hil/sensor/pressure_altitude (sensor_msgs::Range) Altitude data measured by pressure from simulator. /hil/sensor/temperature (sensor_msgs::Temperature) Temperature data from simulator. /hil/sensor/gps (sensor_msgs::NavSatFix) GPS data from simulator. /hil/state/ground_truth (nus_msgs::StateWithCovarianceStamped) Ground truth data from simulator. /hil/state/measurement (nus_msgs::StateWithCovarianceStamped) Measurement data from simulator. "},{"title":"Parameters","type":1,"pageTitle":"Unity Simulator : Outline","url":"docs/systems/unity-simulator/#parameters","content":"~server_ip (string, default: 172.16.142.34) Unity simulator IP address. ~server_port_basic (int, default: 26000) Unity simulator port for basic data. ~server_port_image (int, default: 26500) Unity simulator port for image data. ~server_port_stereo (int, default: 26510) Unity simulator port for stereo image data. ~server_port_laser2d (int, default: 26600) Unity simulator port for 2D laser data. ~server_port_laser3d (int, default: 26700) Unity simulator port for 3D laser data. ~server_port_mavlink (int, default: 27000) Unity simulator port for MAVLink data. "},{"title":"Usage","type":0,"sectionRef":"#","url":"docs/systems/unity-setup/","content":"","keywords":""},{"title":"Simulator","type":1,"pageTitle":"Usage","url":"docs/systems/unity-setup/#simulator","content":""},{"title":"Setup","type":1,"pageTitle":"Usage","url":"docs/systems/unity-setup/#setup","content":"Select the venue needed, download the correspoinding folder to local PC.Go into the folder, you'll see a few executable files. The simulation requires you to run the Server file first, followed by the Client files sequentially.Open the Server file, click Play!, select the map needed, click LAUNCH, you'll see the map loaded.Go back to the folder, open the Client01 file, click Play!, make sure the Sim Mode is at DebugOnlyReference (at the upper left corner). Change the REMOTE DATA IP ADDRESS to that of the drone you are gonna connect, if needed, then click CONNECT.Redo step 4 for Client02, Client03 etc., if there is more than one client needed until all client files are opened.After each simulation, make sure all the .exe files are closed. Re-open them by following step 1 to 5 for another round of simulation.  tip How to choose which client to connect which drone# Open a TX2 terminal and run $ echo $simulator_id which gives you a number that represents the first number of a client's UDP port.simulator_id is one bigger than the number in client's name. So, for example, if simulator_id is 2, you should choose client01. tip For build your own unity scene and guide to modify please refer to (Build-your-own) "},{"title":"Build for Auto Fast Flt","type":1,"pageTitle":"Usage","url":"docs/systems/unity-setup/#build-for-auto-fast-flt","content":"The build folder is too large to upload to bitbucket. (Will be available soon)It has been uploaded to  \"\\\\deptnas.nus.edu.sg\\TSL\\Research\\Centre Flight Science\\Intelligent Unmanned Systems\\Research Data Backup\\Users\\YONG Wen Huei Wayne\\Unity Simulator\\Auto Fast Flt_build-2020-mm-dd\" Start Server.exe. Select environment \"Science Centre\"For 5 degree downward tilt Lidar, use  \"\\\\deptnas.nus.edu.sg\\TSL\\Research\\Centre Flight Science\\Intelligent Unmanned Systems\\Research Data Backup\\Users\\YONG Wen Huei Wayne\\Unity Simulator\\Auto Fast Flt_5_deg_down_build-2020-mm-dd\" For 10 degree downward tilt Lidar, use  \"\\\\deptnas.nus.edu.sg\\TSL\\Research\\Centre Flight Science\\Intelligent Unmanned Systems\\Research Data Backup\\Users\\YONG Wen Huei Wayne\\Unity Simulator\\Auto Fast Flt_10_deg_down_build-2020-mm-dd\" "},{"title":"Build for Fisheye View Mavlink2","type":1,"pageTitle":"Usage","url":"docs/systems/unity-setup/#build-for-fisheye-view-mavlink2","content":"The build folder is too large to upload to bitbucket.It has been uploaded to \"\\deptnas.nus.edu.sg\\TSL\\Research\\Centre Flight Science\\Intelligent Unmanned Systems\\Research Data Backup\\Users\\YONG Wen Huei Wayne\\Unity Simulator\\Fisheye_build-2020-mm-dd\"Start Server.exe. Select environment \"Science Centre\"Start Client01.exe. Select Sim Mode \"DebugOnlyReference\"Launch nus_unity_socketThe fisheye image topic is /hil/sensor/stereo/right/image_raw and /hil/sensor/stereo/right/image_raw "},{"title":"Build for Auto Fast Flt Mavlink2","type":1,"pageTitle":"Usage","url":"docs/systems/unity-setup/#build-for-auto-fast-flt-mavlink2","content":"The build folder is too large to upload to bitbucket.It has been uploaded to \"\\deptnas.nus.edu.sg\\TSL\\Research\\Centre Flight Science\\Intelligent Unmanned Systems\\Research Data Backup\\Users\\YONG Wen Huei Wayne\\Unity Simulator\\Auto Fast Flt-mavlink2_build-2020-mm-dd\"Start Server.exe. Select environment \"Science Centre\"Start Client01.exe. Select Sim Mode \"Vision\" if DJI simulation pose is used.Launch nus_unity_socketPublish DJI (ENU) pose to /mavros/vision_pose/pose "},{"title":"Unity SITL","type":0,"sectionRef":"#","url":"docs/systems/unity-SITL/","content":"Unity SITL","keywords":""}]